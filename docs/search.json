[
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project4/index.html",
    "href": "blog/project4/index.html",
    "title": "This is Project 4",
    "section": "",
    "text": "This project is doing market mapping via dimension reduction with principal components analysis (PCA).\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggrepel)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "blog/project4/index.html#section-1-objective",
    "href": "blog/project4/index.html#section-1-objective",
    "title": "This is Project 4",
    "section": "",
    "text": "This project is doing market mapping via dimension reduction with principal components analysis (PCA)."
  },
  {
    "objectID": "blog/project4/index.html#section-2-analysis",
    "href": "blog/project4/index.html#section-2-analysis",
    "title": "This is Project 4",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggrepel)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I Clean some data\n\n\n\nI analyzed the data\n\nlibrary(tidyverse)\nmtcars |&gt; \n    ggplot(aes(x = wt, y= mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I Clean some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n\nlibrary(tidyverse)\nmtcars |&gt; \n    ggplot(aes(x = wt, y= mpg)) + geom_point()"
  },
  {
    "objectID": "resume/resume.html",
    "href": "resume/resume.html",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Benjamin Ma\n\n\n\n\n\n Ben26msh@gmail.com\n github.com/BenjaminMa\n +1 2069307372\nFor more information, please contact me via email.\n\n\n\n\n\nExperienced in statistical analysis, business mapping methods.\nExperience with business analysis and market (inbound, outbound) research with CRM and Salesforce.\nSkilled in leadership\nSkilled in R, Stata, Excel, and SQL\n\n\n\n\nThis resume was made with the R package pagedown.\nLast updated on 2024-05-14."
  },
  {
    "objectID": "resume/resume.html#contact",
    "href": "resume/resume.html#contact",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Ben26msh@gmail.com\n github.com/BenjaminMa\n +1 2069307372\nFor more information, please contact me via email."
  },
  {
    "objectID": "resume/resume.html#skills",
    "href": "resume/resume.html#skills",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Experienced in statistical analysis, business mapping methods.\nExperience with business analysis and market (inbound, outbound) research with CRM and Salesforce.\nSkilled in leadership\nSkilled in R, Stata, Excel, and SQL"
  },
  {
    "objectID": "resume/resume.html#disclaimer",
    "href": "resume/resume.html#disclaimer",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "This resume was made with the R package pagedown.\nLast updated on 2024-05-14."
  },
  {
    "objectID": "resume/resume.html#title",
    "href": "resume/resume.html#title",
    "title": "Benjamin Ma’s resume",
    "section": "Benjamin Ma",
    "text": "Benjamin Ma\n\nSenior year UC SanDiego Student\nlooking for a skill related job (Business Analyst or marketing related)."
  },
  {
    "objectID": "resume/resume.html#education",
    "href": "resume/resume.html#education",
    "title": "Benjamin Ma’s resume",
    "section": "Education",
    "text": "Education\n\nUniversity of California, San Diego\nB.S. Business Economics\nSan Diego, USA\n2022-2024\n\n\n\nUniversity of Edinburgh\nStudy Abroad\nEdinburgh, UK\n2023"
  },
  {
    "objectID": "resume/resume.html#professional-experience",
    "href": "resume/resume.html#professional-experience",
    "title": "Benjamin Ma’s resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nBusiness Analyst, intern\nCamal Group.\nSan Diego, USA\n2023\n\n\nSourcing, cleaning, and visualizing trade data from prospect.\nGenerating weekly summary report to assist make sale strategy decisions.\nOverfulfil the KPI and achieved multiple sales for company.\nReceived return offer from company."
  },
  {
    "objectID": "resume/resume.html#volunteer-experience",
    "href": "resume/resume.html#volunteer-experience",
    "title": "Benjamin Ma’s resume",
    "section": "Volunteer Experience",
    "text": "Volunteer Experience\n\nUnited Nations Volunteer Program (Goodness and Mercy Missions)\nParticipated in non-profit organization Developed marketing strategies with team. Negotiated fundrasing with government agencies and global institutions. Awarded with a certification\nNew York, USA\n11/01/2020-08/01/2021\n\n\nUnited Nations Volunteer Program (Cameroon Association of Active Youths)\n-Assisted facilitating donations from institutions -Led a safety equipment’s advisory group. -Cleaning donation data -Using data visualization tools for weekly project evaluation. -Received a certification and letter of appreciation.\nNew York, USA\n11/25/2020-03/25/2021"
  },
  {
    "objectID": "resume/resume.html#skills-1",
    "href": "resume/resume.html#skills-1",
    "title": "Benjamin Ma’s resume",
    "section": "Skills",
    "text": "Skills\nCRM, Salesforce, Microsoft-office (Excel, Word, PowerPoint), SQL, STATA, R/Rstudio."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Benjamin’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Benjamin Ma",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "This is Project 3",
    "section": "",
    "text": "blog/project3/smartphone_customer_data.csv ## Section 1: Objective This project leverages real-world data from T-Mobile to perform an advanced customer segmentation analysis using the K-means clustering algorithm. The focus is to understand the relationship between customers’ hand sizes and their gaming durations on mobile devices, and how these insights can inform product customization for gamers.\n\n\nThe analysis begins with a dataset from T-Mobile containing diverse customer attributes including biometric data. The data was processed using the R programming language and libraries from the tidyverse suite.\n\n\n\nInitial data exploration was conducted using basic commands to read and summarize the data. The K-means clustering algorithm was applied to segment customers based on gaming time and hand size. This included scaling the data, determining optimal cluster centers, and iterative refinement of these centers. The results were visualized using scatter plots, illustrating the clusters and their centroids. Further analysis included unscaling the cluster centers to interpret them in the context of the original data units.\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsub &lt;- cust_dat %&gt;% select(gaming, handsize)\nhead(sub)   \n\n# A tibble: 6 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   316.     7.88\n2   442.     5.63\n3   245.     7.5 \n4   386.     5.97\n5   204.     6.71\n6   499.     7.7 \n\nggplot(sub) +\n        geom_point(aes(gaming, handsize)) + \n        theme_minimal()\n\n\n\n scl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\n    scl\n\n# A tibble: 3,000 × 2\n    gaming handsize\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.509    0.661 \n 2  1.35    -1.26  \n 3  0.0268   0.336 \n 4  0.977   -0.971 \n 5 -0.246   -0.339 \n 6  1.74     0.507 \n 7 -0.661    0.0371\n 8 -0.641   -2.13  \n 9  1.72    -0.296 \n10 -0.784   -0.569 \n# ℹ 2,990 more rows\n\n    # let's check that the scaling worked\n    \n    scl %&gt;% summarize_all(mean) %&gt;% round(3) # check means\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      0        0\n\n    scl %&gt;% summarize_all(sd)                # check std devs\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1        1\n\n     out &lt;- kmeans(scl, centers=4, nstart=10)\n      K &lt;- 4\n    D &lt;- 10\n    \n    set.seed(1234)\n    out &lt;- kmeans(scl, centers=K, nstart=D)\n str(out)\n\nList of 9\n $ cluster     : int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n $ centers     : num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"gaming\" \"handsize\"\n $ totss       : num 5998\n $ withinss    : num [1:4] 524 439 463 594\n $ tot.withinss: num 2019\n $ betweenss   : num 3979\n $ size        : int [1:4] 875 568 626 931\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n  # 3 ways to extract a list element -- returns the element\n        str(out$cluster)\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[\"cluster\"]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[1]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        # 2 related ways to subset a list into a one-element list (usually not what you want)\n        str(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[1])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n    # We can also see that out$centers is a k-by-J matrix with the coordinates of the\n    # clusters' centers\n        \n        str(out$centers)\n\n num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"gaming\" \"handsize\"\n\n        out$centers\n\n      gaming   handsize\n1 -0.7389810 -0.7204634\n2  0.9189017 -0.9874657\n3  1.0965455  0.6677762\n4 -0.6033999  0.8305672\n\n#grab the cluster membership as a variable and add it to our \n    # dataset as a factor/categorical variable\n    sub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n     sub %&gt;% count(cluster)\n\n# A tibble: 4 × 2\n  cluster     n\n  &lt;fct&gt;   &lt;int&gt;\n1 1         875\n2 2         568\n3 3         626\n4 4         931\n\n    out$size\n\n[1] 875 568 626 931\n\n     # Then, store the clusters' center locations in their own tibble/dataframe\n    centers &lt;- as_tibble(out$centers) \n    centers\n\n# A tibble: 4 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.739   -0.720\n2  0.919   -0.987\n3  1.10     0.668\n4 -0.603    0.831\n\n      # calculate mean and sd\n        SD   &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(sd)\n        MEAN &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(mean)\n        \n        SD\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   149.     1.17\n\n        MEAN\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   241.     7.11\n\n         # repeat/format the values so we can do math with centers (this is needed for line 129 below)\n        SD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        MEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \n        SD\n\n         [,1]     [,2]\n[1,] 148.6346 1.170988\n[2,] 148.6346 1.170988\n[3,] 148.6346 1.170988\n[4,] 148.6346 1.170988\n\n        MEAN\n\n        [,1]     [,2]\n[1,] 240.746 7.106557\n[2,] 240.746 7.106557\n[3,] 240.746 7.106557\n[4,] 240.746 7.106557\n\n        # unscale the centers (convert back into original units)\n        centers &lt;- centers*SD + MEAN\n        round(centers, 1)\n\n  gaming handsize\n1  130.9      6.3\n2  377.3      6.0\n3  403.7      7.9\n4  151.1      8.1\n\n        #plot the points (colored by cluster membership) and the cluster centers\n    ggplot() + \n        geom_point(data=sub,     aes(x=gaming, y=handsize, color=cluster)) + \n        geom_point(data=centers, aes(x=gaming, y=handsize), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n    # Run this function to show initial cluster points, in scaled space\n    \n        fun1 &lt;- function() { \n            # specify a starting point for the cluster centroids\n            c1 &lt;&lt;- c(gaming=-1, handsize= 2)\n            c2 &lt;&lt;- c(gaming= 1, handsize= 1)\n            c3 &lt;&lt;- c(gaming=-1, handsize=-1)\n            c4 &lt;&lt;- c(gaming= 2, handsize=-1)\n            \n            # convert to a data.frame\n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # pick colors\n            col4 &lt;- c(\"magenta\", \"green\", \"cyan\", \"purple\")\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), \n                           shape=21, fill=col4, color=\"black\", size=5) + \n                ggtitle(\"Kmeans centroids\") + \n                theme_minimal()\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun1()\n\n\n\n    # Run this function to show assignment of points\n    \n        fun2 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            # pick closest centroid as cluster to which each point is assigned\n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n                \n            \n            print(p)\n            return(invisible())\n        }\n    \n        fun2()\n\n\n\n    # run these functions a few times to show convergence\n    \n        fun3 &lt;- function() {\n            # Update cluster centers\n            c1 &lt;&lt;- apply(scl[clust==1, ], 2, mean)\n            c2 &lt;&lt;- apply(scl[clust==2, ], 2, mean)\n            c3 &lt;&lt;- apply(scl[clust==3, ], 2, mean)\n            c4 &lt;&lt;- apply(scl[clust==4, ], 2, mean)\n            \n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun4 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n        \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n# You can keep doing it and see the points moving \n        \n        \n    # clean up\n    rm(cent_dat, centers, c1, c2, c3, c4, clust)\n    \n    \n# add labels back to data\n    \n    cust_dat &lt;- cust_dat %&gt;% mutate(cluster = factor(out$cluster))\n    head(cust_dat)\n\n# A tibble: 6 × 20\n  gender height handsize   age gaming  chat  maps video social reading\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 female   71       7.88    20   316.  282.  77.0  500.   176.    93.4\n2 male     64.6     5.63    20   442.  163.  17.5  588.   104.    62.3\n3 male     66.8     7.5     20   245.  362. 143.   357.   319.    82.7\n4 male     67.3     5.97    19   386.  202.  82.7  538.   244.   117. \n5 male     68.1     6.71    19   204.  408. 144.   242.   344.    67.5\n6 male     67.9     7.7     20   499.  157.  56.6  374.   112.    48.6\n# ℹ 10 more variables: total_minutes &lt;dbl&gt;, days_ago &lt;dbl&gt;, years_ago &lt;dbl&gt;,\n#   discount &lt;chr&gt;, phone_id &lt;chr&gt;, brand &lt;chr&gt;, size_cat &lt;chr&gt;, price &lt;dbl&gt;,\n#   screen_size &lt;dbl&gt;, cluster &lt;fct&gt;\n\n# Also can do other market research\n# Profile the segments by demographics.  Specifically:\n# summarize the segments by age, gender, height, and time spent chatting\n    \n    # For numeric variables, we can simply take means. \n    # For categorical variables, we calculate a proportion by taking the mean over the number of \n    # times something is \"true\"\n    \n    cdat &lt;- cust_dat %&gt;% \n                group_by(cluster) %&gt;% \n                summarize(mean_age    = mean(age), \n                          prop_female = mean(gender==\"female\"), \n                          mean_height = mean(height),\n                          mean_chat   = mean(chat))\n    \n    # view results\n    cdat\n\n# A tibble: 4 × 5\n  cluster mean_age prop_female mean_height mean_chat\n  &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 1           20.0       0.625        63.7      324.\n2 2           20.1       0.674        62.6      197.\n3 3           20.1       0.428        69.2      197.\n4 4           20.0       0.380        69.9      309.\n\n    # We see that cluster two of the clusters chat substantially more than the other two clusters\n    # And we see that two of the clusters have a lower percentage of females than the other two clusters\n    \n    # We can plot some of these relationships\n    \n    ggplot(cdat) + \n        geom_col(aes(y=mean_chat, x=cluster, fill=cluster)) + \n        ggtitle(\"Time spent in chat apps by segment\") + \n        theme_minimal()\n\n\n\n# Similar to privious project lets fiture out what is the best K number by using elbow plot.\n    \n    # we might want more information on which to base our choice of k\n    # One thing we might do is try many different values of k, and evaluate\n    # the performance of the algorithm for each k.  Here, our performance\n    # criteria will be the within-group sum of squares (WSS) from the model.\n    # As k increases, the WSS will decrease. The question is:\n    # how fast does it decrease?\n    \n    # let's try k=1, k=2, ..., k=10\n    \n    # we'll create a vector named 'res' to store our results\n    res &lt;- vector(length=10)\n    \n    # we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"grey\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()\n\n\n\n\n\n\n\nThis Customer Segmentation Analysis project utilizes the K-means clustering algorithm to segment customers based on their gaming habits and hand size, drawing from a dataset that includes various customer attributes. Initially, the dataset is imported and displayed for preliminary inspection. The analysis proceeds by subsetting data relevant to gaming time and hand size, followed by visualization through a scatter plot to understand the distribution before clustering. The data is then standardized to neutralize scale discrepancies, ensuring that clustering reflects genuine patterns rather than differences in measurement units. K-means clustering is applied to this standardized data to determine distinct customer groups, with iterative adjustments of cluster centroids to optimize within-cluster homogeneity. This segmentation allows for targeted product development and marketing strategies, particularly for designing smartphones that cater to the ergonomic and usage preferences of gamers. The success of this approach in identifying meaningful customer segments demonstrates the versatility of K-means clustering, suggesting its applicability across different industries to meet varied market and industrial needs."
  },
  {
    "objectID": "blog/project3/index.html#section-2-data",
    "href": "blog/project3/index.html#section-2-data",
    "title": "This is Project 3",
    "section": "",
    "text": "The analysis begins with a dataset from T-Mobile containing diverse customer attributes including biometric data. The data was processed using the R programming language and libraries from the tidyverse suite."
  },
  {
    "objectID": "blog/project3/index.html#section-3-methodology",
    "href": "blog/project3/index.html#section-3-methodology",
    "title": "This is Project 3",
    "section": "",
    "text": "Initial data exploration was conducted using basic commands to read and summarize the data. The K-means clustering algorithm was applied to segment customers based on gaming time and hand size. This included scaling the data, determining optimal cluster centers, and iterative refinement of these centers. The results were visualized using scatter plots, illustrating the clusters and their centroids. Further analysis included unscaling the cluster centers to interpret them in the context of the original data units."
  },
  {
    "objectID": "blog/project3/index.html#section-4-analysis",
    "href": "blog/project3/index.html#section-4-analysis",
    "title": "This is Project 3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsub &lt;- cust_dat %&gt;% select(gaming, handsize)\nhead(sub)   \n\n# A tibble: 6 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   316.     7.88\n2   442.     5.63\n3   245.     7.5 \n4   386.     5.97\n5   204.     6.71\n6   499.     7.7 \n\nggplot(sub) +\n        geom_point(aes(gaming, handsize)) + \n        theme_minimal()\n\n\n\n scl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\n    scl\n\n# A tibble: 3,000 × 2\n    gaming handsize\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.509    0.661 \n 2  1.35    -1.26  \n 3  0.0268   0.336 \n 4  0.977   -0.971 \n 5 -0.246   -0.339 \n 6  1.74     0.507 \n 7 -0.661    0.0371\n 8 -0.641   -2.13  \n 9  1.72    -0.296 \n10 -0.784   -0.569 \n# ℹ 2,990 more rows\n\n    # let's check that the scaling worked\n    \n    scl %&gt;% summarize_all(mean) %&gt;% round(3) # check means\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      0        0\n\n    scl %&gt;% summarize_all(sd)                # check std devs\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1        1\n\n     out &lt;- kmeans(scl, centers=4, nstart=10)\n      K &lt;- 4\n    D &lt;- 10\n    \n    set.seed(1234)\n    out &lt;- kmeans(scl, centers=K, nstart=D)\n str(out)\n\nList of 9\n $ cluster     : int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n $ centers     : num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"gaming\" \"handsize\"\n $ totss       : num 5998\n $ withinss    : num [1:4] 524 439 463 594\n $ tot.withinss: num 2019\n $ betweenss   : num 3979\n $ size        : int [1:4] 875 568 626 931\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n  # 3 ways to extract a list element -- returns the element\n        str(out$cluster)\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[\"cluster\"]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[1]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        # 2 related ways to subset a list into a one-element list (usually not what you want)\n        str(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[1])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n    # We can also see that out$centers is a k-by-J matrix with the coordinates of the\n    # clusters' centers\n        \n        str(out$centers)\n\n num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"gaming\" \"handsize\"\n\n        out$centers\n\n      gaming   handsize\n1 -0.7389810 -0.7204634\n2  0.9189017 -0.9874657\n3  1.0965455  0.6677762\n4 -0.6033999  0.8305672\n\n#grab the cluster membership as a variable and add it to our \n    # dataset as a factor/categorical variable\n    sub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n     sub %&gt;% count(cluster)\n\n# A tibble: 4 × 2\n  cluster     n\n  &lt;fct&gt;   &lt;int&gt;\n1 1         875\n2 2         568\n3 3         626\n4 4         931\n\n    out$size\n\n[1] 875 568 626 931\n\n     # Then, store the clusters' center locations in their own tibble/dataframe\n    centers &lt;- as_tibble(out$centers) \n    centers\n\n# A tibble: 4 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.739   -0.720\n2  0.919   -0.987\n3  1.10     0.668\n4 -0.603    0.831\n\n      # calculate mean and sd\n        SD   &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(sd)\n        MEAN &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(mean)\n        \n        SD\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   149.     1.17\n\n        MEAN\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   241.     7.11\n\n         # repeat/format the values so we can do math with centers (this is needed for line 129 below)\n        SD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        MEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \n        SD\n\n         [,1]     [,2]\n[1,] 148.6346 1.170988\n[2,] 148.6346 1.170988\n[3,] 148.6346 1.170988\n[4,] 148.6346 1.170988\n\n        MEAN\n\n        [,1]     [,2]\n[1,] 240.746 7.106557\n[2,] 240.746 7.106557\n[3,] 240.746 7.106557\n[4,] 240.746 7.106557\n\n        # unscale the centers (convert back into original units)\n        centers &lt;- centers*SD + MEAN\n        round(centers, 1)\n\n  gaming handsize\n1  130.9      6.3\n2  377.3      6.0\n3  403.7      7.9\n4  151.1      8.1\n\n        #plot the points (colored by cluster membership) and the cluster centers\n    ggplot() + \n        geom_point(data=sub,     aes(x=gaming, y=handsize, color=cluster)) + \n        geom_point(data=centers, aes(x=gaming, y=handsize), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n    # Run this function to show initial cluster points, in scaled space\n    \n        fun1 &lt;- function() { \n            # specify a starting point for the cluster centroids\n            c1 &lt;&lt;- c(gaming=-1, handsize= 2)\n            c2 &lt;&lt;- c(gaming= 1, handsize= 1)\n            c3 &lt;&lt;- c(gaming=-1, handsize=-1)\n            c4 &lt;&lt;- c(gaming= 2, handsize=-1)\n            \n            # convert to a data.frame\n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # pick colors\n            col4 &lt;- c(\"magenta\", \"green\", \"cyan\", \"purple\")\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), \n                           shape=21, fill=col4, color=\"black\", size=5) + \n                ggtitle(\"Kmeans centroids\") + \n                theme_minimal()\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun1()\n\n\n\n    # Run this function to show assignment of points\n    \n        fun2 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            # pick closest centroid as cluster to which each point is assigned\n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n                \n            \n            print(p)\n            return(invisible())\n        }\n    \n        fun2()\n\n\n\n    # run these functions a few times to show convergence\n    \n        fun3 &lt;- function() {\n            # Update cluster centers\n            c1 &lt;&lt;- apply(scl[clust==1, ], 2, mean)\n            c2 &lt;&lt;- apply(scl[clust==2, ], 2, mean)\n            c3 &lt;&lt;- apply(scl[clust==3, ], 2, mean)\n            c4 &lt;&lt;- apply(scl[clust==4, ], 2, mean)\n            \n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun4 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n        \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n# You can keep doing it and see the points moving \n        \n        \n    # clean up\n    rm(cent_dat, centers, c1, c2, c3, c4, clust)\n    \n    \n# add labels back to data\n    \n    cust_dat &lt;- cust_dat %&gt;% mutate(cluster = factor(out$cluster))\n    head(cust_dat)\n\n# A tibble: 6 × 20\n  gender height handsize   age gaming  chat  maps video social reading\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 female   71       7.88    20   316.  282.  77.0  500.   176.    93.4\n2 male     64.6     5.63    20   442.  163.  17.5  588.   104.    62.3\n3 male     66.8     7.5     20   245.  362. 143.   357.   319.    82.7\n4 male     67.3     5.97    19   386.  202.  82.7  538.   244.   117. \n5 male     68.1     6.71    19   204.  408. 144.   242.   344.    67.5\n6 male     67.9     7.7     20   499.  157.  56.6  374.   112.    48.6\n# ℹ 10 more variables: total_minutes &lt;dbl&gt;, days_ago &lt;dbl&gt;, years_ago &lt;dbl&gt;,\n#   discount &lt;chr&gt;, phone_id &lt;chr&gt;, brand &lt;chr&gt;, size_cat &lt;chr&gt;, price &lt;dbl&gt;,\n#   screen_size &lt;dbl&gt;, cluster &lt;fct&gt;\n\n# Also can do other market research\n# Profile the segments by demographics.  Specifically:\n# summarize the segments by age, gender, height, and time spent chatting\n    \n    # For numeric variables, we can simply take means. \n    # For categorical variables, we calculate a proportion by taking the mean over the number of \n    # times something is \"true\"\n    \n    cdat &lt;- cust_dat %&gt;% \n                group_by(cluster) %&gt;% \n                summarize(mean_age    = mean(age), \n                          prop_female = mean(gender==\"female\"), \n                          mean_height = mean(height),\n                          mean_chat   = mean(chat))\n    \n    # view results\n    cdat\n\n# A tibble: 4 × 5\n  cluster mean_age prop_female mean_height mean_chat\n  &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 1           20.0       0.625        63.7      324.\n2 2           20.1       0.674        62.6      197.\n3 3           20.1       0.428        69.2      197.\n4 4           20.0       0.380        69.9      309.\n\n    # We see that cluster two of the clusters chat substantially more than the other two clusters\n    # And we see that two of the clusters have a lower percentage of females than the other two clusters\n    \n    # We can plot some of these relationships\n    \n    ggplot(cdat) + \n        geom_col(aes(y=mean_chat, x=cluster, fill=cluster)) + \n        ggtitle(\"Time spent in chat apps by segment\") + \n        theme_minimal()\n\n\n\n# Similar to privious project lets fiture out what is the best K number by using elbow plot.\n    \n    # we might want more information on which to base our choice of k\n    # One thing we might do is try many different values of k, and evaluate\n    # the performance of the algorithm for each k.  Here, our performance\n    # criteria will be the within-group sum of squares (WSS) from the model.\n    # As k increases, the WSS will decrease. The question is:\n    # how fast does it decrease?\n    \n    # let's try k=1, k=2, ..., k=10\n    \n    # we'll create a vector named 'res' to store our results\n    res &lt;- vector(length=10)\n    \n    # we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"grey\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()"
  },
  {
    "objectID": "blog/project3/index.html#section-5-summary",
    "href": "blog/project3/index.html#section-5-summary",
    "title": "This is Project 3",
    "section": "",
    "text": "This Customer Segmentation Analysis project utilizes the K-means clustering algorithm to segment customers based on their gaming habits and hand size, drawing from a dataset that includes various customer attributes. Initially, the dataset is imported and displayed for preliminary inspection. The analysis proceeds by subsetting data relevant to gaming time and hand size, followed by visualization through a scatter plot to understand the distribution before clustering. The data is then standardized to neutralize scale discrepancies, ensuring that clustering reflects genuine patterns rather than differences in measurement units. K-means clustering is applied to this standardized data to determine distinct customer groups, with iterative adjustments of cluster centroids to optimize within-cluster homogeneity. This segmentation allows for targeted product development and marketing strategies, particularly for designing smartphones that cater to the ergonomic and usage preferences of gamers. The success of this approach in identifying meaningful customer segments demonstrates the versatility of K-means clustering, suggesting its applicability across different industries to meet varied market and industrial needs."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "data file:mtcars I use Kmean algorithm in this project to analyse market segmentation problems.\n\n\n\n[1] mpg Miles/(US) gallon [2] cyl Number of cylinders [3] disp Displacement (cu.in.) [4] hp Gross horsepower [5] drat Rear axle ratio [6] wt Weight (1000 lbs) [7] qsec 1/4 mile time [8] vs Engine (0 = V-shaped, 1 = straight) [9] am Transmission (0 = automatic, 1 = manual) [10] gear Number of forward gears\n\nlibrary(tidyverse)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nmtcars |&gt; \n  ggplot(aes(x = wt, y= mpg)) + geom_point()\n\n\n\nsub &lt;- mtcars %&gt;% select(wt, mpg)\nsub\n\n                       wt  mpg\nMazda RX4           2.620 21.0\nMazda RX4 Wag       2.875 21.0\nDatsun 710          2.320 22.8\nHornet 4 Drive      3.215 21.4\nHornet Sportabout   3.440 18.7\nValiant             3.460 18.1\nDuster 360          3.570 14.3\nMerc 240D           3.190 24.4\nMerc 230            3.150 22.8\nMerc 280            3.440 19.2\nMerc 280C           3.440 17.8\nMerc 450SE          4.070 16.4\nMerc 450SL          3.730 17.3\nMerc 450SLC         3.780 15.2\nCadillac Fleetwood  5.250 10.4\nLincoln Continental 5.424 10.4\nChrysler Imperial   5.345 14.7\nFiat 128            2.200 32.4\nHonda Civic         1.615 30.4\nToyota Corolla      1.835 33.9\nToyota Corona       2.465 21.5\nDodge Challenger    3.520 15.5\nAMC Javelin         3.435 15.2\nCamaro Z28          3.840 13.3\nPontiac Firebird    3.845 19.2\nFiat X1-9           1.935 27.3\nPorsche 914-2       2.140 26.0\nLotus Europa        1.513 30.4\nFord Pantera L      3.170 15.8\nFerrari Dino        2.770 19.7\nMaserati Bora       3.570 15.0\nVolvo 142E          2.780 21.4\n\nggplot(sub) +geom_point(aes(wt, mpg)) + theme_minimal()\n\n\n\nscl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\nscl\n\n# A tibble: 32 × 2\n         wt    mpg\n      &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.610    0.151\n 2 -0.350    0.151\n 3 -0.917    0.450\n 4 -0.00230  0.217\n 5  0.228   -0.231\n 6  0.248   -0.330\n 7  0.361   -0.961\n 8 -0.0278   0.715\n 9 -0.0687   0.450\n10  0.228   -0.148\n# ℹ 22 more rows\n\nscl %&gt;% summarize_all(mean) %&gt;% round(3) \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0     0\n\nscl %&gt;% summarize_all(sd)  \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n\n#save the output from the kmeans() function into an object named \"out\"\nout &lt;- kmeans(scl, centers=4, nstart=10)\nK &lt;- 4\nD &lt;- 10\n#set seed for random number generator \nset.seed(1234)\n\n\n# \"out\" is a list.  This is common with model-fitting functions in R.  To get\n# a better sense for what's included in \"out\", let's run the structure function:\nout &lt;- kmeans(scl, centers=K, nstart=D)\nstr(out)\n\nList of 9\n $ cluster     : int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n $ centers     : num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"wt\" \"mpg\"\n $ totss       : num 62\n $ withinss    : num [1:4] 1.623 2.016 1.271 0.355\n $ tot.withinss: num 5.27\n $ betweenss   : num 56.7\n $ size        : int [1:4] 6 14 9 3\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n# 3 ways to extract a list element -- returns the element\nstr(out$cluster)\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[\"cluster\"]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[1]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# 2 related ways to subset a list into a one-element list (usually not what you want)\nstr(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[1])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# out$centers is a k-by-J matrix with the coordinates of the\n# clusters' centers\n        \nstr(out$centers)\n\n num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"wt\" \"mpg\"\n\nout$centers\n\n          wt        mpg\n1 -1.3738462  1.6552394\n2  0.3846068 -0.5957617\n3 -0.4054284  0.2799348\n4  2.1691456 -1.3700619\n\n#enhancing plot by using the data above\nsub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n#comparing a count from our data to the kmeans() 'size' output\nsub %&gt;% count(cluster)\n\n  cluster  n\n1       1  6\n2       2 14\n3       3  9\n4       4  3\n\nout$size\n\n[1]  6 14  9  3\n\n#store the clusters' center locations in their own tibble/dataframe\ncenters &lt;- as_tibble(out$centers) \ncenters\n\n# A tibble: 4 × 2\n      wt    mpg\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 -1.37   1.66 \n2  0.385 -0.596\n3 -0.405  0.280\n4  2.17  -1.37 \n\n# calculate mean and sd\nSD   &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(sd)\nMEAN &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(mean)\n        \nSD\n\n         wt      mpg\n1 0.9784574 6.026948\n\nMEAN\n\n       wt      mpg\n1 3.21725 20.09062\n\n#repeat/format the values so we can do math with centers \nSD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\nMEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \nSD\n\n          [,1]     [,2]\n[1,] 0.9784574 6.026948\n[2,] 0.9784574 6.026948\n[3,] 0.9784574 6.026948\n[4,] 0.9784574 6.026948\n\nMEAN\n\n        [,1]     [,2]\n[1,] 3.21725 20.09062\n[2,] 3.21725 20.09062\n[3,] 3.21725 20.09062\n[4,] 3.21725 20.09062\n\n# unscale the centers (convert back into original units)\ncenters &lt;- centers*SD + MEAN\nround(centers, 1)\n\n   wt  mpg\n1 1.9 30.1\n2 3.6 16.5\n3 2.8 21.8\n4 5.3 11.8\n\nggplot() + \n        geom_point(data=sub,     aes(x=wt, y=mpg, color=cluster)) + \n        geom_point(data=centers, aes(x=wt, y=mpg), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n# is k=4 the right number for k?\n# let's try k=1, k=2, ..., k=10\nres &lt;- vector(length=10)\n# we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"red\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()\n\n\n\n\n\n\n\n\n# Example with \"iris\" data\n#The Iris dataset comprises measurements of iris flowers from three different \n#species: Setosa, Versicolor, and Virginica. Each sample consists of \n#four features: sepal length, sepal width, petal length, and petal width. \n#Additionally, each sample is labeled with its corresponding species.\n# load data\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# plot petal length vs petal width by species\nggplot(iris) + \ngeom_point(aes(x=Petal.Length, y=Petal.Width, col=Species))\n\n\n\n# run kmeans\nout_iris &lt;- iris %&gt;% \nselect(Petal.Length, Petal.Width) %&gt;% \nkmeans(centers = 3, nstart = 10)\n# add segment membership\niris &lt;- iris %&gt;% mutate(cluster = factor(out_iris$cluster))\n    \n# plot segmented data\nggplot(iris, aes(x=Petal.Length, y=Petal.Width)) +\ngeom_point(aes(col=cluster)) + \ngeom_point(data=as_tibble(out_iris$centers), size=4) + \ntheme_minimal()\n\n\n\n# \"confusion matrix\" -- ie, a table of actual vs predicted\niris %&gt;% select(Species, cluster) %&gt;% table()\n\n            cluster\nSpecies       1  2  3\n  setosa      0 50  0\n  versicolor  2  0 48\n  virginica  46  0  4"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "data file:mtcars I use Kmean algorithm in this project to analyse market segmentation problems."
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "[1] mpg Miles/(US) gallon [2] cyl Number of cylinders [3] disp Displacement (cu.in.) [4] hp Gross horsepower [5] drat Rear axle ratio [6] wt Weight (1000 lbs) [7] qsec 1/4 mile time [8] vs Engine (0 = V-shaped, 1 = straight) [9] am Transmission (0 = automatic, 1 = manual) [10] gear Number of forward gears\n\nlibrary(tidyverse)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nmtcars |&gt; \n  ggplot(aes(x = wt, y= mpg)) + geom_point()\n\n\n\nsub &lt;- mtcars %&gt;% select(wt, mpg)\nsub\n\n                       wt  mpg\nMazda RX4           2.620 21.0\nMazda RX4 Wag       2.875 21.0\nDatsun 710          2.320 22.8\nHornet 4 Drive      3.215 21.4\nHornet Sportabout   3.440 18.7\nValiant             3.460 18.1\nDuster 360          3.570 14.3\nMerc 240D           3.190 24.4\nMerc 230            3.150 22.8\nMerc 280            3.440 19.2\nMerc 280C           3.440 17.8\nMerc 450SE          4.070 16.4\nMerc 450SL          3.730 17.3\nMerc 450SLC         3.780 15.2\nCadillac Fleetwood  5.250 10.4\nLincoln Continental 5.424 10.4\nChrysler Imperial   5.345 14.7\nFiat 128            2.200 32.4\nHonda Civic         1.615 30.4\nToyota Corolla      1.835 33.9\nToyota Corona       2.465 21.5\nDodge Challenger    3.520 15.5\nAMC Javelin         3.435 15.2\nCamaro Z28          3.840 13.3\nPontiac Firebird    3.845 19.2\nFiat X1-9           1.935 27.3\nPorsche 914-2       2.140 26.0\nLotus Europa        1.513 30.4\nFord Pantera L      3.170 15.8\nFerrari Dino        2.770 19.7\nMaserati Bora       3.570 15.0\nVolvo 142E          2.780 21.4\n\nggplot(sub) +geom_point(aes(wt, mpg)) + theme_minimal()\n\n\n\nscl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\nscl\n\n# A tibble: 32 × 2\n         wt    mpg\n      &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.610    0.151\n 2 -0.350    0.151\n 3 -0.917    0.450\n 4 -0.00230  0.217\n 5  0.228   -0.231\n 6  0.248   -0.330\n 7  0.361   -0.961\n 8 -0.0278   0.715\n 9 -0.0687   0.450\n10  0.228   -0.148\n# ℹ 22 more rows\n\nscl %&gt;% summarize_all(mean) %&gt;% round(3) \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0     0\n\nscl %&gt;% summarize_all(sd)  \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n\n#save the output from the kmeans() function into an object named \"out\"\nout &lt;- kmeans(scl, centers=4, nstart=10)\nK &lt;- 4\nD &lt;- 10\n#set seed for random number generator \nset.seed(1234)\n\n\n# \"out\" is a list.  This is common with model-fitting functions in R.  To get\n# a better sense for what's included in \"out\", let's run the structure function:\nout &lt;- kmeans(scl, centers=K, nstart=D)\nstr(out)\n\nList of 9\n $ cluster     : int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n $ centers     : num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"wt\" \"mpg\"\n $ totss       : num 62\n $ withinss    : num [1:4] 1.623 2.016 1.271 0.355\n $ tot.withinss: num 5.27\n $ betweenss   : num 56.7\n $ size        : int [1:4] 6 14 9 3\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n# 3 ways to extract a list element -- returns the element\nstr(out$cluster)\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[\"cluster\"]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[1]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# 2 related ways to subset a list into a one-element list (usually not what you want)\nstr(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[1])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# out$centers is a k-by-J matrix with the coordinates of the\n# clusters' centers\n        \nstr(out$centers)\n\n num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"wt\" \"mpg\"\n\nout$centers\n\n          wt        mpg\n1 -1.3738462  1.6552394\n2  0.3846068 -0.5957617\n3 -0.4054284  0.2799348\n4  2.1691456 -1.3700619\n\n#enhancing plot by using the data above\nsub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n#comparing a count from our data to the kmeans() 'size' output\nsub %&gt;% count(cluster)\n\n  cluster  n\n1       1  6\n2       2 14\n3       3  9\n4       4  3\n\nout$size\n\n[1]  6 14  9  3\n\n#store the clusters' center locations in their own tibble/dataframe\ncenters &lt;- as_tibble(out$centers) \ncenters\n\n# A tibble: 4 × 2\n      wt    mpg\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 -1.37   1.66 \n2  0.385 -0.596\n3 -0.405  0.280\n4  2.17  -1.37 \n\n# calculate mean and sd\nSD   &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(sd)\nMEAN &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(mean)\n        \nSD\n\n         wt      mpg\n1 0.9784574 6.026948\n\nMEAN\n\n       wt      mpg\n1 3.21725 20.09062\n\n#repeat/format the values so we can do math with centers \nSD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\nMEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \nSD\n\n          [,1]     [,2]\n[1,] 0.9784574 6.026948\n[2,] 0.9784574 6.026948\n[3,] 0.9784574 6.026948\n[4,] 0.9784574 6.026948\n\nMEAN\n\n        [,1]     [,2]\n[1,] 3.21725 20.09062\n[2,] 3.21725 20.09062\n[3,] 3.21725 20.09062\n[4,] 3.21725 20.09062\n\n# unscale the centers (convert back into original units)\ncenters &lt;- centers*SD + MEAN\nround(centers, 1)\n\n   wt  mpg\n1 1.9 30.1\n2 3.6 16.5\n3 2.8 21.8\n4 5.3 11.8\n\nggplot() + \n        geom_point(data=sub,     aes(x=wt, y=mpg, color=cluster)) + \n        geom_point(data=centers, aes(x=wt, y=mpg), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n# is k=4 the right number for k?\n# let's try k=1, k=2, ..., k=10\nres &lt;- vector(length=10)\n# we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"red\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()"
  },
  {
    "objectID": "blog/project2/index.html#section-3-iris-dataset",
    "href": "blog/project2/index.html#section-3-iris-dataset",
    "title": "This is Project 2",
    "section": "",
    "text": "# Example with \"iris\" data\n#The Iris dataset comprises measurements of iris flowers from three different \n#species: Setosa, Versicolor, and Virginica. Each sample consists of \n#four features: sepal length, sepal width, petal length, and petal width. \n#Additionally, each sample is labeled with its corresponding species.\n# load data\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# plot petal length vs petal width by species\nggplot(iris) + \ngeom_point(aes(x=Petal.Length, y=Petal.Width, col=Species))\n\n\n\n# run kmeans\nout_iris &lt;- iris %&gt;% \nselect(Petal.Length, Petal.Width) %&gt;% \nkmeans(centers = 3, nstart = 10)\n# add segment membership\niris &lt;- iris %&gt;% mutate(cluster = factor(out_iris$cluster))\n    \n# plot segmented data\nggplot(iris, aes(x=Petal.Length, y=Petal.Width)) +\ngeom_point(aes(col=cluster)) + \ngeom_point(data=as_tibble(out_iris$centers), size=4) + \ntheme_minimal()\n\n\n\n# \"confusion matrix\" -- ie, a table of actual vs predicted\niris %&gt;% select(Species, cluster) %&gt;% table()\n\n            cluster\nSpecies       1  2  3\n  setosa      0 50  0\n  versicolor  2  0 48\n  virginica  46  0  4"
  }
]