[
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Project 4: Market Mapping through PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 5: Data Wranging for Customers Demand Estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project5/index.html",
    "href": "blog/project5/index.html",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "This project is showing how I did data wrangling for the mlogit package to support further researching topics in later projects. it’s a boring topic and feel free to ignore it.\n\n\nThe Multinomial Logit Model (MNL) is a predictive modeling technique often used in demand prediction when the outcome of interest has more than two categorical outcomes. It is an extension of the logistic regression model to multiple classes. This model is particularly useful in scenarios like predicting customer choice among several products, transport mode selection, or any situation where individuals select from multiple options.\nHow the Multinomial Logit Model Works:\n1.  Probabilistic Framework: The model estimates the probabilities of each possible outcome as a function of the independent variables.\n2.  Utility-Based: Each choice option is assumed to have a utility associated with it, which is modeled as a linear combination of explanatory variables.\n3.  Logit Function: It uses a logit function to model the probability that a particular alternative is chosen.\nApplications in Demand Prediction:\n•   Transport Economics: Predicting the mode of transport (bus, train, car, etc.) a person might choose based on various factors like cost, time, and comfort.\n•   Marketing: Determining which product a consumer is likely to purchase based on features such as price, brand, and consumer demographics.\n•   Public Policy: Assessing the likelihood of individuals opting for different public services.\nData Wrangling for Multinomial Logit Model:\nData wrangling, or data preprocessing, is a crucial step before applying any predictive modeling technique. Here’s how you might go about it for the Multinomial Logit Model:\n1.  Data Cleaning:\n•   Missing Values: Handle missing data through imputation or removal.\n•   Outliers: Identify and treat outliers as they can skew the results.\n2.  Data Transformation:\n•   Normalization/Standardization: Scale numeric features to have a mean of zero \n  and a standard deviation of one, or transform them to range between 0 and 1.\n•   Encoding Categorical Variables: Convert categorical variables into dummy/indicator \n  variables.For instance, using one-hot encoding.\n3.  Feature Selection:\n•   Identify which features are most relevant to the prediction. This can be achieved \n  through statistical tests, domain knowledge, or machine learning feature selection \n  techniques.\n4.  Data Integration:\n•   Combine data from multiple sources to enrich the dataset. Ensure alignment \n  and compatibility between different data sources.\n5.  Feature Engineering:\n•   Create new features that can capture important information in a more useful form. \n  This might involve aggregating data, creating interaction terms, \n  or transforming variables.\n\n\n\nDemand modeling with archival data enables data-driven sales predictions at counterfactual characteristics(prices) MNL (usually with extensions) is the most popular demand model Micro founded Well behaved, estimable Based on clearly specified theoretical model of choice Demand parameter estimates may be biased when price is correlated with unobservables that drive demand. This is endogeneity. Can sometimes be dealt with.\nPrice endogeneity is a major issue in demand estimation: Suppose the data do not record all product attributes that vary and affect demand (eg, brand reputation, coolness, aesthetics, reliability, etc) if unobserved attributes affect both price and quantity, then predictors are correlated with errors, and we say endogeneity would bias demand estimates. This is just like the assumption in linear regression that cov(X,e)=0. in this modeling I am going to disregard endogeneity for simplicity. Instead of predict demand people usually see the product’s quantity response to a change in price.\n\n\n\n\n#Using T-mobile customer phone data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mlogit)\n\nLoading required package: dfidx\n\nAttaching package: 'dfidx'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(readr)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\", show_col_types = F)\nn &lt;- nrow(cust_dat)\n# replace missing 'discount' values (currently NA) with empty string (\"\")\ncust_dat &lt;- cust_dat |&gt; mutate(discount = ifelse(is.na(discount), \"\", discount))\nset.seed(1357)   \n    subk &lt;- cust_dat |&gt; select(gaming, chat, maps, video, social, reading)\n    outk &lt;- kmeans(subk, centers = 3, nstart = 10)\n    table(outk$cluster)\n\n\n   1    2    3 \n 542 1302 1156 \n\n    cust_dat$segment &lt;- factor(outk$cluster)\n    rm(subk, outk)\n# import phone attributes     \n    phone_dat &lt;- read_csv(\"./phone_dat.csv\", show_col_types = F)\n\n\n\n\nNow we need to combine these datasets. The customer data has “n” rows and for each customer, there were 6 phones available on the market when the customer purchased their phone. So we will construct a dataset that has n*6 rows. This facilitates mlogit calculating a predicted utility for each available option for each available customer\nI’m going to do this in three steps.\nStep 1: loop over customers and create a dataset of the 6 available phones for that customer. This is a very flexible way to structure the data. It will be useful to us because we may need to adjust the price of a phone if it was on discount, and this adjustment is customer-specific. More generally, this is a good way to structure data for a MNL model since it would allow different customers to choose from different sets of products.\nstep 2: we will stack (ie append) these n datasets on top of each other to create the dataset with the n*6 rows.\nStep 3: the ‘mlogit’ package that fits the MNL model requires us to create an mlogit-data object, so we’ll do that, and then we’ll feed that mlogit-data object into the mlogit() function to estimate the parameters of this model.\n\n# create an empty list to store the n datasets (each dataset will have 6 rows)\n    dat_list &lt;- vector(mode = \"list\", length = n)\npb &lt;- txtProgressBar(min = 1, max = n, style = 3)\n\n#loop for step 1\n\n    for (i in 1:n) {\n      # get cohort, minutes, brand, and size for customer i\n      i_cohort   &lt;- cust_dat |&gt; slice(i) |&gt; pull(years_ago)   \n      i_brand    &lt;- cust_dat |&gt; slice(i) |&gt; pull(brand)\n      i_size     &lt;- cust_dat |&gt; slice(i) |&gt; pull(screen_size)\n      i_discount &lt;- cust_dat |&gt; slice(i) |&gt; pull(discount)\n      i_segment  &lt;- cust_dat |&gt; slice(i) |&gt; pull(segment)\n      i_minutes  &lt;- cust_dat |&gt; slice(i) |&gt; pull(total_minutes)\n    \n      # subset the phone data to the 6 phones for the year when the customer purchased\n      PD &lt;- phone_dat |&gt; filter(years_ago == i_cohort)\n    \n      # adjust one of the phone's price for the 10% discount, if applicable\n      PD &lt;- PD |&gt; mutate(price = price - (phone_id == i_discount) * price * 0.1)\n    \n      # add customer id to PD\n      PD &lt;- PD |&gt; mutate(customer_id = i)\n    \n      # convert the one brand variable into a set of 3 brand dummies (ie, binary variables)\n      PD &lt;- PD |&gt; mutate(\n        apple = as.integer(brand == \"apple\"),\n        huawei = as.integer(brand == \"huawei\"),\n        samsung = as.integer(brand == \"samsung\")\n      )\n    \n      # create a binary variable to indicate the chosen phone\n      # this is going to be the dependent variable in the MNL model (like \"y\" in OLS)\n      PD &lt;- PD |&gt;\n        mutate(choice = (brand == i_brand) & (screen_size == i_size)) |&gt;\n        mutate(choice = as.integer(choice))\n    \n      # add segment and total_minutes\n      PD &lt;- PD |&gt; mutate(segment = i_segment, total_minutes = i_minutes)\n    \n      # store this 6-row dataset in the i'th position of that list we initialized before the loop\n      dat_list[[i]] &lt;- PD |&gt; select(\n        customer_id, phone_id, choice,\n        apple, huawei, samsung,\n        price, screen_size,\n        segment, total_minutes\n      )\n    \n    }\n    \n    # clean up -- delete temporary objects from the loop that we don't need anymore\n    rm(i, i_cohort, i_brand, i_size, i_discount, i_segment, i_minutes, PD, pb)\n    \n    # Let's take a look at two (out of the n) 6-row datasets:\n    dat_list[1]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1           1 A1            0     1      0       0   749         5.5 2      \n2           1 A2            0     1      0       0   899         5.8 2      \n3           1 S1            0     0      0       1   699         5.6 2      \n4           1 S2            0     0      0       1   799         5.9 2      \n5           1 H1            0     0      1       0   599         5.2 2      \n6           1 H2            1     0      1       0   699         5.7 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    dat_list[100]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1         100 A1            1     1      0       0   799         5.8 2      \n2         100 A2            0     1      0       0   899         6.1 2      \n3         100 S1            0     0      0       1   749         5.8 2      \n4         100 S2            0     0      0       1   849         6.3 2      \n5         100 H1            0     0      1       0   699         5.7 2      \n6         100 H2            0     0      1       0   749         6   2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n\n\n # Step 2 -----\n    \n    #Stacking the n 6-row customer-specific dataframes into one big dataframe \n#(that will have n*6 rows)\n    \n    # rbind operates on dataframes to concatenate rows\n    # Using do.call in order to concatenate rows within lists\n    mnl_dat &lt;- as_tibble(do.call(rbind, dat_list))\n    \n    rm(dat_list)\n    \n    #Data frame should looks like this \n    head(mnl_dat, n = 20)\n\n# A tibble: 20 × 10\n   customer_id phone_id choice apple huawei samsung price screen_size segment\n         &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1           1 A1            0     1      0       0  749          5.5 2      \n 2           1 A2            0     1      0       0  899          5.8 2      \n 3           1 S1            0     0      0       1  699          5.6 2      \n 4           1 S2            0     0      0       1  799          5.9 2      \n 5           1 H1            0     0      1       0  599          5.2 2      \n 6           1 H2            1     0      1       0  699          5.7 2      \n 7           2 A1            0     1      0       0  749          5.5 2      \n 8           2 A2            0     1      0       0  899          5.8 2      \n 9           2 S1            0     0      0       1  699          5.6 2      \n10           2 S2            0     0      0       1  799          5.9 2      \n11           2 H1            0     0      1       0  539.         5.2 2      \n12           2 H2            1     0      1       0  699          5.7 2      \n13           3 A1            1     1      0       0  749          5.5 3      \n14           3 A2            0     1      0       0  899          5.8 3      \n15           3 S1            0     0      0       1  699          5.6 3      \n16           3 S2            0     0      0       1  719.         5.9 3      \n17           3 H1            0     0      1       0  599          5.2 3      \n18           3 H2            0     0      1       0  699          5.7 3      \n19           4 A1            0     1      0       0  799          5.8 2      \n20           4 A2            0     1      0       0  899          6.1 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    # Then estimating demand for each year separately, since customer preferences may\n#have changed across product generations\n    \n    # Let's split the big (n*6 row) dataframe into 3 dataframes, one for each year.\n    sub1 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 1))\n    sub2 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 2))\n    sub3 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 3))\n\n\n# Step 3 -----\n\n    # converting the 3 'sub' dataframes into mlogit.data objects.\n    # To do that, need to specify the y variable (choice), whether our datasets\n    # have 6 times as many rows as the original data (shape=\"long\") or 6 times as\n    # many columns (shape=\"wide\"), and the id variable that groups the set of\n    # phones from one choice-occasion together (our \"customer_id\" variable).\n    \n    mdat1 &lt;- mlogit.data(sub1, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat2 &lt;- mlogit.data(sub2, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat3 &lt;- mlogit.data(sub3, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    \n    \n\n    \n    # Then I will use customer that bought phones last year \n    #(ie, sub1 and mdat1 where \"years_ago\" == 1) as an example.\n \n## Calculate market shares ----\n\n    \n    # Calculating product-level and brand-level market shares:\n    \n    brand_shares &lt;- cust_dat |&gt;\n                      filter(years_ago == 1) |&gt;\n                      count(brand) |&gt;\n                      mutate(shr = n / sum(n))\n    \n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    product_shares &lt;- cust_dat |&gt;\n                          filter(years_ago == 1) |&gt;\n                          count(phone_id) |&gt;\n                          mutate(shr = n / sum(n))\n    \n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n# Fit basic (brand-intercept only) model -----\n\n    # Always start simple. For the first model, I will fit a model where our \"X\"\n    # variables are just the binary dummy variables that indicate brand.\n    # to leave out one phone as a \"baseline\" and omit an intercept, so that this\n    # model is \"identified\" (ie, can be estimated).Omiting the intercept by\n    # including the bar-zero (\"|0\") in the formula:\n    \n    out1 &lt;- mlogit(choice ~ apple + samsung | 0, data = mdat1)\n    \n    summary(out1)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n3 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00849 \nsuccessive function values within tolerance limits \n\nCoefficients :\n        Estimate Std. Error z-value Pr(&gt;|z|)  \napple   0.197455   0.080071  2.4660  0.01366 *\nsamsung 0.185829   0.080281  2.3147  0.02063 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1737.8\n\n    # The coefficients for the Apple and Samsung brand dummies are\n    # positive and statistically significantly different from zero. Those are in\n    # comparison to the Huawai coefficient which is restricted to zero for identification.\n    # But what do those parameters mean?\n    \n    # Then using these coefficients to calculate the model's estimate of brand-level\n    # market shares. \n\n        # print the coefficients\n        coef(out1)\n\n    apple   samsung \n0.1974553 0.1858286 \n\n        # print the brand market shares estimated from the model\n        coefs1 &lt;- c(huawei = 0, coef(out1))\n        shares1 &lt;- exp(coefs1) / sum(exp(coefs1)) # 𝑒^Vi/∑_j𝑒^Vj\n        round(shares1, 3)\n\n huawei   apple samsung \n  0.292   0.356   0.352 \n\n        # print the actual brand market shares\n        brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n        # print the actual product market shares\n        product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n        # clean up\n        rm(coefs1, shares1)\n        \n        \n    # The model fits the intercepts in order to **exactly**\n    # match the brand-level market shares from the data. However, it does not match\n    # the product-level market shares.\n    \n    # Second, and this is more subtle but general, the sign and magnitude of the\n    # coefficients inform of us the impact on estimated market shares: larger positive\n    # coefficients predict larger market shares. Apple has the largest coefficient\n    # and thus the largest estimated market share.\n\n    # More illustration, calculate two measures of model fit/performance.\n    # using custom functions to make the calculations easy to repeat.\n\n\n\n# Model Fit Functions -----\n\n    # The first is the \"hit rate\" which is the percent of choices the model\n    # correctly predicts. Creating custom functions for the brand hit rate\n    # and the product hit rate. This measure is something that probably\n    # commonly encounter in industry according to text book, as it has a straightforward interpretation.\n    \n    brand_hit_rate &lt;- function(data, model) {\n        # here I use the model to predict which phone maximizes each customer's utility\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        # here I construct a vector of customer choices for comparisons to predictions\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        # here I compare the model's predictions to the data\n        mean(ceiling(preds / 2) == ceiling(actuals / 2))\n    }\n    \n    # now do the same steps but at the phone level\n    product_hit_rate &lt;- function(data, model) {\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        mean(preds == actuals)\n    }\n    \n    # The second measure of model fit is the likelihood ratio index \n    # (also called McFadden's pseudo # R-squared). \n    # Like R^2 from linear regression, this metric ranges from\n    # zero to one, and the interpretation is the degree of improvement over the\n    # random guessing about consumer choices\n    \n    ll_ratio &lt;- function(data, model) {\n        N &lt;- nrow(model$probabilities)\n        J &lt;- ncol(model$probabilities)\n        ll0 &lt;- N * log(1 / J)   # this is the null model for comparison\n        ll1 &lt;- as.numeric(model$logLik)   # this is lnL(beta) from slides\n        1 - ll1 / ll0\n    }\n    \n\n    \n    \n    # Then calculating the brand hit rate and the likelihood ratio index for\n    # mnl model. \n    brand_hit_rate(mdat1, out1)\n\n[1] 0.3559671\n\n    product_hit_rate(mdat1, out1)\n\n[1] 0.2397119\n\n    ll_ratio(mdat1, out1)\n\n[1] 0.002181383\n\n    # The simple/naive \"model\" is that each brand is chosen 33.3% (=1/3).\n    # The brand hit rate is about 35.6%, which is just a bit better than the naive approach.\n    # The likelihood ratio index confirms that the model is not much better than random guessing.\n    \n    # One way can improve a model's performance is to give it more complete data \n    # (ie, more variables). \n\n\n\n\n\n# Add Price -----\n\n    # Adding the price variable to the model and see what happens:\n    \n    out2 &lt;- mlogit(choice ~ apple + samsung + price | 0, data = mdat1)\n    \n    summary(out2)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000129 \nsuccessive function values within tolerance limits \n\nCoefficients :\n           Estimate  Std. Error  z-value  Pr(&gt;|z|)    \napple    1.05865609  0.10876661   9.7333 &lt; 2.2e-16 ***\nsamsung  0.63450323  0.08959903   7.0816 1.425e-12 ***\nprice   -0.00619721  0.00058165 -10.6545 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1677\n\n\n\n\n\nFirstly, it is observed that the coefficients for Apple and Samsung have significantly increased. This suggests that, when price is held constant, consumers demonstrate a stronger preference for Apple and Samsung over Huawei. Market share data indicates that Apple and Samsung dominate over Huawei, which can be attributed to both brand loyalty and pricing strategies. Huawei’s devices, often priced lower than those of Apple and Samsung, achieve competitive market shares due in part to a positive brand perception of Apple and Samsung. This is counterbalanced by the negative impact of their higher pricing.\nSecondly, the negative coefficient associated with price implies that higher prices result in decreased utility for consumers and, consequently, lower market shares. This relationship is intuitive and aligns with economic principles.\nThirdly, the presence of smaller p-values in the analysis may indicate that the current model provides a better fit to the observed data compared to previous models.\n\n\n\n\n    # Let's test that \"better fitting\" hypothesis by calculating the hit rates and\n    # likelihood ratio index for this model.\n\n        brand_hit_rate(mdat1, out2)\n\n[1] 0.3549383\n\n        product_hit_rate(mdat1, out2)\n\n[1] 0.2479424\n\n        ll_ratio(mdat1, out2)\n\n[1] 0.03709335\n\n    # Got a small improvement in brand hit rate which is now 35.6%, compared to\n    # the prior model's brand hit rate of 35.5%.\n    \n    # Got a product hit rate of 24.8%, better than simpler model's product hit rate \n    # of 24.0%\n    \n    # That improvement is noticeable in the likelihood ratio statistic. .037 is much better\n    # than our previous fit of .002\n    \n    \n    # Let's see what has happened to the brands' market share predictions\n    # First need to predict phone shares, then sum over phones to predict brand shares\n\n        #predict phone shares\n        shares2p &lt;- colMeans(predict(out2, newdata = mdat1))\n        names(shares2p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n        \n        #sum over phones to predict brand shares\n        shares2b &lt;- colSums(matrix(shares2p, nrow = 2))\n        names(shares2b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares2b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # ...still exactly match actual brand-level market shares\n    \n    round(shares2p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.254 0.102 0.227 0.124 0.168 0.124 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # ...and now I have product-level market share estimates that better reflect\n    # the actual product-level market shares, albeit not perfectly.\n    # That's probably because I don't have any product-specific attributes or dummies.\n\n\n\n\n\n# Add Size -----\n\n    # Improve the model further by fitting MNL with brand, price, and size\n    \n    out3 &lt;- mlogit(choice ~ apple + samsung + price + screen_size | 0, data = mdat1)\n    \n    summary(out3)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price + screen_size | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000119 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error z-value  Pr(&gt;|z|)    \napple        0.9983423  0.1443294  6.9171 4.610e-12 ***\nsamsung      0.6357234  0.0894121  7.1100 1.160e-12 ***\nprice       -0.0056366  0.0010562 -5.3366 9.473e-08 ***\nscreen_size -0.1416883  0.2226515 -0.6364    0.5245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1676.8\n\n    brand_hit_rate(mdat1, out3)\n\n[1] 0.3549383\n\n    product_hit_rate(mdat1, out3)\n\n[1] 0.2479424\n\n    ll_ratio(mdat1, out3)\n\n[1] 0.03721033\n\n\n\n\n\nThe “size” variable’s coefficient lacks statistical significance, and the hit rates remain virtually the same. This indicates that the “size” variable might not be contributing significantly to the model. The likely reason could be the high correlation between screen size and price, suggesting that the price variable may already account for most of the information provided by the size variable.\nInterestingly, the coefficient for price increased from -0.006 to -0.005, implying that screen size has an influence on price that was not captured in the previous model. This could mean that the previous model suffered from endogeneity issues.\nSpecifically, the original price coefficient of -0.006 was not solely due to price but also incorporated an effect from screen size. After adjusting for screen size, the price coefficient shifted to -0.005.\nRecalling our market mapping exercise, Samsung’s large phones, which featured very big screens, had a low market share. This observation suggests that it might be beneficial to estimate intercepts specific to each phone model instead of using a common screen size parameter for all phones.\n\n# Use Product-Specific Intercepts -----\n\n    # Now fit the MNL model with product-specific intercepts and price\n    \n    # Now, instead of brand dummy variables, I will use product dummy variables.\n    # So there will be 5 dummies (one phone has to be set to 0 for identification).\n    # Notice that because size does not vary for a given phone, so cannot include\n    # it in the model because it would be perfectly collinear with the phone dummies.\n    \n    out4 &lt;- mlogit(choice ~ phone_id + price | 0, data = mdat1)\n    \n    summary(out4)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000518 \nsuccessive function values within tolerance limits \n\nCoefficients :\n             Estimate Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2  0.3324964  0.2212103  1.5031  0.132819    \nphone_idH1 -1.2251497  0.1724673 -7.1037 1.215e-12 ***\nphone_idH2 -0.9080016  0.1303339 -6.9667 3.244e-12 ***\nphone_idS1 -0.3060387  0.1124682 -2.7211  0.006506 ** \nphone_idS2 -0.5545496  0.1346713 -4.1178 3.825e-05 ***\nprice      -0.0071582  0.0012933 -5.5346 3.119e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1668.6\n\n    # Notice that many of the coefficients are negative. This is because the small\n    # Apple phone is the reference product (simply because it's listed first), so\n    # all phones with smaller market shares than Apple small have lower parameter estimates\n    \n    # Notice also that price coefficient has changed yet again to -0.007. Specifically,\n    # the screen size variable in the prior model was capturing the average effect\n    # of screen size across the 3 brands.  Now, have specified a more flexible model\n    # in which screen size and all other product-specific differences are accounted\n    # for by the product dummies\n    \n    # the fit metrics\n\n        brand_hit_rate(mdat1, out4)\n\n[1] 0.367284\n\n        product_hit_rate(mdat1, out4)\n\n[1] 0.2685185\n\n        ll_ratio(mdat1, out4)\n\n[1] 0.0419071\n\n    # The brand hit rate improved 1% from 35.5% to 36.7%.\n    # The product hit rate improved 2% from 24.8% to 26.9%.\n    # The improvement comes from the flexibility of the model to allow for different\n    # preferences for small and large phones *within* a brand.\n    # LL Ratio is now up to .042, 21x larger than the .002 in the brand-only model\n    \n    # check the brand-level market shares\n    \n    shares4p &lt;- colMeans(predict(out4, newdata = mdat1))\n    names(shares4p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n    \n    shares4b &lt;- colSums(matrix(shares4p, nrow = 2))\n    names(shares4b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares4b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # still exactly match actual brand-level market shares\n    \n    round(shares4p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.240 0.116 0.253 0.099 0.148 0.144 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # and now it's able to exactly match product-level market shares.\n\nThe next step involves enhancing our model by incorporating customer-specific heterogeneity. While the model currently performs well on an aggregate level, assessing and tailoring it to individual variations will potentially increase its accuracy and relevance for specific customers. This could involve adding parameters or features that capture unique behaviors or preferences of different customer segments"
  },
  {
    "objectID": "blog/project5/index.html#introduction",
    "href": "blog/project5/index.html#introduction",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "The Multinomial Logit Model (MNL) is a predictive modeling technique often used in demand prediction when the outcome of interest has more than two categorical outcomes. It is an extension of the logistic regression model to multiple classes. This model is particularly useful in scenarios like predicting customer choice among several products, transport mode selection, or any situation where individuals select from multiple options.\nHow the Multinomial Logit Model Works:\n1.  Probabilistic Framework: The model estimates the probabilities of each possible outcome as a function of the independent variables.\n2.  Utility-Based: Each choice option is assumed to have a utility associated with it, which is modeled as a linear combination of explanatory variables.\n3.  Logit Function: It uses a logit function to model the probability that a particular alternative is chosen.\nApplications in Demand Prediction:\n•   Transport Economics: Predicting the mode of transport (bus, train, car, etc.) a person might choose based on various factors like cost, time, and comfort.\n•   Marketing: Determining which product a consumer is likely to purchase based on features such as price, brand, and consumer demographics.\n•   Public Policy: Assessing the likelihood of individuals opting for different public services.\nData Wrangling for Multinomial Logit Model:\nData wrangling, or data preprocessing, is a crucial step before applying any predictive modeling technique. Here’s how you might go about it for the Multinomial Logit Model:\n1.  Data Cleaning:\n•   Missing Values: Handle missing data through imputation or removal.\n•   Outliers: Identify and treat outliers as they can skew the results.\n2.  Data Transformation:\n•   Normalization/Standardization: Scale numeric features to have a mean of zero \n  and a standard deviation of one, or transform them to range between 0 and 1.\n•   Encoding Categorical Variables: Convert categorical variables into dummy/indicator \n  variables.For instance, using one-hot encoding.\n3.  Feature Selection:\n•   Identify which features are most relevant to the prediction. This can be achieved \n  through statistical tests, domain knowledge, or machine learning feature selection \n  techniques.\n4.  Data Integration:\n•   Combine data from multiple sources to enrich the dataset. Ensure alignment \n  and compatibility between different data sources.\n5.  Feature Engineering:\n•   Create new features that can capture important information in a more useful form. \n  This might involve aggregating data, creating interaction terms, \n  or transforming variables."
  },
  {
    "objectID": "blog/project5/index.html#project-overview",
    "href": "blog/project5/index.html#project-overview",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "Demand modeling with archival data enables data-driven sales predictions at counterfactual characteristics(prices) MNL (usually with extensions) is the most popular demand model Micro founded Well behaved, estimable Based on clearly specified theoretical model of choice Demand parameter estimates may be biased when price is correlated with unobservables that drive demand. This is endogeneity. Can sometimes be dealt with.\nPrice endogeneity is a major issue in demand estimation: Suppose the data do not record all product attributes that vary and affect demand (eg, brand reputation, coolness, aesthetics, reliability, etc) if unobserved attributes affect both price and quantity, then predictors are correlated with errors, and we say endogeneity would bias demand estimates. This is just like the assumption in linear regression that cov(X,e)=0. in this modeling I am going to disregard endogeneity for simplicity. Instead of predict demand people usually see the product’s quantity response to a change in price."
  },
  {
    "objectID": "blog/project5/index.html#import-data",
    "href": "blog/project5/index.html#import-data",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "#Using T-mobile customer phone data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mlogit)\n\nLoading required package: dfidx\n\nAttaching package: 'dfidx'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(readr)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\", show_col_types = F)\nn &lt;- nrow(cust_dat)\n# replace missing 'discount' values (currently NA) with empty string (\"\")\ncust_dat &lt;- cust_dat |&gt; mutate(discount = ifelse(is.na(discount), \"\", discount))\nset.seed(1357)   \n    subk &lt;- cust_dat |&gt; select(gaming, chat, maps, video, social, reading)\n    outk &lt;- kmeans(subk, centers = 3, nstart = 10)\n    table(outk$cluster)\n\n\n   1    2    3 \n 542 1302 1156 \n\n    cust_dat$segment &lt;- factor(outk$cluster)\n    rm(subk, outk)\n# import phone attributes     \n    phone_dat &lt;- read_csv(\"./phone_dat.csv\", show_col_types = F)"
  },
  {
    "objectID": "blog/project5/index.html#create-dataset-for-mnl",
    "href": "blog/project5/index.html#create-dataset-for-mnl",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "Now we need to combine these datasets. The customer data has “n” rows and for each customer, there were 6 phones available on the market when the customer purchased their phone. So we will construct a dataset that has n*6 rows. This facilitates mlogit calculating a predicted utility for each available option for each available customer\nI’m going to do this in three steps.\nStep 1: loop over customers and create a dataset of the 6 available phones for that customer. This is a very flexible way to structure the data. It will be useful to us because we may need to adjust the price of a phone if it was on discount, and this adjustment is customer-specific. More generally, this is a good way to structure data for a MNL model since it would allow different customers to choose from different sets of products.\nstep 2: we will stack (ie append) these n datasets on top of each other to create the dataset with the n*6 rows.\nStep 3: the ‘mlogit’ package that fits the MNL model requires us to create an mlogit-data object, so we’ll do that, and then we’ll feed that mlogit-data object into the mlogit() function to estimate the parameters of this model.\n\n# create an empty list to store the n datasets (each dataset will have 6 rows)\n    dat_list &lt;- vector(mode = \"list\", length = n)\npb &lt;- txtProgressBar(min = 1, max = n, style = 3)\n\n#loop for step 1\n\n    for (i in 1:n) {\n      # get cohort, minutes, brand, and size for customer i\n      i_cohort   &lt;- cust_dat |&gt; slice(i) |&gt; pull(years_ago)   \n      i_brand    &lt;- cust_dat |&gt; slice(i) |&gt; pull(brand)\n      i_size     &lt;- cust_dat |&gt; slice(i) |&gt; pull(screen_size)\n      i_discount &lt;- cust_dat |&gt; slice(i) |&gt; pull(discount)\n      i_segment  &lt;- cust_dat |&gt; slice(i) |&gt; pull(segment)\n      i_minutes  &lt;- cust_dat |&gt; slice(i) |&gt; pull(total_minutes)\n    \n      # subset the phone data to the 6 phones for the year when the customer purchased\n      PD &lt;- phone_dat |&gt; filter(years_ago == i_cohort)\n    \n      # adjust one of the phone's price for the 10% discount, if applicable\n      PD &lt;- PD |&gt; mutate(price = price - (phone_id == i_discount) * price * 0.1)\n    \n      # add customer id to PD\n      PD &lt;- PD |&gt; mutate(customer_id = i)\n    \n      # convert the one brand variable into a set of 3 brand dummies (ie, binary variables)\n      PD &lt;- PD |&gt; mutate(\n        apple = as.integer(brand == \"apple\"),\n        huawei = as.integer(brand == \"huawei\"),\n        samsung = as.integer(brand == \"samsung\")\n      )\n    \n      # create a binary variable to indicate the chosen phone\n      # this is going to be the dependent variable in the MNL model (like \"y\" in OLS)\n      PD &lt;- PD |&gt;\n        mutate(choice = (brand == i_brand) & (screen_size == i_size)) |&gt;\n        mutate(choice = as.integer(choice))\n    \n      # add segment and total_minutes\n      PD &lt;- PD |&gt; mutate(segment = i_segment, total_minutes = i_minutes)\n    \n      # store this 6-row dataset in the i'th position of that list we initialized before the loop\n      dat_list[[i]] &lt;- PD |&gt; select(\n        customer_id, phone_id, choice,\n        apple, huawei, samsung,\n        price, screen_size,\n        segment, total_minutes\n      )\n    \n    }\n    \n    # clean up -- delete temporary objects from the loop that we don't need anymore\n    rm(i, i_cohort, i_brand, i_size, i_discount, i_segment, i_minutes, PD, pb)\n    \n    # Let's take a look at two (out of the n) 6-row datasets:\n    dat_list[1]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1           1 A1            0     1      0       0   749         5.5 2      \n2           1 A2            0     1      0       0   899         5.8 2      \n3           1 S1            0     0      0       1   699         5.6 2      \n4           1 S2            0     0      0       1   799         5.9 2      \n5           1 H1            0     0      1       0   599         5.2 2      \n6           1 H2            1     0      1       0   699         5.7 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    dat_list[100]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1         100 A1            1     1      0       0   799         5.8 2      \n2         100 A2            0     1      0       0   899         6.1 2      \n3         100 S1            0     0      0       1   749         5.8 2      \n4         100 S2            0     0      0       1   849         6.3 2      \n5         100 H1            0     0      1       0   699         5.7 2      \n6         100 H2            0     0      1       0   749         6   2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n\n\n # Step 2 -----\n    \n    #Stacking the n 6-row customer-specific dataframes into one big dataframe \n#(that will have n*6 rows)\n    \n    # rbind operates on dataframes to concatenate rows\n    # Using do.call in order to concatenate rows within lists\n    mnl_dat &lt;- as_tibble(do.call(rbind, dat_list))\n    \n    rm(dat_list)\n    \n    #Data frame should looks like this \n    head(mnl_dat, n = 20)\n\n# A tibble: 20 × 10\n   customer_id phone_id choice apple huawei samsung price screen_size segment\n         &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1           1 A1            0     1      0       0  749          5.5 2      \n 2           1 A2            0     1      0       0  899          5.8 2      \n 3           1 S1            0     0      0       1  699          5.6 2      \n 4           1 S2            0     0      0       1  799          5.9 2      \n 5           1 H1            0     0      1       0  599          5.2 2      \n 6           1 H2            1     0      1       0  699          5.7 2      \n 7           2 A1            0     1      0       0  749          5.5 2      \n 8           2 A2            0     1      0       0  899          5.8 2      \n 9           2 S1            0     0      0       1  699          5.6 2      \n10           2 S2            0     0      0       1  799          5.9 2      \n11           2 H1            0     0      1       0  539.         5.2 2      \n12           2 H2            1     0      1       0  699          5.7 2      \n13           3 A1            1     1      0       0  749          5.5 3      \n14           3 A2            0     1      0       0  899          5.8 3      \n15           3 S1            0     0      0       1  699          5.6 3      \n16           3 S2            0     0      0       1  719.         5.9 3      \n17           3 H1            0     0      1       0  599          5.2 3      \n18           3 H2            0     0      1       0  699          5.7 3      \n19           4 A1            0     1      0       0  799          5.8 2      \n20           4 A2            0     1      0       0  899          6.1 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    # Then estimating demand for each year separately, since customer preferences may\n#have changed across product generations\n    \n    # Let's split the big (n*6 row) dataframe into 3 dataframes, one for each year.\n    sub1 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 1))\n    sub2 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 2))\n    sub3 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 3))\n\n\n# Step 3 -----\n\n    # converting the 3 'sub' dataframes into mlogit.data objects.\n    # To do that, need to specify the y variable (choice), whether our datasets\n    # have 6 times as many rows as the original data (shape=\"long\") or 6 times as\n    # many columns (shape=\"wide\"), and the id variable that groups the set of\n    # phones from one choice-occasion together (our \"customer_id\" variable).\n    \n    mdat1 &lt;- mlogit.data(sub1, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat2 &lt;- mlogit.data(sub2, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat3 &lt;- mlogit.data(sub3, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    \n    \n\n    \n    # Then I will use customer that bought phones last year \n    #(ie, sub1 and mdat1 where \"years_ago\" == 1) as an example.\n \n## Calculate market shares ----\n\n    \n    # Calculating product-level and brand-level market shares:\n    \n    brand_shares &lt;- cust_dat |&gt;\n                      filter(years_ago == 1) |&gt;\n                      count(brand) |&gt;\n                      mutate(shr = n / sum(n))\n    \n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    product_shares &lt;- cust_dat |&gt;\n                          filter(years_ago == 1) |&gt;\n                          count(phone_id) |&gt;\n                          mutate(shr = n / sum(n))\n    \n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n# Fit basic (brand-intercept only) model -----\n\n    # Always start simple. For the first model, I will fit a model where our \"X\"\n    # variables are just the binary dummy variables that indicate brand.\n    # to leave out one phone as a \"baseline\" and omit an intercept, so that this\n    # model is \"identified\" (ie, can be estimated).Omiting the intercept by\n    # including the bar-zero (\"|0\") in the formula:\n    \n    out1 &lt;- mlogit(choice ~ apple + samsung | 0, data = mdat1)\n    \n    summary(out1)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n3 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00849 \nsuccessive function values within tolerance limits \n\nCoefficients :\n        Estimate Std. Error z-value Pr(&gt;|z|)  \napple   0.197455   0.080071  2.4660  0.01366 *\nsamsung 0.185829   0.080281  2.3147  0.02063 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1737.8\n\n    # The coefficients for the Apple and Samsung brand dummies are\n    # positive and statistically significantly different from zero. Those are in\n    # comparison to the Huawai coefficient which is restricted to zero for identification.\n    # But what do those parameters mean?\n    \n    # Then using these coefficients to calculate the model's estimate of brand-level\n    # market shares. \n\n        # print the coefficients\n        coef(out1)\n\n    apple   samsung \n0.1974553 0.1858286 \n\n        # print the brand market shares estimated from the model\n        coefs1 &lt;- c(huawei = 0, coef(out1))\n        shares1 &lt;- exp(coefs1) / sum(exp(coefs1)) # 𝑒^Vi/∑_j𝑒^Vj\n        round(shares1, 3)\n\n huawei   apple samsung \n  0.292   0.356   0.352 \n\n        # print the actual brand market shares\n        brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n        # print the actual product market shares\n        product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n        # clean up\n        rm(coefs1, shares1)\n        \n        \n    # The model fits the intercepts in order to **exactly**\n    # match the brand-level market shares from the data. However, it does not match\n    # the product-level market shares.\n    \n    # Second, and this is more subtle but general, the sign and magnitude of the\n    # coefficients inform of us the impact on estimated market shares: larger positive\n    # coefficients predict larger market shares. Apple has the largest coefficient\n    # and thus the largest estimated market share.\n\n    # More illustration, calculate two measures of model fit/performance.\n    # using custom functions to make the calculations easy to repeat.\n\n\n\n# Model Fit Functions -----\n\n    # The first is the \"hit rate\" which is the percent of choices the model\n    # correctly predicts. Creating custom functions for the brand hit rate\n    # and the product hit rate. This measure is something that probably\n    # commonly encounter in industry according to text book, as it has a straightforward interpretation.\n    \n    brand_hit_rate &lt;- function(data, model) {\n        # here I use the model to predict which phone maximizes each customer's utility\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        # here I construct a vector of customer choices for comparisons to predictions\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        # here I compare the model's predictions to the data\n        mean(ceiling(preds / 2) == ceiling(actuals / 2))\n    }\n    \n    # now do the same steps but at the phone level\n    product_hit_rate &lt;- function(data, model) {\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        mean(preds == actuals)\n    }\n    \n    # The second measure of model fit is the likelihood ratio index \n    # (also called McFadden's pseudo # R-squared). \n    # Like R^2 from linear regression, this metric ranges from\n    # zero to one, and the interpretation is the degree of improvement over the\n    # random guessing about consumer choices\n    \n    ll_ratio &lt;- function(data, model) {\n        N &lt;- nrow(model$probabilities)\n        J &lt;- ncol(model$probabilities)\n        ll0 &lt;- N * log(1 / J)   # this is the null model for comparison\n        ll1 &lt;- as.numeric(model$logLik)   # this is lnL(beta) from slides\n        1 - ll1 / ll0\n    }\n    \n\n    \n    \n    # Then calculating the brand hit rate and the likelihood ratio index for\n    # mnl model. \n    brand_hit_rate(mdat1, out1)\n\n[1] 0.3559671\n\n    product_hit_rate(mdat1, out1)\n\n[1] 0.2397119\n\n    ll_ratio(mdat1, out1)\n\n[1] 0.002181383\n\n    # The simple/naive \"model\" is that each brand is chosen 33.3% (=1/3).\n    # The brand hit rate is about 35.6%, which is just a bit better than the naive approach.\n    # The likelihood ratio index confirms that the model is not much better than random guessing.\n    \n    # One way can improve a model's performance is to give it more complete data \n    # (ie, more variables). \n\n\n\n\n\n# Add Price -----\n\n    # Adding the price variable to the model and see what happens:\n    \n    out2 &lt;- mlogit(choice ~ apple + samsung + price | 0, data = mdat1)\n    \n    summary(out2)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000129 \nsuccessive function values within tolerance limits \n\nCoefficients :\n           Estimate  Std. Error  z-value  Pr(&gt;|z|)    \napple    1.05865609  0.10876661   9.7333 &lt; 2.2e-16 ***\nsamsung  0.63450323  0.08959903   7.0816 1.425e-12 ***\nprice   -0.00619721  0.00058165 -10.6545 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1677"
  },
  {
    "objectID": "blog/project5/index.html#some-observations",
    "href": "blog/project5/index.html#some-observations",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "Firstly, it is observed that the coefficients for Apple and Samsung have significantly increased. This suggests that, when price is held constant, consumers demonstrate a stronger preference for Apple and Samsung over Huawei. Market share data indicates that Apple and Samsung dominate over Huawei, which can be attributed to both brand loyalty and pricing strategies. Huawei’s devices, often priced lower than those of Apple and Samsung, achieve competitive market shares due in part to a positive brand perception of Apple and Samsung. This is counterbalanced by the negative impact of their higher pricing.\nSecondly, the negative coefficient associated with price implies that higher prices result in decreased utility for consumers and, consequently, lower market shares. This relationship is intuitive and aligns with economic principles.\nThirdly, the presence of smaller p-values in the analysis may indicate that the current model provides a better fit to the observed data compared to previous models."
  },
  {
    "objectID": "blog/project5/index.html#testing-for-the-better-fitting-hypothesis",
    "href": "blog/project5/index.html#testing-for-the-better-fitting-hypothesis",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "# Let's test that \"better fitting\" hypothesis by calculating the hit rates and\n    # likelihood ratio index for this model.\n\n        brand_hit_rate(mdat1, out2)\n\n[1] 0.3549383\n\n        product_hit_rate(mdat1, out2)\n\n[1] 0.2479424\n\n        ll_ratio(mdat1, out2)\n\n[1] 0.03709335\n\n    # Got a small improvement in brand hit rate which is now 35.6%, compared to\n    # the prior model's brand hit rate of 35.5%.\n    \n    # Got a product hit rate of 24.8%, better than simpler model's product hit rate \n    # of 24.0%\n    \n    # That improvement is noticeable in the likelihood ratio statistic. .037 is much better\n    # than our previous fit of .002\n    \n    \n    # Let's see what has happened to the brands' market share predictions\n    # First need to predict phone shares, then sum over phones to predict brand shares\n\n        #predict phone shares\n        shares2p &lt;- colMeans(predict(out2, newdata = mdat1))\n        names(shares2p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n        \n        #sum over phones to predict brand shares\n        shares2b &lt;- colSums(matrix(shares2p, nrow = 2))\n        names(shares2b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares2b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # ...still exactly match actual brand-level market shares\n    \n    round(shares2p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.254 0.102 0.227 0.124 0.168 0.124 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # ...and now I have product-level market share estimates that better reflect\n    # the actual product-level market shares, albeit not perfectly.\n    # That's probably because I don't have any product-specific attributes or dummies.\n\n\n\n\n\n# Add Size -----\n\n    # Improve the model further by fitting MNL with brand, price, and size\n    \n    out3 &lt;- mlogit(choice ~ apple + samsung + price + screen_size | 0, data = mdat1)\n    \n    summary(out3)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price + screen_size | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000119 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error z-value  Pr(&gt;|z|)    \napple        0.9983423  0.1443294  6.9171 4.610e-12 ***\nsamsung      0.6357234  0.0894121  7.1100 1.160e-12 ***\nprice       -0.0056366  0.0010562 -5.3366 9.473e-08 ***\nscreen_size -0.1416883  0.2226515 -0.6364    0.5245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1676.8\n\n    brand_hit_rate(mdat1, out3)\n\n[1] 0.3549383\n\n    product_hit_rate(mdat1, out3)\n\n[1] 0.2479424\n\n    ll_ratio(mdat1, out3)\n\n[1] 0.03721033"
  },
  {
    "objectID": "blog/project5/index.html#some-observations-1",
    "href": "blog/project5/index.html#some-observations-1",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "The “size” variable’s coefficient lacks statistical significance, and the hit rates remain virtually the same. This indicates that the “size” variable might not be contributing significantly to the model. The likely reason could be the high correlation between screen size and price, suggesting that the price variable may already account for most of the information provided by the size variable.\nInterestingly, the coefficient for price increased from -0.006 to -0.005, implying that screen size has an influence on price that was not captured in the previous model. This could mean that the previous model suffered from endogeneity issues.\nSpecifically, the original price coefficient of -0.006 was not solely due to price but also incorporated an effect from screen size. After adjusting for screen size, the price coefficient shifted to -0.005.\nRecalling our market mapping exercise, Samsung’s large phones, which featured very big screens, had a low market share. This observation suggests that it might be beneficial to estimate intercepts specific to each phone model instead of using a common screen size parameter for all phones.\n\n# Use Product-Specific Intercepts -----\n\n    # Now fit the MNL model with product-specific intercepts and price\n    \n    # Now, instead of brand dummy variables, I will use product dummy variables.\n    # So there will be 5 dummies (one phone has to be set to 0 for identification).\n    # Notice that because size does not vary for a given phone, so cannot include\n    # it in the model because it would be perfectly collinear with the phone dummies.\n    \n    out4 &lt;- mlogit(choice ~ phone_id + price | 0, data = mdat1)\n    \n    summary(out4)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000518 \nsuccessive function values within tolerance limits \n\nCoefficients :\n             Estimate Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2  0.3324964  0.2212103  1.5031  0.132819    \nphone_idH1 -1.2251497  0.1724673 -7.1037 1.215e-12 ***\nphone_idH2 -0.9080016  0.1303339 -6.9667 3.244e-12 ***\nphone_idS1 -0.3060387  0.1124682 -2.7211  0.006506 ** \nphone_idS2 -0.5545496  0.1346713 -4.1178 3.825e-05 ***\nprice      -0.0071582  0.0012933 -5.5346 3.119e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1668.6\n\n    # Notice that many of the coefficients are negative. This is because the small\n    # Apple phone is the reference product (simply because it's listed first), so\n    # all phones with smaller market shares than Apple small have lower parameter estimates\n    \n    # Notice also that price coefficient has changed yet again to -0.007. Specifically,\n    # the screen size variable in the prior model was capturing the average effect\n    # of screen size across the 3 brands.  Now, have specified a more flexible model\n    # in which screen size and all other product-specific differences are accounted\n    # for by the product dummies\n    \n    # the fit metrics\n\n        brand_hit_rate(mdat1, out4)\n\n[1] 0.367284\n\n        product_hit_rate(mdat1, out4)\n\n[1] 0.2685185\n\n        ll_ratio(mdat1, out4)\n\n[1] 0.0419071\n\n    # The brand hit rate improved 1% from 35.5% to 36.7%.\n    # The product hit rate improved 2% from 24.8% to 26.9%.\n    # The improvement comes from the flexibility of the model to allow for different\n    # preferences for small and large phones *within* a brand.\n    # LL Ratio is now up to .042, 21x larger than the .002 in the brand-only model\n    \n    # check the brand-level market shares\n    \n    shares4p &lt;- colMeans(predict(out4, newdata = mdat1))\n    names(shares4p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n    \n    shares4b &lt;- colSums(matrix(shares4p, nrow = 2))\n    names(shares4b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares4b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # still exactly match actual brand-level market shares\n    \n    round(shares4p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.240 0.116 0.253 0.099 0.148 0.144 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # and now it's able to exactly match product-level market shares.\n\nThe next step involves enhancing our model by incorporating customer-specific heterogeneity. While the model currently performs well on an aggregate level, assessing and tailoring it to individual variations will potentially increase its accuracy and relevance for specific customers. This could involve adding parameters or features that capture unique behaviors or preferences of different customer segments"
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "This is Project 3",
    "section": "",
    "text": "K-mean clustering algoritm practical using on customer segmentation in marketing research\n\n\nThis project leverages real-world data from T-Mobile to perform an advanced customer segmentation analysis using the K-means clustering algorithm. The focus is to understand the relationship between customers’ hand sizes and their gaming durations on mobile devices, and how these insights can inform product customization for gamers.\n\n\n\nThe analysis begins with a dataset from T-Mobile containing diverse customer attributes including biometric data. The data was processed using the R programming language and libraries from the tidyverse suite.\n\n\n\nInitial data exploration was conducted using basic commands to read and summarize the data. The K-means clustering algorithm was applied to segment customers based on gaming time and hand size. This included scaling the data, determining optimal cluster centers, and iterative refinement of these centers. The results were visualized using scatter plots, illustrating the clusters and their centroids. Further analysis included unscaling the cluster centers to interpret them in the context of the original data units.\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsub &lt;- cust_dat %&gt;% select(gaming, handsize)\nhead(sub)   \n\n# A tibble: 6 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   316.     7.88\n2   442.     5.63\n3   245.     7.5 \n4   386.     5.97\n5   204.     6.71\n6   499.     7.7 \n\nggplot(sub) +\n        geom_point(aes(gaming, handsize)) + \n        theme_minimal()\n\n\n\n scl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\n    scl\n\n# A tibble: 3,000 × 2\n    gaming handsize\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.509    0.661 \n 2  1.35    -1.26  \n 3  0.0268   0.336 \n 4  0.977   -0.971 \n 5 -0.246   -0.339 \n 6  1.74     0.507 \n 7 -0.661    0.0371\n 8 -0.641   -2.13  \n 9  1.72    -0.296 \n10 -0.784   -0.569 \n# ℹ 2,990 more rows\n\n    # let's check that the scaling worked\n    \n    scl %&gt;% summarize_all(mean) %&gt;% round(3) # check means\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      0        0\n\n    scl %&gt;% summarize_all(sd)                # check std devs\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1        1\n\n     out &lt;- kmeans(scl, centers=4, nstart=10)\n      K &lt;- 4\n    D &lt;- 10\n    \n    set.seed(1234)\n    out &lt;- kmeans(scl, centers=K, nstart=D)\n str(out)\n\nList of 9\n $ cluster     : int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n $ centers     : num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"gaming\" \"handsize\"\n $ totss       : num 5998\n $ withinss    : num [1:4] 524 439 463 594\n $ tot.withinss: num 2019\n $ betweenss   : num 3979\n $ size        : int [1:4] 875 568 626 931\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n  # 3 ways to extract a list element -- returns the element\n        str(out$cluster)\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[\"cluster\"]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[1]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        # 2 related ways to subset a list into a one-element list (usually not what you want)\n        str(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[1])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n    # We can also see that out$centers is a k-by-J matrix with the coordinates of the\n    # clusters' centers\n        \n        str(out$centers)\n\n num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"gaming\" \"handsize\"\n\n        out$centers\n\n      gaming   handsize\n1 -0.7389810 -0.7204634\n2  0.9189017 -0.9874657\n3  1.0965455  0.6677762\n4 -0.6033999  0.8305672\n\n#grab the cluster membership as a variable and add it to our \n    # dataset as a factor/categorical variable\n    sub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n     sub %&gt;% count(cluster)\n\n# A tibble: 4 × 2\n  cluster     n\n  &lt;fct&gt;   &lt;int&gt;\n1 1         875\n2 2         568\n3 3         626\n4 4         931\n\n    out$size\n\n[1] 875 568 626 931\n\n     # Then, store the clusters' center locations in their own tibble/dataframe\n    centers &lt;- as_tibble(out$centers) \n    centers\n\n# A tibble: 4 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.739   -0.720\n2  0.919   -0.987\n3  1.10     0.668\n4 -0.603    0.831\n\n      # calculate mean and sd\n        SD   &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(sd)\n        MEAN &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(mean)\n        \n        SD\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   149.     1.17\n\n        MEAN\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   241.     7.11\n\n         # repeat/format the values so we can do math with centers (this is needed for line 129 below)\n        SD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        MEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \n        SD\n\n         [,1]     [,2]\n[1,] 148.6346 1.170988\n[2,] 148.6346 1.170988\n[3,] 148.6346 1.170988\n[4,] 148.6346 1.170988\n\n        MEAN\n\n        [,1]     [,2]\n[1,] 240.746 7.106557\n[2,] 240.746 7.106557\n[3,] 240.746 7.106557\n[4,] 240.746 7.106557\n\n        # unscale the centers (convert back into original units)\n        centers &lt;- centers*SD + MEAN\n        round(centers, 1)\n\n  gaming handsize\n1  130.9      6.3\n2  377.3      6.0\n3  403.7      7.9\n4  151.1      8.1\n\n        #plot the points (colored by cluster membership) and the cluster centers\n    ggplot() + \n        geom_point(data=sub,     aes(x=gaming, y=handsize, color=cluster)) + \n        geom_point(data=centers, aes(x=gaming, y=handsize), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n    # Run this function to show initial cluster points, in scaled space\n    \n        fun1 &lt;- function() { \n            # specify a starting point for the cluster centroids\n            c1 &lt;&lt;- c(gaming=-1, handsize= 2)\n            c2 &lt;&lt;- c(gaming= 1, handsize= 1)\n            c3 &lt;&lt;- c(gaming=-1, handsize=-1)\n            c4 &lt;&lt;- c(gaming= 2, handsize=-1)\n            \n            # convert to a data.frame\n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # pick colors\n            col4 &lt;- c(\"magenta\", \"green\", \"cyan\", \"purple\")\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), \n                           shape=21, fill=col4, color=\"black\", size=5) + \n                ggtitle(\"Kmeans centroids\") + \n                theme_minimal()\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun1()\n\n\n\n    # Run this function to show assignment of points\n    \n        fun2 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            # pick closest centroid as cluster to which each point is assigned\n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n                \n            \n            print(p)\n            return(invisible())\n        }\n    \n        fun2()\n\n\n\n    # run these functions a few times to show convergence\n    \n        fun3 &lt;- function() {\n            # Update cluster centers\n            c1 &lt;&lt;- apply(scl[clust==1, ], 2, mean)\n            c2 &lt;&lt;- apply(scl[clust==2, ], 2, mean)\n            c3 &lt;&lt;- apply(scl[clust==3, ], 2, mean)\n            c4 &lt;&lt;- apply(scl[clust==4, ], 2, mean)\n            \n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun4 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n        \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n# You can keep doing it and see the points moving \n        \n        \n    # clean up\n    rm(cent_dat, centers, c1, c2, c3, c4, clust)\n    \n    \n# add labels back to data\n    \n    cust_dat &lt;- cust_dat %&gt;% mutate(cluster = factor(out$cluster))\n    head(cust_dat)\n\n# A tibble: 6 × 20\n  gender height handsize   age gaming  chat  maps video social reading\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 female   71       7.88    20   316.  282.  77.0  500.   176.    93.4\n2 male     64.6     5.63    20   442.  163.  17.5  588.   104.    62.3\n3 male     66.8     7.5     20   245.  362. 143.   357.   319.    82.7\n4 male     67.3     5.97    19   386.  202.  82.7  538.   244.   117. \n5 male     68.1     6.71    19   204.  408. 144.   242.   344.    67.5\n6 male     67.9     7.7     20   499.  157.  56.6  374.   112.    48.6\n# ℹ 10 more variables: total_minutes &lt;dbl&gt;, days_ago &lt;dbl&gt;, years_ago &lt;dbl&gt;,\n#   discount &lt;chr&gt;, phone_id &lt;chr&gt;, brand &lt;chr&gt;, size_cat &lt;chr&gt;, price &lt;dbl&gt;,\n#   screen_size &lt;dbl&gt;, cluster &lt;fct&gt;\n\n# Also can do other market research\n# Profile the segments by demographics.  Specifically:\n# summarize the segments by age, gender, height, and time spent chatting\n    \n    # For numeric variables, we can simply take means. \n    # For categorical variables, we calculate a proportion by taking the mean over the number of \n    # times something is \"true\"\n    \n    cdat &lt;- cust_dat %&gt;% \n                group_by(cluster) %&gt;% \n                summarize(mean_age    = mean(age), \n                          prop_female = mean(gender==\"female\"), \n                          mean_height = mean(height),\n                          mean_chat   = mean(chat))\n    \n    # view results\n    cdat\n\n# A tibble: 4 × 5\n  cluster mean_age prop_female mean_height mean_chat\n  &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 1           20.0       0.625        63.7      324.\n2 2           20.1       0.674        62.6      197.\n3 3           20.1       0.428        69.2      197.\n4 4           20.0       0.380        69.9      309.\n\n    # We see that cluster two of the clusters chat substantially more than the other two clusters\n    # And we see that two of the clusters have a lower percentage of females than the other two clusters\n    \n    # We can plot some of these relationships\n    \n    ggplot(cdat) + \n        geom_col(aes(y=mean_chat, x=cluster, fill=cluster)) + \n        ggtitle(\"Time spent in chat apps by segment\") + \n        theme_minimal()\n\n\n\n# Similar to privious project lets fiture out what is the best K number by using elbow plot.\n    \n    # we might want more information on which to base our choice of k\n    # One thing we might do is try many different values of k, and evaluate\n    # the performance of the algorithm for each k.  Here, our performance\n    # criteria will be the within-group sum of squares (WSS) from the model.\n    # As k increases, the WSS will decrease. The question is:\n    # how fast does it decrease?\n    \n    # let's try k=1, k=2, ..., k=10\n    \n    # we'll create a vector named 'res' to store our results\n    res &lt;- vector(length=10)\n    \n    # we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"grey\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()\n\n\n\n\n\n\n\nThis Customer Segmentation Analysis project utilizes the K-means clustering algorithm to segment customers based on their gaming habits and hand size, drawing from a dataset that includes various customer attributes. Initially, the dataset is imported and displayed for preliminary inspection. The analysis proceeds by subsetting data relevant to gaming time and hand size, followed by visualization through a scatter plot to understand the distribution before clustering. The data is then standardized to neutralize scale discrepancies, ensuring that clustering reflects genuine patterns rather than differences in measurement units. K-means clustering is applied to this standardized data to determine distinct customer groups, with iterative adjustments of cluster centroids to optimize within-cluster homogeneity. This segmentation allows for targeted product development and marketing strategies, particularly for designing smartphones that cater to the ergonomic and usage preferences of gamers. The success of this approach in identifying meaningful customer segments demonstrates the versatility of K-means clustering, suggesting its applicability across different industries to meet varied market and industrial needs."
  },
  {
    "objectID": "blog/project3/index.html#section-1-objective",
    "href": "blog/project3/index.html#section-1-objective",
    "title": "This is Project 3",
    "section": "",
    "text": "This project leverages real-world data from T-Mobile to perform an advanced customer segmentation analysis using the K-means clustering algorithm. The focus is to understand the relationship between customers’ hand sizes and their gaming durations on mobile devices, and how these insights can inform product customization for gamers."
  },
  {
    "objectID": "blog/project3/index.html#section-2-data",
    "href": "blog/project3/index.html#section-2-data",
    "title": "This is Project 3",
    "section": "",
    "text": "The analysis begins with a dataset from T-Mobile containing diverse customer attributes including biometric data. The data was processed using the R programming language and libraries from the tidyverse suite."
  },
  {
    "objectID": "blog/project3/index.html#section-3-methodology",
    "href": "blog/project3/index.html#section-3-methodology",
    "title": "This is Project 3",
    "section": "",
    "text": "Initial data exploration was conducted using basic commands to read and summarize the data. The K-means clustering algorithm was applied to segment customers based on gaming time and hand size. This included scaling the data, determining optimal cluster centers, and iterative refinement of these centers. The results were visualized using scatter plots, illustrating the clusters and their centroids. Further analysis included unscaling the cluster centers to interpret them in the context of the original data units."
  },
  {
    "objectID": "blog/project3/index.html#section-4-analysis",
    "href": "blog/project3/index.html#section-4-analysis",
    "title": "This is Project 3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsub &lt;- cust_dat %&gt;% select(gaming, handsize)\nhead(sub)   \n\n# A tibble: 6 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   316.     7.88\n2   442.     5.63\n3   245.     7.5 \n4   386.     5.97\n5   204.     6.71\n6   499.     7.7 \n\nggplot(sub) +\n        geom_point(aes(gaming, handsize)) + \n        theme_minimal()\n\n\n\n scl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\n    scl\n\n# A tibble: 3,000 × 2\n    gaming handsize\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.509    0.661 \n 2  1.35    -1.26  \n 3  0.0268   0.336 \n 4  0.977   -0.971 \n 5 -0.246   -0.339 \n 6  1.74     0.507 \n 7 -0.661    0.0371\n 8 -0.641   -2.13  \n 9  1.72    -0.296 \n10 -0.784   -0.569 \n# ℹ 2,990 more rows\n\n    # let's check that the scaling worked\n    \n    scl %&gt;% summarize_all(mean) %&gt;% round(3) # check means\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      0        0\n\n    scl %&gt;% summarize_all(sd)                # check std devs\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1        1\n\n     out &lt;- kmeans(scl, centers=4, nstart=10)\n      K &lt;- 4\n    D &lt;- 10\n    \n    set.seed(1234)\n    out &lt;- kmeans(scl, centers=K, nstart=D)\n str(out)\n\nList of 9\n $ cluster     : int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n $ centers     : num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"gaming\" \"handsize\"\n $ totss       : num 5998\n $ withinss    : num [1:4] 524 439 463 594\n $ tot.withinss: num 2019\n $ betweenss   : num 3979\n $ size        : int [1:4] 875 568 626 931\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n  # 3 ways to extract a list element -- returns the element\n        str(out$cluster)\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[\"cluster\"]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[1]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        # 2 related ways to subset a list into a one-element list (usually not what you want)\n        str(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[1])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n    # We can also see that out$centers is a k-by-J matrix with the coordinates of the\n    # clusters' centers\n        \n        str(out$centers)\n\n num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"gaming\" \"handsize\"\n\n        out$centers\n\n      gaming   handsize\n1 -0.7389810 -0.7204634\n2  0.9189017 -0.9874657\n3  1.0965455  0.6677762\n4 -0.6033999  0.8305672\n\n#grab the cluster membership as a variable and add it to our \n    # dataset as a factor/categorical variable\n    sub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n     sub %&gt;% count(cluster)\n\n# A tibble: 4 × 2\n  cluster     n\n  &lt;fct&gt;   &lt;int&gt;\n1 1         875\n2 2         568\n3 3         626\n4 4         931\n\n    out$size\n\n[1] 875 568 626 931\n\n     # Then, store the clusters' center locations in their own tibble/dataframe\n    centers &lt;- as_tibble(out$centers) \n    centers\n\n# A tibble: 4 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.739   -0.720\n2  0.919   -0.987\n3  1.10     0.668\n4 -0.603    0.831\n\n      # calculate mean and sd\n        SD   &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(sd)\n        MEAN &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(mean)\n        \n        SD\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   149.     1.17\n\n        MEAN\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   241.     7.11\n\n         # repeat/format the values so we can do math with centers (this is needed for line 129 below)\n        SD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        MEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \n        SD\n\n         [,1]     [,2]\n[1,] 148.6346 1.170988\n[2,] 148.6346 1.170988\n[3,] 148.6346 1.170988\n[4,] 148.6346 1.170988\n\n        MEAN\n\n        [,1]     [,2]\n[1,] 240.746 7.106557\n[2,] 240.746 7.106557\n[3,] 240.746 7.106557\n[4,] 240.746 7.106557\n\n        # unscale the centers (convert back into original units)\n        centers &lt;- centers*SD + MEAN\n        round(centers, 1)\n\n  gaming handsize\n1  130.9      6.3\n2  377.3      6.0\n3  403.7      7.9\n4  151.1      8.1\n\n        #plot the points (colored by cluster membership) and the cluster centers\n    ggplot() + \n        geom_point(data=sub,     aes(x=gaming, y=handsize, color=cluster)) + \n        geom_point(data=centers, aes(x=gaming, y=handsize), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n    # Run this function to show initial cluster points, in scaled space\n    \n        fun1 &lt;- function() { \n            # specify a starting point for the cluster centroids\n            c1 &lt;&lt;- c(gaming=-1, handsize= 2)\n            c2 &lt;&lt;- c(gaming= 1, handsize= 1)\n            c3 &lt;&lt;- c(gaming=-1, handsize=-1)\n            c4 &lt;&lt;- c(gaming= 2, handsize=-1)\n            \n            # convert to a data.frame\n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # pick colors\n            col4 &lt;- c(\"magenta\", \"green\", \"cyan\", \"purple\")\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), \n                           shape=21, fill=col4, color=\"black\", size=5) + \n                ggtitle(\"Kmeans centroids\") + \n                theme_minimal()\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun1()\n\n\n\n    # Run this function to show assignment of points\n    \n        fun2 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            # pick closest centroid as cluster to which each point is assigned\n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n                \n            \n            print(p)\n            return(invisible())\n        }\n    \n        fun2()\n\n\n\n    # run these functions a few times to show convergence\n    \n        fun3 &lt;- function() {\n            # Update cluster centers\n            c1 &lt;&lt;- apply(scl[clust==1, ], 2, mean)\n            c2 &lt;&lt;- apply(scl[clust==2, ], 2, mean)\n            c3 &lt;&lt;- apply(scl[clust==3, ], 2, mean)\n            c4 &lt;&lt;- apply(scl[clust==4, ], 2, mean)\n            \n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun4 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n        \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n# You can keep doing it and see the points moving \n        \n        \n    # clean up\n    rm(cent_dat, centers, c1, c2, c3, c4, clust)\n    \n    \n# add labels back to data\n    \n    cust_dat &lt;- cust_dat %&gt;% mutate(cluster = factor(out$cluster))\n    head(cust_dat)\n\n# A tibble: 6 × 20\n  gender height handsize   age gaming  chat  maps video social reading\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 female   71       7.88    20   316.  282.  77.0  500.   176.    93.4\n2 male     64.6     5.63    20   442.  163.  17.5  588.   104.    62.3\n3 male     66.8     7.5     20   245.  362. 143.   357.   319.    82.7\n4 male     67.3     5.97    19   386.  202.  82.7  538.   244.   117. \n5 male     68.1     6.71    19   204.  408. 144.   242.   344.    67.5\n6 male     67.9     7.7     20   499.  157.  56.6  374.   112.    48.6\n# ℹ 10 more variables: total_minutes &lt;dbl&gt;, days_ago &lt;dbl&gt;, years_ago &lt;dbl&gt;,\n#   discount &lt;chr&gt;, phone_id &lt;chr&gt;, brand &lt;chr&gt;, size_cat &lt;chr&gt;, price &lt;dbl&gt;,\n#   screen_size &lt;dbl&gt;, cluster &lt;fct&gt;\n\n# Also can do other market research\n# Profile the segments by demographics.  Specifically:\n# summarize the segments by age, gender, height, and time spent chatting\n    \n    # For numeric variables, we can simply take means. \n    # For categorical variables, we calculate a proportion by taking the mean over the number of \n    # times something is \"true\"\n    \n    cdat &lt;- cust_dat %&gt;% \n                group_by(cluster) %&gt;% \n                summarize(mean_age    = mean(age), \n                          prop_female = mean(gender==\"female\"), \n                          mean_height = mean(height),\n                          mean_chat   = mean(chat))\n    \n    # view results\n    cdat\n\n# A tibble: 4 × 5\n  cluster mean_age prop_female mean_height mean_chat\n  &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 1           20.0       0.625        63.7      324.\n2 2           20.1       0.674        62.6      197.\n3 3           20.1       0.428        69.2      197.\n4 4           20.0       0.380        69.9      309.\n\n    # We see that cluster two of the clusters chat substantially more than the other two clusters\n    # And we see that two of the clusters have a lower percentage of females than the other two clusters\n    \n    # We can plot some of these relationships\n    \n    ggplot(cdat) + \n        geom_col(aes(y=mean_chat, x=cluster, fill=cluster)) + \n        ggtitle(\"Time spent in chat apps by segment\") + \n        theme_minimal()\n\n\n\n# Similar to privious project lets fiture out what is the best K number by using elbow plot.\n    \n    # we might want more information on which to base our choice of k\n    # One thing we might do is try many different values of k, and evaluate\n    # the performance of the algorithm for each k.  Here, our performance\n    # criteria will be the within-group sum of squares (WSS) from the model.\n    # As k increases, the WSS will decrease. The question is:\n    # how fast does it decrease?\n    \n    # let's try k=1, k=2, ..., k=10\n    \n    # we'll create a vector named 'res' to store our results\n    res &lt;- vector(length=10)\n    \n    # we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"grey\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()"
  },
  {
    "objectID": "blog/project3/index.html#section-5-summary",
    "href": "blog/project3/index.html#section-5-summary",
    "title": "This is Project 3",
    "section": "",
    "text": "This Customer Segmentation Analysis project utilizes the K-means clustering algorithm to segment customers based on their gaming habits and hand size, drawing from a dataset that includes various customer attributes. Initially, the dataset is imported and displayed for preliminary inspection. The analysis proceeds by subsetting data relevant to gaming time and hand size, followed by visualization through a scatter plot to understand the distribution before clustering. The data is then standardized to neutralize scale discrepancies, ensuring that clustering reflects genuine patterns rather than differences in measurement units. K-means clustering is applied to this standardized data to determine distinct customer groups, with iterative adjustments of cluster centroids to optimize within-cluster homogeneity. This segmentation allows for targeted product development and marketing strategies, particularly for designing smartphones that cater to the ergonomic and usage preferences of gamers. The success of this approach in identifying meaningful customer segments demonstrates the versatility of K-means clustering, suggesting its applicability across different industries to meet varied market and industrial needs."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I Clean some data\n\n\n\nI analyzed the data\n\nlibrary(tidyverse)\nmtcars |&gt; \n    ggplot(aes(x = wt, y= mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I Clean some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n\nlibrary(tidyverse)\nmtcars |&gt; \n    ggplot(aes(x = wt, y= mpg)) + geom_point()"
  },
  {
    "objectID": "resume/resume.html",
    "href": "resume/resume.html",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Benjamin Ma\n\n\n\n\n\n Ben26msh@gmail.com\n github.com/BenjaminMa\n +1 2069307372\nFor more information, please contact me via email.\n\n\n\n\n\nExperienced in statistical analysis, business mapping methods.\nExperience with business analysis and market (inbound, outbound) research with CRM and Salesforce.\nSkilled in leadership\nSkilled in R, Stata, Excel, and SQL\n\n\n\n\nThis resume was made with the R package pagedown.\nLast updated on 2024-05-20."
  },
  {
    "objectID": "resume/resume.html#contact",
    "href": "resume/resume.html#contact",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Ben26msh@gmail.com\n github.com/BenjaminMa\n +1 2069307372\nFor more information, please contact me via email."
  },
  {
    "objectID": "resume/resume.html#skills",
    "href": "resume/resume.html#skills",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Experienced in statistical analysis, business mapping methods.\nExperience with business analysis and market (inbound, outbound) research with CRM and Salesforce.\nSkilled in leadership\nSkilled in R, Stata, Excel, and SQL"
  },
  {
    "objectID": "resume/resume.html#disclaimer",
    "href": "resume/resume.html#disclaimer",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "This resume was made with the R package pagedown.\nLast updated on 2024-05-20."
  },
  {
    "objectID": "resume/resume.html#title",
    "href": "resume/resume.html#title",
    "title": "Benjamin Ma’s resume",
    "section": "Benjamin Ma",
    "text": "Benjamin Ma\n\nSenior year UC SanDiego Student\nlooking for a skill related job (Business Analyst or marketing related)."
  },
  {
    "objectID": "resume/resume.html#education",
    "href": "resume/resume.html#education",
    "title": "Benjamin Ma’s resume",
    "section": "Education",
    "text": "Education\n\nUniversity of California, San Diego\nB.S. Business Economics\nSan Diego, USA\n2022-2024\n\n\n\nUniversity of Edinburgh\nStudy Abroad\nEdinburgh, UK\n2023"
  },
  {
    "objectID": "resume/resume.html#professional-experience",
    "href": "resume/resume.html#professional-experience",
    "title": "Benjamin Ma’s resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nBusiness Analyst, intern\nCamal Group.\nSan Diego, USA\n2023\n\n\nSourcing, cleaning, and visualizing trade data from prospect.\nGenerating weekly summary report to assist make sale strategy decisions.\nOverfulfil the KPI and achieved multiple sales for company.\nReceived return offer from company."
  },
  {
    "objectID": "resume/resume.html#volunteer-experience",
    "href": "resume/resume.html#volunteer-experience",
    "title": "Benjamin Ma’s resume",
    "section": "Volunteer Experience",
    "text": "Volunteer Experience\n\nUnited Nations Volunteer Program (Goodness and Mercy Missions)\nParticipated in non-profit organization Developed marketing strategies with team. Negotiated fundrasing with government agencies and global institutions. Awarded with a certification\nNew York, USA\n11/01/2020-08/01/2021\n\n\nUnited Nations Volunteer Program (Cameroon Association of Active Youths)\n-Assisted facilitating donations from institutions -Led a safety equipment’s advisory group. -Cleaning donation data -Using data visualization tools for weekly project evaluation. -Received a certification and letter of appreciation.\nNew York, USA\n11/25/2020-03/25/2021"
  },
  {
    "objectID": "resume/resume.html#skills-1",
    "href": "resume/resume.html#skills-1",
    "title": "Benjamin Ma’s resume",
    "section": "Skills",
    "text": "Skills\nCRM, Salesforce, Microsoft-office (Excel, Word, PowerPoint), SQL, STATA, R/Rstudio."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Benjamin’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project4/index.html",
    "href": "blog/project4/index.html",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "The primary goal of this project is to utilize Principal Component Analysis (PCA) for market mapping by reducing dimensionality. This method streamlines complex data sets, extracting principal components that reveal crucial variances and trends within the market, thereby simplifying the analysis and enhancing visualization capabilities.\nThis market analysis initiative, conducted on behalf of T-Mobile, leverages data sourced directly from the company. The research aims to discern the target demographics for various phone brands within T-Mobile’s portfolio, focusing particularly on the relationship between phone size and consumer attributes. Key questions addressed in this study include:\nAre larger phones predominantly purchased by individuals with larger hands? Is there a trend towards increasing hand size over successive years? Do consumers with similar hand sizes tend to choose phones with the same screen size? Conversely, do individuals with identical hand sizes purchase phones of varying screen sizes? Over time, how does the relationship between hand size and phone size evolve?\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggrepel)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#map phones in 1D attribute space based on their screen sizes \n\n# create a small dataset with one row per phone\n    sub &lt;- cust_dat |&gt; \n            select(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            arrange(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            distinct()\n   head(sub)\n\n# A tibble: 6 × 5\n  years_ago brand   screen_size size_cat phone_id\n      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1         1 apple           6   s        A1      \n2         1 apple           6.5 l        A2      \n3         1 huawei          5.9 s        H1      \n4         1 huawei          6.3 l        H2      \n5         1 samsung         6.1 s        S1      \n6         1 samsung         6.8 l        S2      \n\n  # plot phones by size, one facet per year\n    ggplot(sub, aes(x=screen_size, y=0)) +        \n        facet_grid(rows=vars(years_ago)) +        # facet_grid to create separate plots by years_ago\n        geom_point(size=2) +                      \n        geom_hline(yintercept=0) +                # horizontal line at y=0\n        geom_text_repel(aes(label=phone_id)) +    # using ggrepel package, adds texts to points\n        theme_bw()                                # theme\n\n\n\n\n\n\n\nIn this one-dimensional analysis, several key trends are evident: Samsung’s larger phone models have consistently been significantly bigger than those offered by Apple and Huawei over the past two years. There is a noticeable annual increase in phone sizes across all brands. Not only are the phones getting larger each year, but the variability in phone sizes is also expanding. Despite these changes in size, the relative ordering of phones by size remains stable from year to year."
  },
  {
    "objectID": "blog/project4/index.html#project-overview",
    "href": "blog/project4/index.html#project-overview",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "The primary goal of this project is to utilize Principal Component Analysis (PCA) for market mapping by reducing dimensionality. This method streamlines complex data sets, extracting principal components that reveal crucial variances and trends within the market, thereby simplifying the analysis and enhancing visualization capabilities.\nThis market analysis initiative, conducted on behalf of T-Mobile, leverages data sourced directly from the company. The research aims to discern the target demographics for various phone brands within T-Mobile’s portfolio, focusing particularly on the relationship between phone size and consumer attributes. Key questions addressed in this study include:\nAre larger phones predominantly purchased by individuals with larger hands? Is there a trend towards increasing hand size over successive years? Do consumers with similar hand sizes tend to choose phones with the same screen size? Conversely, do individuals with identical hand sizes purchase phones of varying screen sizes? Over time, how does the relationship between hand size and phone size evolve?"
  },
  {
    "objectID": "blog/project4/index.html#analysis",
    "href": "blog/project4/index.html#analysis",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggrepel)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#map phones in 1D attribute space based on their screen sizes \n\n# create a small dataset with one row per phone\n    sub &lt;- cust_dat |&gt; \n            select(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            arrange(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            distinct()\n   head(sub)\n\n# A tibble: 6 × 5\n  years_ago brand   screen_size size_cat phone_id\n      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1         1 apple           6   s        A1      \n2         1 apple           6.5 l        A2      \n3         1 huawei          5.9 s        H1      \n4         1 huawei          6.3 l        H2      \n5         1 samsung         6.1 s        S1      \n6         1 samsung         6.8 l        S2      \n\n  # plot phones by size, one facet per year\n    ggplot(sub, aes(x=screen_size, y=0)) +        \n        facet_grid(rows=vars(years_ago)) +        # facet_grid to create separate plots by years_ago\n        geom_point(size=2) +                      \n        geom_hline(yintercept=0) +                # horizontal line at y=0\n        geom_text_repel(aes(label=phone_id)) +    # using ggrepel package, adds texts to points\n        theme_bw()                                # theme"
  },
  {
    "objectID": "blog/project4/index.html#summary",
    "href": "blog/project4/index.html#summary",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "In this one-dimensional analysis, several key trends are evident: Samsung’s larger phone models have consistently been significantly bigger than those offered by Apple and Huawei over the past two years. There is a noticeable annual increase in phone sizes across all brands. Not only are the phones getting larger each year, but the variability in phone sizes is also expanding. Despite these changes in size, the relative ordering of phones by size remains stable from year to year."
  },
  {
    "objectID": "blog/project4/index.html#key-questions-for-market-mapping",
    "href": "blog/project4/index.html#key-questions-for-market-mapping",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Key Questions for Market Mapping:",
    "text": "Key Questions for Market Mapping:\n\nScreen Size vs. Hand Size: What patterns emerge when correlating screen sizes with hand sizes?\nPhone Size and Hand Size Correlation: Are larger phones preferred by individuals with larger hands?\nHand Size Trends: Is there a trend of increasing hand sizes over the years?\nConsistency in Consumer Choices: Do consumers with similar hand sizes choose phones with the same screen size, or do they select varying sizes?\nChanging Size Relationships: How does the relationship between hand size and phone size change over time?"
  },
  {
    "objectID": "blog/project4/index.html#what-would-we-expect-to-see-in-a-screensize-x-handsize-map",
    "href": "blog/project4/index.html#what-would-we-expect-to-see-in-a-screensize-x-handsize-map",
    "title": "Project 4: Market Mapping through PCA",
    "section": "What would we expect to see in a screensize x handsize map?",
    "text": "What would we expect to see in a screensize x handsize map?\n\nScreen Size vs. Hand Size Correlation: We expect to see a correlation where larger phones are preferred by individuals with larger hands.\nHand Size Trends: A potential trend of increasing hand sizes over the years could be explored to see if it aligns with the trend of increasing phone screen sizes.\nConsistency in Consumer Choices: We might find that consumers with similar hand sizes consistently choose phones with similar screen sizes, or conversely, that they choose varying sizes, indicating a diverse preference range.\nChanging Size Relationships: The relationship between hand size and phone size may evolve over time, which could indicate changing consumer preferences or innovations in phone design.\n\n\n# Plot consumers' screensize vs handsize, facet by phone, for only years_ago==1\n    ggplot(cust_dat |&gt; filter(years_ago==1)) + \n        geom_histogram(aes(handsize), bins=50) + \n        facet_grid(rows=vars(screen_size)) + \n        ggtitle(\"Hand Size Distributions by Screen Size\") + \n        theme_bw()\n\n\n\n # Plot/map phones in screensize x handsize space\n    ggplot(sub, aes(x=screen_size, y=mhs)) +   # \n        geom_point() +                         #\n        facet_grid(rows=vars(years_ago)) +     # different plots for different years_ago\n        geom_smooth(method=\"lm\", se=F) +       # add linear trend (ie regression) line\n        geom_text_repel(aes(label=phone_id)) + #\n        theme_bw()                        \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "blog/project4/index.html#observations-summary",
    "href": "blog/project4/index.html#observations-summary",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Observations Summary",
    "text": "Observations Summary\nPositive Correlation: There exists a positive relationship between screen size and hand size, indicating that typically, larger screens are favored by individuals with larger hands.\nImperfect Correlation: This relationship, however, is not consistent across all observations. For instance, in the third facet of our analysis:\nBoth S1 and H2 models have comparable screen sizes, yet the H2 model is more commonly purchased by consumers with larger hands. Conversely, the S1 and A2 models have differing screen sizes but are frequently chosen by consumers with similar hand sizes. Evolving Trends: Over time, the strength of this relationship appears to be diminishing slightly, as evidenced by a gradual flattening of the trend line."
  },
  {
    "objectID": "blog/project4/index.html#pca",
    "href": "blog/project4/index.html#pca",
    "title": "Project 4: Market Mapping through PCA",
    "section": "PCA",
    "text": "PCA\n\n#use PCA to reduce our 2D (screensize, avg handsize) to 1D\npca_out1 &lt;- sub |&gt; ungroup() |&gt; filter(years_ago == 1) |&gt; select(screen_size, mhs) |&gt; prcomp()\npca_out2 &lt;- sub |&gt; ungroup() |&gt; filter(years_ago == 2) |&gt; select(screen_size, mhs) |&gt; prcomp()\npca_out3 &lt;- sub |&gt; ungroup() |&gt; filter(years_ago == 3) |&gt; select(screen_size, mhs) |&gt; prcomp()\n    \nsummary(pca_out1)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.7015 0.06153\nProportion of Variance 0.9924 0.00763\nCumulative Proportion  0.9924 1.00000\n\nsummary(pca_out2)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.5450 0.07835\nProportion of Variance 0.9798 0.02025\nCumulative Proportion  0.9798 1.00000\n\nsummary(pca_out3)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.5978 0.07243\nProportion of Variance 0.9855 0.01447\nCumulative Proportion  0.9855 1.00000\n\n # We see that one component explains the majority of variance in the data\n    # This is because mean-handsize and screen size are highly correlated\n    \n    sub |&gt; group_by(years_ago) |&gt; summarize(cor=cor(mhs, screen_size))\n\n# A tibble: 3 × 2\n  years_ago   cor\n      &lt;dbl&gt; &lt;dbl&gt;\n1         1 0.978\n2         2 0.926\n3         3 0.948\n\n    # Let's also compare the variance along the principal components to the \n    # variance alone the original dimensions\n    \n    summary(pca_out1)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.7015 0.06153\nProportion of Variance 0.9924 0.00763\nCumulative Proportion  0.9924 1.00000\n\n    sub |&gt; ungroup() |&gt; select(screen_size, mhs) |&gt; summarize_all(sd)\n\n# A tibble: 1 × 2\n  screen_size   mhs\n        &lt;dbl&gt; &lt;dbl&gt;\n1       0.376 0.526\n\n    # we see that the original data varied along both variables, while the rotated\n    # data varies mostly along the first principal component\n    \n    \n    # the rotated data is in pca$x\n    # we can use the first column of x for our 1D plot\n    \n    # first, \"extract\" x into a tibble, to make it easier to work with\n    pcs1 &lt;- as_tibble(pca_out1$x)\n    pcs2 &lt;- as_tibble(pca_out2$x)\n    pcs3 &lt;- as_tibble(pca_out3$x)\n    \n    # and append these tibbles together\n    pcs &lt;- bind_rows(pcs1, pcs2, pcs3, .id=\"years_ago\")\n    \n    # now we can plot in 1D space\n    ggplot(pcs, aes(x=PC1, y=0)) + \n        facet_grid(rows=vars(years_ago)) + \n        geom_point(size=3) + \n        geom_hline(yintercept=0) + \n        geom_text_repel(aes(label=sub$phone_id)) + \n        theme_bw()"
  },
  {
    "objectID": "blog/project4/index.html#observations-summary-1",
    "href": "blog/project4/index.html#observations-summary-1",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Observations Summary",
    "text": "Observations Summary\nDistance Between Models: The H1 model is now notably more distant from the other phone models in terms of selected attributes.\nYearly Comparison: In the first year, the A1 and S1 models were significantly further apart. In the second year, these models moved closer together. By the third year, the positioning of A1 and S1 relative to each other remained unchanged."
  },
  {
    "objectID": "blog/project4/index.html#key-findings",
    "href": "blog/project4/index.html#key-findings",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Key Findings:",
    "text": "Key Findings:\n\nPositive Correlation: The analysis confirmed a positive correlation between screen size and hand size, indicating that larger phones are typically favored by individuals with larger hands.\nInconsistent Trends: Despite the overall trend, the correlation between phone size and hand size was not uniformly observed across all models and brands. Specific instances showed that similar screen sizes could attract consumers with different hand sizes, and vice versa.\nDynamic Market Behavior: Over time, the relationship between hand size and phone size showed signs of weakening. This suggests that as the market evolves, consumer preferences may be influenced by factors other than just the physical size of the device."
  },
  {
    "objectID": "blog/project4/index.html#yearly-model-comparison",
    "href": "blog/project4/index.html#yearly-model-comparison",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Yearly Model Comparison:",
    "text": "Yearly Model Comparison:\nThe distance between specific models such as the H1 and others highlighted significant differentiation in market positioning. Additionally, the A1 and S1 models showed variable proximity over three years, indicating shifts in market strategy or consumer acceptance."
  },
  {
    "objectID": "blog/project4/index.html#strategic-implications",
    "href": "blog/project4/index.html#strategic-implications",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Strategic Implications:",
    "text": "Strategic Implications:\nThe findings from this research are instrumental for T-Mobile to tailor its product strategies. Understanding that consumer preferences are not static but evolve with time can aid in more dynamically aligning product features with consumer needs. Moreover, recognizing the diverse preferences within consumer segments can help in targeted marketing and product development efforts.\n##Future Directions:\nBased on the outcomes of this PCA-driven market mapping, further research could explore more granular consumer data, incorporate additional variables such as consumer lifestyle and usage patterns, and apply predictive analytics to forecast upcoming trends in smartphone design and functionality."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "data file:mtcars I use Kmean algorithm in this project to analyse market segmentation problems.\n\n\n\n[1] mpg Miles/(US) gallon [2] cyl Number of cylinders [3] disp Displacement (cu.in.) [4] hp Gross horsepower [5] drat Rear axle ratio [6] wt Weight (1000 lbs) [7] qsec 1/4 mile time [8] vs Engine (0 = V-shaped, 1 = straight) [9] am Transmission (0 = automatic, 1 = manual) [10] gear Number of forward gears\n\nlibrary(tidyverse)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nmtcars |&gt; \n  ggplot(aes(x = wt, y= mpg)) + geom_point()\n\n\n\nsub &lt;- mtcars %&gt;% select(wt, mpg)\nsub\n\n                       wt  mpg\nMazda RX4           2.620 21.0\nMazda RX4 Wag       2.875 21.0\nDatsun 710          2.320 22.8\nHornet 4 Drive      3.215 21.4\nHornet Sportabout   3.440 18.7\nValiant             3.460 18.1\nDuster 360          3.570 14.3\nMerc 240D           3.190 24.4\nMerc 230            3.150 22.8\nMerc 280            3.440 19.2\nMerc 280C           3.440 17.8\nMerc 450SE          4.070 16.4\nMerc 450SL          3.730 17.3\nMerc 450SLC         3.780 15.2\nCadillac Fleetwood  5.250 10.4\nLincoln Continental 5.424 10.4\nChrysler Imperial   5.345 14.7\nFiat 128            2.200 32.4\nHonda Civic         1.615 30.4\nToyota Corolla      1.835 33.9\nToyota Corona       2.465 21.5\nDodge Challenger    3.520 15.5\nAMC Javelin         3.435 15.2\nCamaro Z28          3.840 13.3\nPontiac Firebird    3.845 19.2\nFiat X1-9           1.935 27.3\nPorsche 914-2       2.140 26.0\nLotus Europa        1.513 30.4\nFord Pantera L      3.170 15.8\nFerrari Dino        2.770 19.7\nMaserati Bora       3.570 15.0\nVolvo 142E          2.780 21.4\n\nggplot(sub) +geom_point(aes(wt, mpg)) + theme_minimal()\n\n\n\nscl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\nscl\n\n# A tibble: 32 × 2\n         wt    mpg\n      &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.610    0.151\n 2 -0.350    0.151\n 3 -0.917    0.450\n 4 -0.00230  0.217\n 5  0.228   -0.231\n 6  0.248   -0.330\n 7  0.361   -0.961\n 8 -0.0278   0.715\n 9 -0.0687   0.450\n10  0.228   -0.148\n# ℹ 22 more rows\n\nscl %&gt;% summarize_all(mean) %&gt;% round(3) \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0     0\n\nscl %&gt;% summarize_all(sd)  \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n\n#save the output from the kmeans() function into an object named \"out\"\nout &lt;- kmeans(scl, centers=4, nstart=10)\nK &lt;- 4\nD &lt;- 10\n#set seed for random number generator \nset.seed(1234)\n\n\n# \"out\" is a list.  This is common with model-fitting functions in R.  To get\n# a better sense for what's included in \"out\", let's run the structure function:\nout &lt;- kmeans(scl, centers=K, nstart=D)\nstr(out)\n\nList of 9\n $ cluster     : int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n $ centers     : num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"wt\" \"mpg\"\n $ totss       : num 62\n $ withinss    : num [1:4] 1.623 2.016 1.271 0.355\n $ tot.withinss: num 5.27\n $ betweenss   : num 56.7\n $ size        : int [1:4] 6 14 9 3\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n# 3 ways to extract a list element -- returns the element\nstr(out$cluster)\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[\"cluster\"]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[1]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# 2 related ways to subset a list into a one-element list (usually not what you want)\nstr(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[1])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# out$centers is a k-by-J matrix with the coordinates of the\n# clusters' centers\n        \nstr(out$centers)\n\n num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"wt\" \"mpg\"\n\nout$centers\n\n          wt        mpg\n1 -1.3738462  1.6552394\n2  0.3846068 -0.5957617\n3 -0.4054284  0.2799348\n4  2.1691456 -1.3700619\n\n#enhancing plot by using the data above\nsub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n#comparing a count from our data to the kmeans() 'size' output\nsub %&gt;% count(cluster)\n\n  cluster  n\n1       1  6\n2       2 14\n3       3  9\n4       4  3\n\nout$size\n\n[1]  6 14  9  3\n\n#store the clusters' center locations in their own tibble/dataframe\ncenters &lt;- as_tibble(out$centers) \ncenters\n\n# A tibble: 4 × 2\n      wt    mpg\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 -1.37   1.66 \n2  0.385 -0.596\n3 -0.405  0.280\n4  2.17  -1.37 \n\n# calculate mean and sd\nSD   &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(sd)\nMEAN &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(mean)\n        \nSD\n\n         wt      mpg\n1 0.9784574 6.026948\n\nMEAN\n\n       wt      mpg\n1 3.21725 20.09062\n\n#repeat/format the values so we can do math with centers \nSD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\nMEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \nSD\n\n          [,1]     [,2]\n[1,] 0.9784574 6.026948\n[2,] 0.9784574 6.026948\n[3,] 0.9784574 6.026948\n[4,] 0.9784574 6.026948\n\nMEAN\n\n        [,1]     [,2]\n[1,] 3.21725 20.09062\n[2,] 3.21725 20.09062\n[3,] 3.21725 20.09062\n[4,] 3.21725 20.09062\n\n# unscale the centers (convert back into original units)\ncenters &lt;- centers*SD + MEAN\nround(centers, 1)\n\n   wt  mpg\n1 1.9 30.1\n2 3.6 16.5\n3 2.8 21.8\n4 5.3 11.8\n\nggplot() + \n        geom_point(data=sub,     aes(x=wt, y=mpg, color=cluster)) + \n        geom_point(data=centers, aes(x=wt, y=mpg), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n# is k=4 the right number for k?\n# let's try k=1, k=2, ..., k=10\nres &lt;- vector(length=10)\n# we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"red\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()\n\n\n\n\n\n\n\n\n# Example with \"iris\" data\n#The Iris dataset comprises measurements of iris flowers from three different \n#species: Setosa, Versicolor, and Virginica. Each sample consists of \n#four features: sepal length, sepal width, petal length, and petal width. \n#Additionally, each sample is labeled with its corresponding species.\n# load data\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# plot petal length vs petal width by species\nggplot(iris) + \ngeom_point(aes(x=Petal.Length, y=Petal.Width, col=Species))\n\n\n\n# run kmeans\nout_iris &lt;- iris %&gt;% \nselect(Petal.Length, Petal.Width) %&gt;% \nkmeans(centers = 3, nstart = 10)\n# add segment membership\niris &lt;- iris %&gt;% mutate(cluster = factor(out_iris$cluster))\n    \n# plot segmented data\nggplot(iris, aes(x=Petal.Length, y=Petal.Width)) +\ngeom_point(aes(col=cluster)) + \ngeom_point(data=as_tibble(out_iris$centers), size=4) + \ntheme_minimal()\n\n\n\n# \"confusion matrix\" -- ie, a table of actual vs predicted\niris %&gt;% select(Species, cluster) %&gt;% table()\n\n            cluster\nSpecies       1  2  3\n  setosa      0 50  0\n  versicolor  2  0 48\n  virginica  46  0  4"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "data file:mtcars I use Kmean algorithm in this project to analyse market segmentation problems."
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "[1] mpg Miles/(US) gallon [2] cyl Number of cylinders [3] disp Displacement (cu.in.) [4] hp Gross horsepower [5] drat Rear axle ratio [6] wt Weight (1000 lbs) [7] qsec 1/4 mile time [8] vs Engine (0 = V-shaped, 1 = straight) [9] am Transmission (0 = automatic, 1 = manual) [10] gear Number of forward gears\n\nlibrary(tidyverse)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nmtcars |&gt; \n  ggplot(aes(x = wt, y= mpg)) + geom_point()\n\n\n\nsub &lt;- mtcars %&gt;% select(wt, mpg)\nsub\n\n                       wt  mpg\nMazda RX4           2.620 21.0\nMazda RX4 Wag       2.875 21.0\nDatsun 710          2.320 22.8\nHornet 4 Drive      3.215 21.4\nHornet Sportabout   3.440 18.7\nValiant             3.460 18.1\nDuster 360          3.570 14.3\nMerc 240D           3.190 24.4\nMerc 230            3.150 22.8\nMerc 280            3.440 19.2\nMerc 280C           3.440 17.8\nMerc 450SE          4.070 16.4\nMerc 450SL          3.730 17.3\nMerc 450SLC         3.780 15.2\nCadillac Fleetwood  5.250 10.4\nLincoln Continental 5.424 10.4\nChrysler Imperial   5.345 14.7\nFiat 128            2.200 32.4\nHonda Civic         1.615 30.4\nToyota Corolla      1.835 33.9\nToyota Corona       2.465 21.5\nDodge Challenger    3.520 15.5\nAMC Javelin         3.435 15.2\nCamaro Z28          3.840 13.3\nPontiac Firebird    3.845 19.2\nFiat X1-9           1.935 27.3\nPorsche 914-2       2.140 26.0\nLotus Europa        1.513 30.4\nFord Pantera L      3.170 15.8\nFerrari Dino        2.770 19.7\nMaserati Bora       3.570 15.0\nVolvo 142E          2.780 21.4\n\nggplot(sub) +geom_point(aes(wt, mpg)) + theme_minimal()\n\n\n\nscl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\nscl\n\n# A tibble: 32 × 2\n         wt    mpg\n      &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.610    0.151\n 2 -0.350    0.151\n 3 -0.917    0.450\n 4 -0.00230  0.217\n 5  0.228   -0.231\n 6  0.248   -0.330\n 7  0.361   -0.961\n 8 -0.0278   0.715\n 9 -0.0687   0.450\n10  0.228   -0.148\n# ℹ 22 more rows\n\nscl %&gt;% summarize_all(mean) %&gt;% round(3) \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0     0\n\nscl %&gt;% summarize_all(sd)  \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n\n#save the output from the kmeans() function into an object named \"out\"\nout &lt;- kmeans(scl, centers=4, nstart=10)\nK &lt;- 4\nD &lt;- 10\n#set seed for random number generator \nset.seed(1234)\n\n\n# \"out\" is a list.  This is common with model-fitting functions in R.  To get\n# a better sense for what's included in \"out\", let's run the structure function:\nout &lt;- kmeans(scl, centers=K, nstart=D)\nstr(out)\n\nList of 9\n $ cluster     : int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n $ centers     : num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"wt\" \"mpg\"\n $ totss       : num 62\n $ withinss    : num [1:4] 1.623 2.016 1.271 0.355\n $ tot.withinss: num 5.27\n $ betweenss   : num 56.7\n $ size        : int [1:4] 6 14 9 3\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n# 3 ways to extract a list element -- returns the element\nstr(out$cluster)\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[\"cluster\"]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[1]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# 2 related ways to subset a list into a one-element list (usually not what you want)\nstr(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[1])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# out$centers is a k-by-J matrix with the coordinates of the\n# clusters' centers\n        \nstr(out$centers)\n\n num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"wt\" \"mpg\"\n\nout$centers\n\n          wt        mpg\n1 -1.3738462  1.6552394\n2  0.3846068 -0.5957617\n3 -0.4054284  0.2799348\n4  2.1691456 -1.3700619\n\n#enhancing plot by using the data above\nsub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n#comparing a count from our data to the kmeans() 'size' output\nsub %&gt;% count(cluster)\n\n  cluster  n\n1       1  6\n2       2 14\n3       3  9\n4       4  3\n\nout$size\n\n[1]  6 14  9  3\n\n#store the clusters' center locations in their own tibble/dataframe\ncenters &lt;- as_tibble(out$centers) \ncenters\n\n# A tibble: 4 × 2\n      wt    mpg\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 -1.37   1.66 \n2  0.385 -0.596\n3 -0.405  0.280\n4  2.17  -1.37 \n\n# calculate mean and sd\nSD   &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(sd)\nMEAN &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(mean)\n        \nSD\n\n         wt      mpg\n1 0.9784574 6.026948\n\nMEAN\n\n       wt      mpg\n1 3.21725 20.09062\n\n#repeat/format the values so we can do math with centers \nSD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\nMEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \nSD\n\n          [,1]     [,2]\n[1,] 0.9784574 6.026948\n[2,] 0.9784574 6.026948\n[3,] 0.9784574 6.026948\n[4,] 0.9784574 6.026948\n\nMEAN\n\n        [,1]     [,2]\n[1,] 3.21725 20.09062\n[2,] 3.21725 20.09062\n[3,] 3.21725 20.09062\n[4,] 3.21725 20.09062\n\n# unscale the centers (convert back into original units)\ncenters &lt;- centers*SD + MEAN\nround(centers, 1)\n\n   wt  mpg\n1 1.9 30.1\n2 3.6 16.5\n3 2.8 21.8\n4 5.3 11.8\n\nggplot() + \n        geom_point(data=sub,     aes(x=wt, y=mpg, color=cluster)) + \n        geom_point(data=centers, aes(x=wt, y=mpg), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n# is k=4 the right number for k?\n# let's try k=1, k=2, ..., k=10\nres &lt;- vector(length=10)\n# we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"red\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()"
  },
  {
    "objectID": "blog/project2/index.html#section-3-iris-dataset",
    "href": "blog/project2/index.html#section-3-iris-dataset",
    "title": "This is Project 2",
    "section": "",
    "text": "# Example with \"iris\" data\n#The Iris dataset comprises measurements of iris flowers from three different \n#species: Setosa, Versicolor, and Virginica. Each sample consists of \n#four features: sepal length, sepal width, petal length, and petal width. \n#Additionally, each sample is labeled with its corresponding species.\n# load data\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# plot petal length vs petal width by species\nggplot(iris) + \ngeom_point(aes(x=Petal.Length, y=Petal.Width, col=Species))\n\n\n\n# run kmeans\nout_iris &lt;- iris %&gt;% \nselect(Petal.Length, Petal.Width) %&gt;% \nkmeans(centers = 3, nstart = 10)\n# add segment membership\niris &lt;- iris %&gt;% mutate(cluster = factor(out_iris$cluster))\n    \n# plot segmented data\nggplot(iris, aes(x=Petal.Length, y=Petal.Width)) +\ngeom_point(aes(col=cluster)) + \ngeom_point(data=as_tibble(out_iris$centers), size=4) + \ntheme_minimal()\n\n\n\n# \"confusion matrix\" -- ie, a table of actual vs predicted\niris %&gt;% select(Species, cluster) %&gt;% table()\n\n            cluster\nSpecies       1  2  3\n  setosa      0 50  0\n  versicolor  2  0 48\n  virginica  46  0  4"
  }
]