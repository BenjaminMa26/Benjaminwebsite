[
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Project 4: Market Mapping through PCA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 5: Data Wranging for Customers Demand Estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 7: Price Optimization Using Demand Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject 8: Price Optimization Using Demand Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project5/index.html",
    "href": "blog/project5/index.html",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "This project is showing how I did data wrangling for the mlogit package to support further researching topics in later projects. it’s a boring topic and feel free to ignore it.\n\n\nThe Multinomial Logit Model (MNL) is a predictive modeling technique often used in demand prediction when the outcome of interest has more than two categorical outcomes. It is an extension of the logistic regression model to multiple classes. This model is particularly useful in scenarios like predicting customer choice among several products, transport mode selection, or any situation where individuals select from multiple options.\nHow the Multinomial Logit Model Works:\n1.  Probabilistic Framework: The model estimates the probabilities of each possible outcome as a function of the independent variables.\n2.  Utility-Based: Each choice option is assumed to have a utility associated with it, which is modeled as a linear combination of explanatory variables.\n3.  Logit Function: It uses a logit function to model the probability that a particular alternative is chosen.\nApplications in Demand Prediction:\n•   Transport Economics: Predicting the mode of transport (bus, train, car, etc.) a person might choose based on various factors like cost, time, and comfort.\n•   Marketing: Determining which product a consumer is likely to purchase based on features such as price, brand, and consumer demographics.\n•   Public Policy: Assessing the likelihood of individuals opting for different public services.\nData Wrangling for Multinomial Logit Model:\nData wrangling, or data preprocessing, is a crucial step before applying any predictive modeling technique. Here’s how you might go about it for the Multinomial Logit Model:\n1.  Data Cleaning:\n•   Missing Values: Handle missing data through imputation or removal.\n•   Outliers: Identify and treat outliers as they can skew the results.\n2.  Data Transformation:\n•   Normalization/Standardization: Scale numeric features to have a mean of zero \n  and a standard deviation of one, or transform them to range between 0 and 1.\n•   Encoding Categorical Variables: Convert categorical variables into dummy/indicator \n  variables.For instance, using one-hot encoding.\n3.  Feature Selection:\n•   Identify which features are most relevant to the prediction. This can be achieved \n  through statistical tests, domain knowledge, or machine learning feature selection \n  techniques.\n4.  Data Integration:\n•   Combine data from multiple sources to enrich the dataset. Ensure alignment \n  and compatibility between different data sources.\n5.  Feature Engineering:\n•   Create new features that can capture important information in a more useful form. \n  This might involve aggregating data, creating interaction terms, \n  or transforming variables.\n\n\n\nDemand modeling with archival data enables data-driven sales predictions at counterfactual characteristics(prices) MNL (usually with extensions) is the most popular demand model Micro founded Well behaved, estimable Based on clearly specified theoretical model of choice Demand parameter estimates may be biased when price is correlated with unobservables that drive demand. This is endogeneity. Can sometimes be dealt with.\nPrice endogeneity is a major issue in demand estimation: Suppose the data do not record all product attributes that vary and affect demand (eg, brand reputation, coolness, aesthetics, reliability, etc) if unobserved attributes affect both price and quantity, then predictors are correlated with errors, and we say endogeneity would bias demand estimates. This is just like the assumption in linear regression that cov(X,e)=0. in this modeling I am going to disregard endogeneity for simplicity. Instead of predict demand people usually see the product’s quantity response to a change in price.\n\n\n\n\n#Using T-mobile customer phone data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mlogit)\n\nLoading required package: dfidx\n\nAttaching package: 'dfidx'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(readr)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\", show_col_types = F)\nn &lt;- nrow(cust_dat)\n# replace missing 'discount' values (currently NA) with empty string (\"\")\ncust_dat &lt;- cust_dat |&gt; mutate(discount = ifelse(is.na(discount), \"\", discount))\nset.seed(1357)   \n    subk &lt;- cust_dat |&gt; select(gaming, chat, maps, video, social, reading)\n    outk &lt;- kmeans(subk, centers = 3, nstart = 10)\n    table(outk$cluster)\n\n\n   1    2    3 \n 542 1302 1156 \n\n    cust_dat$segment &lt;- factor(outk$cluster)\n    rm(subk, outk)\n# import phone attributes     \n    phone_dat &lt;- read_csv(\"./phone_dat.csv\", show_col_types = F)\n\n\n\n\nNow we need to combine these datasets. The customer data has “n” rows and for each customer, there were 6 phones available on the market when the customer purchased their phone. So we will construct a dataset that has n*6 rows. This facilitates mlogit calculating a predicted utility for each available option for each available customer\nI’m going to do this in three steps.\nStep 1: loop over customers and create a dataset of the 6 available phones for that customer. This is a very flexible way to structure the data. It will be useful to us because we may need to adjust the price of a phone if it was on discount, and this adjustment is customer-specific. More generally, this is a good way to structure data for a MNL model since it would allow different customers to choose from different sets of products.\nstep 2: we will stack (ie append) these n datasets on top of each other to create the dataset with the n*6 rows.\nStep 3: the ‘mlogit’ package that fits the MNL model requires us to create an mlogit-data object, so we’ll do that, and then we’ll feed that mlogit-data object into the mlogit() function to estimate the parameters of this model.\n\n# create an empty list to store the n datasets (each dataset will have 6 rows)\n    dat_list &lt;- vector(mode = \"list\", length = n)\npb &lt;- txtProgressBar(min = 1, max = n, style = 3)\n\n#loop for step 1\n\n    for (i in 1:n) {\n      # get cohort, minutes, brand, and size for customer i\n      i_cohort   &lt;- cust_dat |&gt; slice(i) |&gt; pull(years_ago)   \n      i_brand    &lt;- cust_dat |&gt; slice(i) |&gt; pull(brand)\n      i_size     &lt;- cust_dat |&gt; slice(i) |&gt; pull(screen_size)\n      i_discount &lt;- cust_dat |&gt; slice(i) |&gt; pull(discount)\n      i_segment  &lt;- cust_dat |&gt; slice(i) |&gt; pull(segment)\n      i_minutes  &lt;- cust_dat |&gt; slice(i) |&gt; pull(total_minutes)\n    \n      # subset the phone data to the 6 phones for the year when the customer purchased\n      PD &lt;- phone_dat |&gt; filter(years_ago == i_cohort)\n    \n      # adjust one of the phone's price for the 10% discount, if applicable\n      PD &lt;- PD |&gt; mutate(price = price - (phone_id == i_discount) * price * 0.1)\n    \n      # add customer id to PD\n      PD &lt;- PD |&gt; mutate(customer_id = i)\n    \n      # convert the one brand variable into a set of 3 brand dummies (ie, binary variables)\n      PD &lt;- PD |&gt; mutate(\n        apple = as.integer(brand == \"apple\"),\n        huawei = as.integer(brand == \"huawei\"),\n        samsung = as.integer(brand == \"samsung\")\n      )\n    \n      # create a binary variable to indicate the chosen phone\n      # this is going to be the dependent variable in the MNL model (like \"y\" in OLS)\n      PD &lt;- PD |&gt;\n        mutate(choice = (brand == i_brand) & (screen_size == i_size)) |&gt;\n        mutate(choice = as.integer(choice))\n    \n      # add segment and total_minutes\n      PD &lt;- PD |&gt; mutate(segment = i_segment, total_minutes = i_minutes)\n    \n      # store this 6-row dataset in the i'th position of that list we initialized before the loop\n      dat_list[[i]] &lt;- PD |&gt; select(\n        customer_id, phone_id, choice,\n        apple, huawei, samsung,\n        price, screen_size,\n        segment, total_minutes\n      )\n    \n    }\n    \n    # clean up -- delete temporary objects from the loop that we don't need anymore\n    rm(i, i_cohort, i_brand, i_size, i_discount, i_segment, i_minutes, PD, pb)\n    \n    # Let's take a look at two (out of the n) 6-row datasets:\n    dat_list[1]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1           1 A1            0     1      0       0   749         5.5 2      \n2           1 A2            0     1      0       0   899         5.8 2      \n3           1 S1            0     0      0       1   699         5.6 2      \n4           1 S2            0     0      0       1   799         5.9 2      \n5           1 H1            0     0      1       0   599         5.2 2      \n6           1 H2            1     0      1       0   699         5.7 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    dat_list[100]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1         100 A1            1     1      0       0   799         5.8 2      \n2         100 A2            0     1      0       0   899         6.1 2      \n3         100 S1            0     0      0       1   749         5.8 2      \n4         100 S2            0     0      0       1   849         6.3 2      \n5         100 H1            0     0      1       0   699         5.7 2      \n6         100 H2            0     0      1       0   749         6   2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n\n\n # Step 2 -----\n    \n    #Stacking the n 6-row customer-specific dataframes into one big dataframe \n#(that will have n*6 rows)\n    \n    # rbind operates on dataframes to concatenate rows\n    # Using do.call in order to concatenate rows within lists\n    mnl_dat &lt;- as_tibble(do.call(rbind, dat_list))\n    \n    rm(dat_list)\n    \n    #Data frame should looks like this \n    head(mnl_dat, n = 20)\n\n# A tibble: 20 × 10\n   customer_id phone_id choice apple huawei samsung price screen_size segment\n         &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1           1 A1            0     1      0       0  749          5.5 2      \n 2           1 A2            0     1      0       0  899          5.8 2      \n 3           1 S1            0     0      0       1  699          5.6 2      \n 4           1 S2            0     0      0       1  799          5.9 2      \n 5           1 H1            0     0      1       0  599          5.2 2      \n 6           1 H2            1     0      1       0  699          5.7 2      \n 7           2 A1            0     1      0       0  749          5.5 2      \n 8           2 A2            0     1      0       0  899          5.8 2      \n 9           2 S1            0     0      0       1  699          5.6 2      \n10           2 S2            0     0      0       1  799          5.9 2      \n11           2 H1            0     0      1       0  539.         5.2 2      \n12           2 H2            1     0      1       0  699          5.7 2      \n13           3 A1            1     1      0       0  749          5.5 3      \n14           3 A2            0     1      0       0  899          5.8 3      \n15           3 S1            0     0      0       1  699          5.6 3      \n16           3 S2            0     0      0       1  719.         5.9 3      \n17           3 H1            0     0      1       0  599          5.2 3      \n18           3 H2            0     0      1       0  699          5.7 3      \n19           4 A1            0     1      0       0  799          5.8 2      \n20           4 A2            0     1      0       0  899          6.1 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    # Then estimating demand for each year separately, since customer preferences may\n#have changed across product generations\n    \n    # Let's split the big (n*6 row) dataframe into 3 dataframes, one for each year.\n    sub1 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 1))\n    sub2 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 2))\n    sub3 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 3))\n\n\n# Step 3 -----\n\n    # converting the 3 'sub' dataframes into mlogit.data objects.\n    # To do that, need to specify the y variable (choice), whether our datasets\n    # have 6 times as many rows as the original data (shape=\"long\") or 6 times as\n    # many columns (shape=\"wide\"), and the id variable that groups the set of\n    # phones from one choice-occasion together (our \"customer_id\" variable).\n    \n    mdat1 &lt;- mlogit.data(sub1, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat2 &lt;- mlogit.data(sub2, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat3 &lt;- mlogit.data(sub3, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    \n    \n\n    \n    # Then I will use customer that bought phones last year \n    #(ie, sub1 and mdat1 where \"years_ago\" == 1) as an example.\n \n## Calculate market shares ----\n\n    \n    # Calculating product-level and brand-level market shares:\n    \n    brand_shares &lt;- cust_dat |&gt;\n                      filter(years_ago == 1) |&gt;\n                      count(brand) |&gt;\n                      mutate(shr = n / sum(n))\n    \n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    product_shares &lt;- cust_dat |&gt;\n                          filter(years_ago == 1) |&gt;\n                          count(phone_id) |&gt;\n                          mutate(shr = n / sum(n))\n    \n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n# Fit basic (brand-intercept only) model -----\n\n    # Always start simple. For the first model, I will fit a model where our \"X\"\n    # variables are just the binary dummy variables that indicate brand.\n    # to leave out one phone as a \"baseline\" and omit an intercept, so that this\n    # model is \"identified\" (ie, can be estimated).Omiting the intercept by\n    # including the bar-zero (\"|0\") in the formula:\n    \n    out1 &lt;- mlogit(choice ~ apple + samsung | 0, data = mdat1)\n    \n    summary(out1)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n3 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00849 \nsuccessive function values within tolerance limits \n\nCoefficients :\n        Estimate Std. Error z-value Pr(&gt;|z|)  \napple   0.197455   0.080071  2.4660  0.01366 *\nsamsung 0.185829   0.080281  2.3147  0.02063 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1737.8\n\n    # The coefficients for the Apple and Samsung brand dummies are\n    # positive and statistically significantly different from zero. Those are in\n    # comparison to the Huawai coefficient which is restricted to zero for identification.\n    # But what do those parameters mean?\n    \n    # Then using these coefficients to calculate the model's estimate of brand-level\n    # market shares. \n\n        # print the coefficients\n        coef(out1)\n\n    apple   samsung \n0.1974553 0.1858286 \n\n        # print the brand market shares estimated from the model\n        coefs1 &lt;- c(huawei = 0, coef(out1))\n        shares1 &lt;- exp(coefs1) / sum(exp(coefs1)) # 𝑒^Vi/∑_j𝑒^Vj\n        round(shares1, 3)\n\n huawei   apple samsung \n  0.292   0.356   0.352 \n\n        # print the actual brand market shares\n        brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n        # print the actual product market shares\n        product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n        # clean up\n        rm(coefs1, shares1)\n        \n        \n    # The model fits the intercepts in order to **exactly**\n    # match the brand-level market shares from the data. However, it does not match\n    # the product-level market shares.\n    \n    # Second, and this is more subtle but general, the sign and magnitude of the\n    # coefficients inform of us the impact on estimated market shares: larger positive\n    # coefficients predict larger market shares. Apple has the largest coefficient\n    # and thus the largest estimated market share.\n\n    # More illustration, calculate two measures of model fit/performance.\n    # using custom functions to make the calculations easy to repeat.\n\n\n\n# Model Fit Functions -----\n\n    # The first is the \"hit rate\" which is the percent of choices the model\n    # correctly predicts. Creating custom functions for the brand hit rate\n    # and the product hit rate. This measure is something that probably\n    # commonly encounter in industry according to text book, as it has a straightforward interpretation.\n    \n    brand_hit_rate &lt;- function(data, model) {\n        # here I use the model to predict which phone maximizes each customer's utility\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        # here I construct a vector of customer choices for comparisons to predictions\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        # here I compare the model's predictions to the data\n        mean(ceiling(preds / 2) == ceiling(actuals / 2))\n    }\n    \n    # now do the same steps but at the phone level\n    product_hit_rate &lt;- function(data, model) {\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        mean(preds == actuals)\n    }\n    \n    # The second measure of model fit is the likelihood ratio index \n    # (also called McFadden's pseudo # R-squared). \n    # Like R^2 from linear regression, this metric ranges from\n    # zero to one, and the interpretation is the degree of improvement over the\n    # random guessing about consumer choices\n    \n    ll_ratio &lt;- function(data, model) {\n        N &lt;- nrow(model$probabilities)\n        J &lt;- ncol(model$probabilities)\n        ll0 &lt;- N * log(1 / J)   # this is the null model for comparison\n        ll1 &lt;- as.numeric(model$logLik)   # this is lnL(beta) from slides\n        1 - ll1 / ll0\n    }\n    \n\n    \n    \n    # Then calculating the brand hit rate and the likelihood ratio index for\n    # mnl model. \n    brand_hit_rate(mdat1, out1)\n\n[1] 0.3559671\n\n    product_hit_rate(mdat1, out1)\n\n[1] 0.2397119\n\n    ll_ratio(mdat1, out1)\n\n[1] 0.002181383\n\n    # The simple/naive \"model\" is that each brand is chosen 33.3% (=1/3).\n    # The brand hit rate is about 35.6%, which is just a bit better than the naive approach.\n    # The likelihood ratio index confirms that the model is not much better than random guessing.\n    \n    # One way can improve a model's performance is to give it more complete data \n    # (ie, more variables). \n\n\n\n\n\n# Add Price -----\n\n    # Adding the price variable to the model and see what happens:\n    \n    out2 &lt;- mlogit(choice ~ apple + samsung + price | 0, data = mdat1)\n    \n    summary(out2)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000129 \nsuccessive function values within tolerance limits \n\nCoefficients :\n           Estimate  Std. Error  z-value  Pr(&gt;|z|)    \napple    1.05865609  0.10876661   9.7333 &lt; 2.2e-16 ***\nsamsung  0.63450323  0.08959903   7.0816 1.425e-12 ***\nprice   -0.00619721  0.00058165 -10.6545 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1677\n\n\n\n\n\nFirstly, it is observed that the coefficients for Apple and Samsung have significantly increased. This suggests that, when price is held constant, consumers demonstrate a stronger preference for Apple and Samsung over Huawei. Market share data indicates that Apple and Samsung dominate over Huawei, which can be attributed to both brand loyalty and pricing strategies. Huawei’s devices, often priced lower than those of Apple and Samsung, achieve competitive market shares due in part to a positive brand perception of Apple and Samsung. This is counterbalanced by the negative impact of their higher pricing.\nSecondly, the negative coefficient associated with price implies that higher prices result in decreased utility for consumers and, consequently, lower market shares. This relationship is intuitive and aligns with economic principles.\nThirdly, the presence of smaller p-values in the analysis may indicate that the current model provides a better fit to the observed data compared to previous models.\n\n\n\n\n    # Let's test that \"better fitting\" hypothesis by calculating the hit rates and\n    # likelihood ratio index for this model.\n\n        brand_hit_rate(mdat1, out2)\n\n[1] 0.3549383\n\n        product_hit_rate(mdat1, out2)\n\n[1] 0.2479424\n\n        ll_ratio(mdat1, out2)\n\n[1] 0.03709335\n\n    # Got a small improvement in brand hit rate which is now 35.6%, compared to\n    # the prior model's brand hit rate of 35.5%.\n    \n    # Got a product hit rate of 24.8%, better than simpler model's product hit rate \n    # of 24.0%\n    \n    # That improvement is noticeable in the likelihood ratio statistic. .037 is much better\n    # than our previous fit of .002\n    \n    \n    # Let's see what has happened to the brands' market share predictions\n    # First need to predict phone shares, then sum over phones to predict brand shares\n\n        #predict phone shares\n        shares2p &lt;- colMeans(predict(out2, newdata = mdat1))\n        names(shares2p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n        \n        #sum over phones to predict brand shares\n        shares2b &lt;- colSums(matrix(shares2p, nrow = 2))\n        names(shares2b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares2b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # ...still exactly match actual brand-level market shares\n    \n    round(shares2p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.254 0.102 0.227 0.124 0.168 0.124 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # ...and now I have product-level market share estimates that better reflect\n    # the actual product-level market shares, albeit not perfectly.\n    # That's probably because I don't have any product-specific attributes or dummies.\n\n\n\n\n\n# Add Size -----\n\n    # Improve the model further by fitting MNL with brand, price, and size\n    \n    out3 &lt;- mlogit(choice ~ apple + samsung + price + screen_size | 0, data = mdat1)\n    \n    summary(out3)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price + screen_size | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000119 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error z-value  Pr(&gt;|z|)    \napple        0.9983423  0.1443294  6.9171 4.610e-12 ***\nsamsung      0.6357234  0.0894121  7.1100 1.160e-12 ***\nprice       -0.0056366  0.0010562 -5.3366 9.473e-08 ***\nscreen_size -0.1416883  0.2226515 -0.6364    0.5245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1676.8\n\n    brand_hit_rate(mdat1, out3)\n\n[1] 0.3549383\n\n    product_hit_rate(mdat1, out3)\n\n[1] 0.2479424\n\n    ll_ratio(mdat1, out3)\n\n[1] 0.03721033\n\n\n\n\n\nThe “size” variable’s coefficient lacks statistical significance, and the hit rates remain virtually the same. This indicates that the “size” variable might not be contributing significantly to the model. The likely reason could be the high correlation between screen size and price, suggesting that the price variable may already account for most of the information provided by the size variable.\nInterestingly, the coefficient for price increased from -0.006 to -0.005, implying that screen size has an influence on price that was not captured in the previous model. This could mean that the previous model suffered from endogeneity issues.\nSpecifically, the original price coefficient of -0.006 was not solely due to price but also incorporated an effect from screen size. After adjusting for screen size, the price coefficient shifted to -0.005.\nRecalling our market mapping exercise, Samsung’s large phones, which featured very big screens, had a low market share. This observation suggests that it might be beneficial to estimate intercepts specific to each phone model instead of using a common screen size parameter for all phones.\n\n# Use Product-Specific Intercepts -----\n\n    # Now fit the MNL model with product-specific intercepts and price\n    \n    # Now, instead of brand dummy variables, I will use product dummy variables.\n    # So there will be 5 dummies (one phone has to be set to 0 for identification).\n    # Notice that because size does not vary for a given phone, so cannot include\n    # it in the model because it would be perfectly collinear with the phone dummies.\n    \n    out4 &lt;- mlogit(choice ~ phone_id + price | 0, data = mdat1)\n    \n    summary(out4)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000518 \nsuccessive function values within tolerance limits \n\nCoefficients :\n             Estimate Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2  0.3324964  0.2212103  1.5031  0.132819    \nphone_idH1 -1.2251497  0.1724673 -7.1037 1.215e-12 ***\nphone_idH2 -0.9080016  0.1303339 -6.9667 3.244e-12 ***\nphone_idS1 -0.3060387  0.1124682 -2.7211  0.006506 ** \nphone_idS2 -0.5545496  0.1346713 -4.1178 3.825e-05 ***\nprice      -0.0071582  0.0012933 -5.5346 3.119e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1668.6\n\n    # Notice that many of the coefficients are negative. This is because the small\n    # Apple phone is the reference product (simply because it's listed first), so\n    # all phones with smaller market shares than Apple small have lower parameter estimates\n    \n    # Notice also that price coefficient has changed yet again to -0.007. Specifically,\n    # the screen size variable in the prior model was capturing the average effect\n    # of screen size across the 3 brands.  Now, have specified a more flexible model\n    # in which screen size and all other product-specific differences are accounted\n    # for by the product dummies\n    \n    # the fit metrics\n\n        brand_hit_rate(mdat1, out4)\n\n[1] 0.367284\n\n        product_hit_rate(mdat1, out4)\n\n[1] 0.2685185\n\n        ll_ratio(mdat1, out4)\n\n[1] 0.0419071\n\n    # The brand hit rate improved 1% from 35.5% to 36.7%.\n    # The product hit rate improved 2% from 24.8% to 26.9%.\n    # The improvement comes from the flexibility of the model to allow for different\n    # preferences for small and large phones *within* a brand.\n    # LL Ratio is now up to .042, 21x larger than the .002 in the brand-only model\n    \n    # check the brand-level market shares\n    \n    shares4p &lt;- colMeans(predict(out4, newdata = mdat1))\n    names(shares4p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n    \n    shares4b &lt;- colSums(matrix(shares4p, nrow = 2))\n    names(shares4b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares4b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # still exactly match actual brand-level market shares\n    \n    round(shares4p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.240 0.116 0.253 0.099 0.148 0.144 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # and now it's able to exactly match product-level market shares.\n\nThe next step involves enhancing our model by incorporating customer-specific heterogeneity. While the model currently performs well on an aggregate level, assessing and tailoring it to individual variations will potentially increase its accuracy and relevance for specific customers. This could involve adding parameters or features that capture unique behaviors or preferences of different customer segments"
  },
  {
    "objectID": "blog/project5/index.html#introduction",
    "href": "blog/project5/index.html#introduction",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "The Multinomial Logit Model (MNL) is a predictive modeling technique often used in demand prediction when the outcome of interest has more than two categorical outcomes. It is an extension of the logistic regression model to multiple classes. This model is particularly useful in scenarios like predicting customer choice among several products, transport mode selection, or any situation where individuals select from multiple options.\nHow the Multinomial Logit Model Works:\n1.  Probabilistic Framework: The model estimates the probabilities of each possible outcome as a function of the independent variables.\n2.  Utility-Based: Each choice option is assumed to have a utility associated with it, which is modeled as a linear combination of explanatory variables.\n3.  Logit Function: It uses a logit function to model the probability that a particular alternative is chosen.\nApplications in Demand Prediction:\n•   Transport Economics: Predicting the mode of transport (bus, train, car, etc.) a person might choose based on various factors like cost, time, and comfort.\n•   Marketing: Determining which product a consumer is likely to purchase based on features such as price, brand, and consumer demographics.\n•   Public Policy: Assessing the likelihood of individuals opting for different public services.\nData Wrangling for Multinomial Logit Model:\nData wrangling, or data preprocessing, is a crucial step before applying any predictive modeling technique. Here’s how you might go about it for the Multinomial Logit Model:\n1.  Data Cleaning:\n•   Missing Values: Handle missing data through imputation or removal.\n•   Outliers: Identify and treat outliers as they can skew the results.\n2.  Data Transformation:\n•   Normalization/Standardization: Scale numeric features to have a mean of zero \n  and a standard deviation of one, or transform them to range between 0 and 1.\n•   Encoding Categorical Variables: Convert categorical variables into dummy/indicator \n  variables.For instance, using one-hot encoding.\n3.  Feature Selection:\n•   Identify which features are most relevant to the prediction. This can be achieved \n  through statistical tests, domain knowledge, or machine learning feature selection \n  techniques.\n4.  Data Integration:\n•   Combine data from multiple sources to enrich the dataset. Ensure alignment \n  and compatibility between different data sources.\n5.  Feature Engineering:\n•   Create new features that can capture important information in a more useful form. \n  This might involve aggregating data, creating interaction terms, \n  or transforming variables."
  },
  {
    "objectID": "blog/project5/index.html#project-overview",
    "href": "blog/project5/index.html#project-overview",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "Demand modeling with archival data enables data-driven sales predictions at counterfactual characteristics(prices) MNL (usually with extensions) is the most popular demand model Micro founded Well behaved, estimable Based on clearly specified theoretical model of choice Demand parameter estimates may be biased when price is correlated with unobservables that drive demand. This is endogeneity. Can sometimes be dealt with.\nPrice endogeneity is a major issue in demand estimation: Suppose the data do not record all product attributes that vary and affect demand (eg, brand reputation, coolness, aesthetics, reliability, etc) if unobserved attributes affect both price and quantity, then predictors are correlated with errors, and we say endogeneity would bias demand estimates. This is just like the assumption in linear regression that cov(X,e)=0. in this modeling I am going to disregard endogeneity for simplicity. Instead of predict demand people usually see the product’s quantity response to a change in price."
  },
  {
    "objectID": "blog/project5/index.html#import-data",
    "href": "blog/project5/index.html#import-data",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "#Using T-mobile customer phone data\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mlogit)\n\nLoading required package: dfidx\n\nAttaching package: 'dfidx'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(readr)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\", show_col_types = F)\nn &lt;- nrow(cust_dat)\n# replace missing 'discount' values (currently NA) with empty string (\"\")\ncust_dat &lt;- cust_dat |&gt; mutate(discount = ifelse(is.na(discount), \"\", discount))\nset.seed(1357)   \n    subk &lt;- cust_dat |&gt; select(gaming, chat, maps, video, social, reading)\n    outk &lt;- kmeans(subk, centers = 3, nstart = 10)\n    table(outk$cluster)\n\n\n   1    2    3 \n 542 1302 1156 \n\n    cust_dat$segment &lt;- factor(outk$cluster)\n    rm(subk, outk)\n# import phone attributes     \n    phone_dat &lt;- read_csv(\"./phone_dat.csv\", show_col_types = F)"
  },
  {
    "objectID": "blog/project5/index.html#create-dataset-for-mnl",
    "href": "blog/project5/index.html#create-dataset-for-mnl",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "Now we need to combine these datasets. The customer data has “n” rows and for each customer, there were 6 phones available on the market when the customer purchased their phone. So we will construct a dataset that has n*6 rows. This facilitates mlogit calculating a predicted utility for each available option for each available customer\nI’m going to do this in three steps.\nStep 1: loop over customers and create a dataset of the 6 available phones for that customer. This is a very flexible way to structure the data. It will be useful to us because we may need to adjust the price of a phone if it was on discount, and this adjustment is customer-specific. More generally, this is a good way to structure data for a MNL model since it would allow different customers to choose from different sets of products.\nstep 2: we will stack (ie append) these n datasets on top of each other to create the dataset with the n*6 rows.\nStep 3: the ‘mlogit’ package that fits the MNL model requires us to create an mlogit-data object, so we’ll do that, and then we’ll feed that mlogit-data object into the mlogit() function to estimate the parameters of this model.\n\n# create an empty list to store the n datasets (each dataset will have 6 rows)\n    dat_list &lt;- vector(mode = \"list\", length = n)\npb &lt;- txtProgressBar(min = 1, max = n, style = 3)\n\n#loop for step 1\n\n    for (i in 1:n) {\n      # get cohort, minutes, brand, and size for customer i\n      i_cohort   &lt;- cust_dat |&gt; slice(i) |&gt; pull(years_ago)   \n      i_brand    &lt;- cust_dat |&gt; slice(i) |&gt; pull(brand)\n      i_size     &lt;- cust_dat |&gt; slice(i) |&gt; pull(screen_size)\n      i_discount &lt;- cust_dat |&gt; slice(i) |&gt; pull(discount)\n      i_segment  &lt;- cust_dat |&gt; slice(i) |&gt; pull(segment)\n      i_minutes  &lt;- cust_dat |&gt; slice(i) |&gt; pull(total_minutes)\n    \n      # subset the phone data to the 6 phones for the year when the customer purchased\n      PD &lt;- phone_dat |&gt; filter(years_ago == i_cohort)\n    \n      # adjust one of the phone's price for the 10% discount, if applicable\n      PD &lt;- PD |&gt; mutate(price = price - (phone_id == i_discount) * price * 0.1)\n    \n      # add customer id to PD\n      PD &lt;- PD |&gt; mutate(customer_id = i)\n    \n      # convert the one brand variable into a set of 3 brand dummies (ie, binary variables)\n      PD &lt;- PD |&gt; mutate(\n        apple = as.integer(brand == \"apple\"),\n        huawei = as.integer(brand == \"huawei\"),\n        samsung = as.integer(brand == \"samsung\")\n      )\n    \n      # create a binary variable to indicate the chosen phone\n      # this is going to be the dependent variable in the MNL model (like \"y\" in OLS)\n      PD &lt;- PD |&gt;\n        mutate(choice = (brand == i_brand) & (screen_size == i_size)) |&gt;\n        mutate(choice = as.integer(choice))\n    \n      # add segment and total_minutes\n      PD &lt;- PD |&gt; mutate(segment = i_segment, total_minutes = i_minutes)\n    \n      # store this 6-row dataset in the i'th position of that list we initialized before the loop\n      dat_list[[i]] &lt;- PD |&gt; select(\n        customer_id, phone_id, choice,\n        apple, huawei, samsung,\n        price, screen_size,\n        segment, total_minutes\n      )\n    \n    }\n    \n    # clean up -- delete temporary objects from the loop that we don't need anymore\n    rm(i, i_cohort, i_brand, i_size, i_discount, i_segment, i_minutes, PD, pb)\n    \n    # Let's take a look at two (out of the n) 6-row datasets:\n    dat_list[1]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1           1 A1            0     1      0       0   749         5.5 2      \n2           1 A2            0     1      0       0   899         5.8 2      \n3           1 S1            0     0      0       1   699         5.6 2      \n4           1 S2            0     0      0       1   799         5.9 2      \n5           1 H1            0     0      1       0   599         5.2 2      \n6           1 H2            1     0      1       0   699         5.7 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    dat_list[100]\n\n[[1]]\n# A tibble: 6 × 10\n  customer_id phone_id choice apple huawei samsung price screen_size segment\n        &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n1         100 A1            1     1      0       0   799         5.8 2      \n2         100 A2            0     1      0       0   899         6.1 2      \n3         100 S1            0     0      0       1   749         5.8 2      \n4         100 S2            0     0      0       1   849         6.3 2      \n5         100 H1            0     0      1       0   699         5.7 2      \n6         100 H2            0     0      1       0   749         6   2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n\n\n # Step 2 -----\n    \n    #Stacking the n 6-row customer-specific dataframes into one big dataframe \n#(that will have n*6 rows)\n    \n    # rbind operates on dataframes to concatenate rows\n    # Using do.call in order to concatenate rows within lists\n    mnl_dat &lt;- as_tibble(do.call(rbind, dat_list))\n    \n    rm(dat_list)\n    \n    #Data frame should looks like this \n    head(mnl_dat, n = 20)\n\n# A tibble: 20 × 10\n   customer_id phone_id choice apple huawei samsung price screen_size segment\n         &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1           1 A1            0     1      0       0  749          5.5 2      \n 2           1 A2            0     1      0       0  899          5.8 2      \n 3           1 S1            0     0      0       1  699          5.6 2      \n 4           1 S2            0     0      0       1  799          5.9 2      \n 5           1 H1            0     0      1       0  599          5.2 2      \n 6           1 H2            1     0      1       0  699          5.7 2      \n 7           2 A1            0     1      0       0  749          5.5 2      \n 8           2 A2            0     1      0       0  899          5.8 2      \n 9           2 S1            0     0      0       1  699          5.6 2      \n10           2 S2            0     0      0       1  799          5.9 2      \n11           2 H1            0     0      1       0  539.         5.2 2      \n12           2 H2            1     0      1       0  699          5.7 2      \n13           3 A1            1     1      0       0  749          5.5 3      \n14           3 A2            0     1      0       0  899          5.8 3      \n15           3 S1            0     0      0       1  699          5.6 3      \n16           3 S2            0     0      0       1  719.         5.9 3      \n17           3 H1            0     0      1       0  599          5.2 3      \n18           3 H2            0     0      1       0  699          5.7 3      \n19           4 A1            0     1      0       0  799          5.8 2      \n20           4 A2            0     1      0       0  899          6.1 2      \n# ℹ 1 more variable: total_minutes &lt;dbl&gt;\n\n    # Then estimating demand for each year separately, since customer preferences may\n#have changed across product generations\n    \n    # Let's split the big (n*6 row) dataframe into 3 dataframes, one for each year.\n    sub1 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 1))\n    sub2 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 2))\n    sub3 &lt;- mnl_dat |&gt; filter(customer_id %in% which(cust_dat$years_ago == 3))\n\n\n# Step 3 -----\n\n    # converting the 3 'sub' dataframes into mlogit.data objects.\n    # To do that, need to specify the y variable (choice), whether our datasets\n    # have 6 times as many rows as the original data (shape=\"long\") or 6 times as\n    # many columns (shape=\"wide\"), and the id variable that groups the set of\n    # phones from one choice-occasion together (our \"customer_id\" variable).\n    \n    mdat1 &lt;- mlogit.data(sub1, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat2 &lt;- mlogit.data(sub2, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    mdat3 &lt;- mlogit.data(sub3, choice = \"choice\", shape = \"long\", chid.var = \"customer_id\")\n    \n    \n\n    \n    # Then I will use customer that bought phones last year \n    #(ie, sub1 and mdat1 where \"years_ago\" == 1) as an example.\n \n## Calculate market shares ----\n\n    \n    # Calculating product-level and brand-level market shares:\n    \n    brand_shares &lt;- cust_dat |&gt;\n                      filter(years_ago == 1) |&gt;\n                      count(brand) |&gt;\n                      mutate(shr = n / sum(n))\n    \n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    product_shares &lt;- cust_dat |&gt;\n                          filter(years_ago == 1) |&gt;\n                          count(phone_id) |&gt;\n                          mutate(shr = n / sum(n))\n    \n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n# Fit basic (brand-intercept only) model -----\n\n    # Always start simple. For the first model, I will fit a model where our \"X\"\n    # variables are just the binary dummy variables that indicate brand.\n    # to leave out one phone as a \"baseline\" and omit an intercept, so that this\n    # model is \"identified\" (ie, can be estimated).Omiting the intercept by\n    # including the bar-zero (\"|0\") in the formula:\n    \n    out1 &lt;- mlogit(choice ~ apple + samsung | 0, data = mdat1)\n    \n    summary(out1)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n3 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00849 \nsuccessive function values within tolerance limits \n\nCoefficients :\n        Estimate Std. Error z-value Pr(&gt;|z|)  \napple   0.197455   0.080071  2.4660  0.01366 *\nsamsung 0.185829   0.080281  2.3147  0.02063 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1737.8\n\n    # The coefficients for the Apple and Samsung brand dummies are\n    # positive and statistically significantly different from zero. Those are in\n    # comparison to the Huawai coefficient which is restricted to zero for identification.\n    # But what do those parameters mean?\n    \n    # Then using these coefficients to calculate the model's estimate of brand-level\n    # market shares. \n\n        # print the coefficients\n        coef(out1)\n\n    apple   samsung \n0.1974553 0.1858286 \n\n        # print the brand market shares estimated from the model\n        coefs1 &lt;- c(huawei = 0, coef(out1))\n        shares1 &lt;- exp(coefs1) / sum(exp(coefs1)) # 𝑒^Vi/∑_j𝑒^Vj\n        round(shares1, 3)\n\n huawei   apple samsung \n  0.292   0.356   0.352 \n\n        # print the actual brand market shares\n        brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n        # print the actual product market shares\n        product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n        # clean up\n        rm(coefs1, shares1)\n        \n        \n    # The model fits the intercepts in order to **exactly**\n    # match the brand-level market shares from the data. However, it does not match\n    # the product-level market shares.\n    \n    # Second, and this is more subtle but general, the sign and magnitude of the\n    # coefficients inform of us the impact on estimated market shares: larger positive\n    # coefficients predict larger market shares. Apple has the largest coefficient\n    # and thus the largest estimated market share.\n\n    # More illustration, calculate two measures of model fit/performance.\n    # using custom functions to make the calculations easy to repeat.\n\n\n\n# Model Fit Functions -----\n\n    # The first is the \"hit rate\" which is the percent of choices the model\n    # correctly predicts. Creating custom functions for the brand hit rate\n    # and the product hit rate. This measure is something that probably\n    # commonly encounter in industry according to text book, as it has a straightforward interpretation.\n    \n    brand_hit_rate &lt;- function(data, model) {\n        # here I use the model to predict which phone maximizes each customer's utility\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        # here I construct a vector of customer choices for comparisons to predictions\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        # here I compare the model's predictions to the data\n        mean(ceiling(preds / 2) == ceiling(actuals / 2))\n    }\n    \n    # now do the same steps but at the phone level\n    product_hit_rate &lt;- function(data, model) {\n        preds &lt;- apply(predict(model, newdata = data), 1, which.max)\n        actuals &lt;- apply(matrix(data$choice, ncol = 6, byrow = T), 1, which.max)\n        mean(preds == actuals)\n    }\n    \n    # The second measure of model fit is the likelihood ratio index \n    # (also called McFadden's pseudo # R-squared). \n    # Like R^2 from linear regression, this metric ranges from\n    # zero to one, and the interpretation is the degree of improvement over the\n    # random guessing about consumer choices\n    \n    ll_ratio &lt;- function(data, model) {\n        N &lt;- nrow(model$probabilities)\n        J &lt;- ncol(model$probabilities)\n        ll0 &lt;- N * log(1 / J)   # this is the null model for comparison\n        ll1 &lt;- as.numeric(model$logLik)   # this is lnL(beta) from slides\n        1 - ll1 / ll0\n    }\n    \n\n    \n    \n    # Then calculating the brand hit rate and the likelihood ratio index for\n    # mnl model. \n    brand_hit_rate(mdat1, out1)\n\n[1] 0.3559671\n\n    product_hit_rate(mdat1, out1)\n\n[1] 0.2397119\n\n    ll_ratio(mdat1, out1)\n\n[1] 0.002181383\n\n    # The simple/naive \"model\" is that each brand is chosen 33.3% (=1/3).\n    # The brand hit rate is about 35.6%, which is just a bit better than the naive approach.\n    # The likelihood ratio index confirms that the model is not much better than random guessing.\n    \n    # One way can improve a model's performance is to give it more complete data \n    # (ie, more variables). \n\n\n\n\n\n# Add Price -----\n\n    # Adding the price variable to the model and see what happens:\n    \n    out2 &lt;- mlogit(choice ~ apple + samsung + price | 0, data = mdat1)\n    \n    summary(out2)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000129 \nsuccessive function values within tolerance limits \n\nCoefficients :\n           Estimate  Std. Error  z-value  Pr(&gt;|z|)    \napple    1.05865609  0.10876661   9.7333 &lt; 2.2e-16 ***\nsamsung  0.63450323  0.08959903   7.0816 1.425e-12 ***\nprice   -0.00619721  0.00058165 -10.6545 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1677"
  },
  {
    "objectID": "blog/project5/index.html#some-observations",
    "href": "blog/project5/index.html#some-observations",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "Firstly, it is observed that the coefficients for Apple and Samsung have significantly increased. This suggests that, when price is held constant, consumers demonstrate a stronger preference for Apple and Samsung over Huawei. Market share data indicates that Apple and Samsung dominate over Huawei, which can be attributed to both brand loyalty and pricing strategies. Huawei’s devices, often priced lower than those of Apple and Samsung, achieve competitive market shares due in part to a positive brand perception of Apple and Samsung. This is counterbalanced by the negative impact of their higher pricing.\nSecondly, the negative coefficient associated with price implies that higher prices result in decreased utility for consumers and, consequently, lower market shares. This relationship is intuitive and aligns with economic principles.\nThirdly, the presence of smaller p-values in the analysis may indicate that the current model provides a better fit to the observed data compared to previous models."
  },
  {
    "objectID": "blog/project5/index.html#testing-for-the-better-fitting-hypothesis",
    "href": "blog/project5/index.html#testing-for-the-better-fitting-hypothesis",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "# Let's test that \"better fitting\" hypothesis by calculating the hit rates and\n    # likelihood ratio index for this model.\n\n        brand_hit_rate(mdat1, out2)\n\n[1] 0.3549383\n\n        product_hit_rate(mdat1, out2)\n\n[1] 0.2479424\n\n        ll_ratio(mdat1, out2)\n\n[1] 0.03709335\n\n    # Got a small improvement in brand hit rate which is now 35.6%, compared to\n    # the prior model's brand hit rate of 35.5%.\n    \n    # Got a product hit rate of 24.8%, better than simpler model's product hit rate \n    # of 24.0%\n    \n    # That improvement is noticeable in the likelihood ratio statistic. .037 is much better\n    # than our previous fit of .002\n    \n    \n    # Let's see what has happened to the brands' market share predictions\n    # First need to predict phone shares, then sum over phones to predict brand shares\n\n        #predict phone shares\n        shares2p &lt;- colMeans(predict(out2, newdata = mdat1))\n        names(shares2p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n        \n        #sum over phones to predict brand shares\n        shares2b &lt;- colSums(matrix(shares2p, nrow = 2))\n        names(shares2b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares2b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # ...still exactly match actual brand-level market shares\n    \n    round(shares2p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.254 0.102 0.227 0.124 0.168 0.124 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # ...and now I have product-level market share estimates that better reflect\n    # the actual product-level market shares, albeit not perfectly.\n    # That's probably because I don't have any product-specific attributes or dummies.\n\n\n\n\n\n# Add Size -----\n\n    # Improve the model further by fitting MNL with brand, price, and size\n    \n    out3 &lt;- mlogit(choice ~ apple + samsung + price + screen_size | 0, data = mdat1)\n    \n    summary(out3)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price + screen_size | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000119 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error z-value  Pr(&gt;|z|)    \napple        0.9983423  0.1443294  6.9171 4.610e-12 ***\nsamsung      0.6357234  0.0894121  7.1100 1.160e-12 ***\nprice       -0.0056366  0.0010562 -5.3366 9.473e-08 ***\nscreen_size -0.1416883  0.2226515 -0.6364    0.5245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1676.8\n\n    brand_hit_rate(mdat1, out3)\n\n[1] 0.3549383\n\n    product_hit_rate(mdat1, out3)\n\n[1] 0.2479424\n\n    ll_ratio(mdat1, out3)\n\n[1] 0.03721033"
  },
  {
    "objectID": "blog/project5/index.html#some-observations-1",
    "href": "blog/project5/index.html#some-observations-1",
    "title": "Project 5: Data Wranging for Customers Demand Estimation",
    "section": "",
    "text": "The “size” variable’s coefficient lacks statistical significance, and the hit rates remain virtually the same. This indicates that the “size” variable might not be contributing significantly to the model. The likely reason could be the high correlation between screen size and price, suggesting that the price variable may already account for most of the information provided by the size variable.\nInterestingly, the coefficient for price increased from -0.006 to -0.005, implying that screen size has an influence on price that was not captured in the previous model. This could mean that the previous model suffered from endogeneity issues.\nSpecifically, the original price coefficient of -0.006 was not solely due to price but also incorporated an effect from screen size. After adjusting for screen size, the price coefficient shifted to -0.005.\nRecalling our market mapping exercise, Samsung’s large phones, which featured very big screens, had a low market share. This observation suggests that it might be beneficial to estimate intercepts specific to each phone model instead of using a common screen size parameter for all phones.\n\n# Use Product-Specific Intercepts -----\n\n    # Now fit the MNL model with product-specific intercepts and price\n    \n    # Now, instead of brand dummy variables, I will use product dummy variables.\n    # So there will be 5 dummies (one phone has to be set to 0 for identification).\n    # Notice that because size does not vary for a given phone, so cannot include\n    # it in the model because it would be perfectly collinear with the phone dummies.\n    \n    out4 &lt;- mlogit(choice ~ phone_id + price | 0, data = mdat1)\n    \n    summary(out4)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000518 \nsuccessive function values within tolerance limits \n\nCoefficients :\n             Estimate Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2  0.3324964  0.2212103  1.5031  0.132819    \nphone_idH1 -1.2251497  0.1724673 -7.1037 1.215e-12 ***\nphone_idH2 -0.9080016  0.1303339 -6.9667 3.244e-12 ***\nphone_idS1 -0.3060387  0.1124682 -2.7211  0.006506 ** \nphone_idS2 -0.5545496  0.1346713 -4.1178 3.825e-05 ***\nprice      -0.0071582  0.0012933 -5.5346 3.119e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1668.6\n\n    # Notice that many of the coefficients are negative. This is because the small\n    # Apple phone is the reference product (simply because it's listed first), so\n    # all phones with smaller market shares than Apple small have lower parameter estimates\n    \n    # Notice also that price coefficient has changed yet again to -0.007. Specifically,\n    # the screen size variable in the prior model was capturing the average effect\n    # of screen size across the 3 brands.  Now, have specified a more flexible model\n    # in which screen size and all other product-specific differences are accounted\n    # for by the product dummies\n    \n    # the fit metrics\n\n        brand_hit_rate(mdat1, out4)\n\n[1] 0.367284\n\n        product_hit_rate(mdat1, out4)\n\n[1] 0.2685185\n\n        ll_ratio(mdat1, out4)\n\n[1] 0.0419071\n\n    # The brand hit rate improved 1% from 35.5% to 36.7%.\n    # The product hit rate improved 2% from 24.8% to 26.9%.\n    # The improvement comes from the flexibility of the model to allow for different\n    # preferences for small and large phones *within* a brand.\n    # LL Ratio is now up to .042, 21x larger than the .002 in the brand-only model\n    \n    # check the brand-level market shares\n    \n    shares4p &lt;- colMeans(predict(out4, newdata = mdat1))\n    names(shares4p) &lt;- sub1 |&gt; head(6) |&gt; pull(phone_id)\n    \n    shares4b &lt;- colSums(matrix(shares4p, nrow = 2))\n    names(shares4b) &lt;- c(\"apple\", \"samsung\", \"huawei\")\n    \n    round(shares4b, 3)\n\n  apple samsung  huawei \n  0.356   0.352   0.292 \n\n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    # still exactly match actual brand-level market shares\n    \n    round(shares4p, 3)\n\n   A1    A2    S1    S2    H1    H2 \n0.240 0.116 0.253 0.099 0.148 0.144 \n\n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n    # and now it's able to exactly match product-level market shares.\n\nThe next step involves enhancing our model by incorporating customer-specific heterogeneity. While the model currently performs well on an aggregate level, assessing and tailoring it to individual variations will potentially increase its accuracy and relevance for specific customers. This could involve adding parameters or features that capture unique behaviors or preferences of different customer segments"
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "This is Project 3",
    "section": "",
    "text": "K-mean clustering algoritm practical using on customer segmentation in marketing research\n\n\nThis project leverages real-world data from T-Mobile to perform an advanced customer segmentation analysis using the K-means clustering algorithm. The focus is to understand the relationship between customers’ hand sizes and their gaming durations on mobile devices, and how these insights can inform product customization for gamers.\n\n\n\nThe analysis begins with a dataset from T-Mobile containing diverse customer attributes including biometric data. The data was processed using the R programming language and libraries from the tidyverse suite.\n\n\n\nInitial data exploration was conducted using basic commands to read and summarize the data. The K-means clustering algorithm was applied to segment customers based on gaming time and hand size. This included scaling the data, determining optimal cluster centers, and iterative refinement of these centers. The results were visualized using scatter plots, illustrating the clusters and their centroids. Further analysis included unscaling the cluster centers to interpret them in the context of the original data units.\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsub &lt;- cust_dat %&gt;% select(gaming, handsize)\nhead(sub)   \n\n# A tibble: 6 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   316.     7.88\n2   442.     5.63\n3   245.     7.5 \n4   386.     5.97\n5   204.     6.71\n6   499.     7.7 \n\nggplot(sub) +\n        geom_point(aes(gaming, handsize)) + \n        theme_minimal()\n\n\n\n scl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\n    scl\n\n# A tibble: 3,000 × 2\n    gaming handsize\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.509    0.661 \n 2  1.35    -1.26  \n 3  0.0268   0.336 \n 4  0.977   -0.971 \n 5 -0.246   -0.339 \n 6  1.74     0.507 \n 7 -0.661    0.0371\n 8 -0.641   -2.13  \n 9  1.72    -0.296 \n10 -0.784   -0.569 \n# ℹ 2,990 more rows\n\n    # let's check that the scaling worked\n    \n    scl %&gt;% summarize_all(mean) %&gt;% round(3) # check means\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      0        0\n\n    scl %&gt;% summarize_all(sd)                # check std devs\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1        1\n\n     out &lt;- kmeans(scl, centers=4, nstart=10)\n      K &lt;- 4\n    D &lt;- 10\n    \n    set.seed(1234)\n    out &lt;- kmeans(scl, centers=K, nstart=D)\n str(out)\n\nList of 9\n $ cluster     : int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n $ centers     : num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"gaming\" \"handsize\"\n $ totss       : num 5998\n $ withinss    : num [1:4] 524 439 463 594\n $ tot.withinss: num 2019\n $ betweenss   : num 3979\n $ size        : int [1:4] 875 568 626 931\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n  # 3 ways to extract a list element -- returns the element\n        str(out$cluster)\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[\"cluster\"]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[1]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        # 2 related ways to subset a list into a one-element list (usually not what you want)\n        str(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[1])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n    # We can also see that out$centers is a k-by-J matrix with the coordinates of the\n    # clusters' centers\n        \n        str(out$centers)\n\n num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"gaming\" \"handsize\"\n\n        out$centers\n\n      gaming   handsize\n1 -0.7389810 -0.7204634\n2  0.9189017 -0.9874657\n3  1.0965455  0.6677762\n4 -0.6033999  0.8305672\n\n#grab the cluster membership as a variable and add it to our \n    # dataset as a factor/categorical variable\n    sub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n     sub %&gt;% count(cluster)\n\n# A tibble: 4 × 2\n  cluster     n\n  &lt;fct&gt;   &lt;int&gt;\n1 1         875\n2 2         568\n3 3         626\n4 4         931\n\n    out$size\n\n[1] 875 568 626 931\n\n     # Then, store the clusters' center locations in their own tibble/dataframe\n    centers &lt;- as_tibble(out$centers) \n    centers\n\n# A tibble: 4 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.739   -0.720\n2  0.919   -0.987\n3  1.10     0.668\n4 -0.603    0.831\n\n      # calculate mean and sd\n        SD   &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(sd)\n        MEAN &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(mean)\n        \n        SD\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   149.     1.17\n\n        MEAN\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   241.     7.11\n\n         # repeat/format the values so we can do math with centers (this is needed for line 129 below)\n        SD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        MEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \n        SD\n\n         [,1]     [,2]\n[1,] 148.6346 1.170988\n[2,] 148.6346 1.170988\n[3,] 148.6346 1.170988\n[4,] 148.6346 1.170988\n\n        MEAN\n\n        [,1]     [,2]\n[1,] 240.746 7.106557\n[2,] 240.746 7.106557\n[3,] 240.746 7.106557\n[4,] 240.746 7.106557\n\n        # unscale the centers (convert back into original units)\n        centers &lt;- centers*SD + MEAN\n        round(centers, 1)\n\n  gaming handsize\n1  130.9      6.3\n2  377.3      6.0\n3  403.7      7.9\n4  151.1      8.1\n\n        #plot the points (colored by cluster membership) and the cluster centers\n    ggplot() + \n        geom_point(data=sub,     aes(x=gaming, y=handsize, color=cluster)) + \n        geom_point(data=centers, aes(x=gaming, y=handsize), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n    # Run this function to show initial cluster points, in scaled space\n    \n        fun1 &lt;- function() { \n            # specify a starting point for the cluster centroids\n            c1 &lt;&lt;- c(gaming=-1, handsize= 2)\n            c2 &lt;&lt;- c(gaming= 1, handsize= 1)\n            c3 &lt;&lt;- c(gaming=-1, handsize=-1)\n            c4 &lt;&lt;- c(gaming= 2, handsize=-1)\n            \n            # convert to a data.frame\n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # pick colors\n            col4 &lt;- c(\"magenta\", \"green\", \"cyan\", \"purple\")\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), \n                           shape=21, fill=col4, color=\"black\", size=5) + \n                ggtitle(\"Kmeans centroids\") + \n                theme_minimal()\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun1()\n\n\n\n    # Run this function to show assignment of points\n    \n        fun2 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            # pick closest centroid as cluster to which each point is assigned\n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n                \n            \n            print(p)\n            return(invisible())\n        }\n    \n        fun2()\n\n\n\n    # run these functions a few times to show convergence\n    \n        fun3 &lt;- function() {\n            # Update cluster centers\n            c1 &lt;&lt;- apply(scl[clust==1, ], 2, mean)\n            c2 &lt;&lt;- apply(scl[clust==2, ], 2, mean)\n            c3 &lt;&lt;- apply(scl[clust==3, ], 2, mean)\n            c4 &lt;&lt;- apply(scl[clust==4, ], 2, mean)\n            \n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun4 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n        \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n# You can keep doing it and see the points moving \n        \n        \n    # clean up\n    rm(cent_dat, centers, c1, c2, c3, c4, clust)\n    \n    \n# add labels back to data\n    \n    cust_dat &lt;- cust_dat %&gt;% mutate(cluster = factor(out$cluster))\n    head(cust_dat)\n\n# A tibble: 6 × 20\n  gender height handsize   age gaming  chat  maps video social reading\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 female   71       7.88    20   316.  282.  77.0  500.   176.    93.4\n2 male     64.6     5.63    20   442.  163.  17.5  588.   104.    62.3\n3 male     66.8     7.5     20   245.  362. 143.   357.   319.    82.7\n4 male     67.3     5.97    19   386.  202.  82.7  538.   244.   117. \n5 male     68.1     6.71    19   204.  408. 144.   242.   344.    67.5\n6 male     67.9     7.7     20   499.  157.  56.6  374.   112.    48.6\n# ℹ 10 more variables: total_minutes &lt;dbl&gt;, days_ago &lt;dbl&gt;, years_ago &lt;dbl&gt;,\n#   discount &lt;chr&gt;, phone_id &lt;chr&gt;, brand &lt;chr&gt;, size_cat &lt;chr&gt;, price &lt;dbl&gt;,\n#   screen_size &lt;dbl&gt;, cluster &lt;fct&gt;\n\n# Also can do other market research\n# Profile the segments by demographics.  Specifically:\n# summarize the segments by age, gender, height, and time spent chatting\n    \n    # For numeric variables, we can simply take means. \n    # For categorical variables, we calculate a proportion by taking the mean over the number of \n    # times something is \"true\"\n    \n    cdat &lt;- cust_dat %&gt;% \n                group_by(cluster) %&gt;% \n                summarize(mean_age    = mean(age), \n                          prop_female = mean(gender==\"female\"), \n                          mean_height = mean(height),\n                          mean_chat   = mean(chat))\n    \n    # view results\n    cdat\n\n# A tibble: 4 × 5\n  cluster mean_age prop_female mean_height mean_chat\n  &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 1           20.0       0.625        63.7      324.\n2 2           20.1       0.674        62.6      197.\n3 3           20.1       0.428        69.2      197.\n4 4           20.0       0.380        69.9      309.\n\n    # We see that cluster two of the clusters chat substantially more than the other two clusters\n    # And we see that two of the clusters have a lower percentage of females than the other two clusters\n    \n    # We can plot some of these relationships\n    \n    ggplot(cdat) + \n        geom_col(aes(y=mean_chat, x=cluster, fill=cluster)) + \n        ggtitle(\"Time spent in chat apps by segment\") + \n        theme_minimal()\n\n\n\n# Similar to privious project lets fiture out what is the best K number by using elbow plot.\n    \n    # we might want more information on which to base our choice of k\n    # One thing we might do is try many different values of k, and evaluate\n    # the performance of the algorithm for each k.  Here, our performance\n    # criteria will be the within-group sum of squares (WSS) from the model.\n    # As k increases, the WSS will decrease. The question is:\n    # how fast does it decrease?\n    \n    # let's try k=1, k=2, ..., k=10\n    \n    # we'll create a vector named 'res' to store our results\n    res &lt;- vector(length=10)\n    \n    # we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"grey\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()\n\n\n\n\n\n\n\nThis Customer Segmentation Analysis project utilizes the K-means clustering algorithm to segment customers based on their gaming habits and hand size, drawing from a dataset that includes various customer attributes. Initially, the dataset is imported and displayed for preliminary inspection. The analysis proceeds by subsetting data relevant to gaming time and hand size, followed by visualization through a scatter plot to understand the distribution before clustering. The data is then standardized to neutralize scale discrepancies, ensuring that clustering reflects genuine patterns rather than differences in measurement units. K-means clustering is applied to this standardized data to determine distinct customer groups, with iterative adjustments of cluster centroids to optimize within-cluster homogeneity. This segmentation allows for targeted product development and marketing strategies, particularly for designing smartphones that cater to the ergonomic and usage preferences of gamers. The success of this approach in identifying meaningful customer segments demonstrates the versatility of K-means clustering, suggesting its applicability across different industries to meet varied market and industrial needs."
  },
  {
    "objectID": "blog/project3/index.html#section-1-objective",
    "href": "blog/project3/index.html#section-1-objective",
    "title": "This is Project 3",
    "section": "",
    "text": "This project leverages real-world data from T-Mobile to perform an advanced customer segmentation analysis using the K-means clustering algorithm. The focus is to understand the relationship between customers’ hand sizes and their gaming durations on mobile devices, and how these insights can inform product customization for gamers."
  },
  {
    "objectID": "blog/project3/index.html#section-2-data",
    "href": "blog/project3/index.html#section-2-data",
    "title": "This is Project 3",
    "section": "",
    "text": "The analysis begins with a dataset from T-Mobile containing diverse customer attributes including biometric data. The data was processed using the R programming language and libraries from the tidyverse suite."
  },
  {
    "objectID": "blog/project3/index.html#section-3-methodology",
    "href": "blog/project3/index.html#section-3-methodology",
    "title": "This is Project 3",
    "section": "",
    "text": "Initial data exploration was conducted using basic commands to read and summarize the data. The K-means clustering algorithm was applied to segment customers based on gaming time and hand size. This included scaling the data, determining optimal cluster centers, and iterative refinement of these centers. The results were visualized using scatter plots, illustrating the clusters and their centroids. Further analysis included unscaling the cluster centers to interpret them in the context of the original data units."
  },
  {
    "objectID": "blog/project3/index.html#section-4-analysis",
    "href": "blog/project3/index.html#section-4-analysis",
    "title": "This is Project 3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsub &lt;- cust_dat %&gt;% select(gaming, handsize)\nhead(sub)   \n\n# A tibble: 6 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   316.     7.88\n2   442.     5.63\n3   245.     7.5 \n4   386.     5.97\n5   204.     6.71\n6   499.     7.7 \n\nggplot(sub) +\n        geom_point(aes(gaming, handsize)) + \n        theme_minimal()\n\n\n\n scl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\n    scl\n\n# A tibble: 3,000 × 2\n    gaming handsize\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.509    0.661 \n 2  1.35    -1.26  \n 3  0.0268   0.336 \n 4  0.977   -0.971 \n 5 -0.246   -0.339 \n 6  1.74     0.507 \n 7 -0.661    0.0371\n 8 -0.641   -2.13  \n 9  1.72    -0.296 \n10 -0.784   -0.569 \n# ℹ 2,990 more rows\n\n    # let's check that the scaling worked\n    \n    scl %&gt;% summarize_all(mean) %&gt;% round(3) # check means\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      0        0\n\n    scl %&gt;% summarize_all(sd)                # check std devs\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1      1        1\n\n     out &lt;- kmeans(scl, centers=4, nstart=10)\n      K &lt;- 4\n    D &lt;- 10\n    \n    set.seed(1234)\n    out &lt;- kmeans(scl, centers=K, nstart=D)\n str(out)\n\nList of 9\n $ cluster     : int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n $ centers     : num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"gaming\" \"handsize\"\n $ totss       : num 5998\n $ withinss    : num [1:4] 524 439 463 594\n $ tot.withinss: num 2019\n $ betweenss   : num 3979\n $ size        : int [1:4] 875 568 626 931\n $ iter        : int 4\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n  # 3 ways to extract a list element -- returns the element\n        str(out$cluster)\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[\"cluster\"]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[[1]])\n\n int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        # 2 related ways to subset a list into a one-element list (usually not what you want)\n        str(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n        str(out[1])\n\nList of 1\n $ cluster: int [1:3000] 3 2 4 2 1 3 1 1 2 1 ...\n\n    # We can also see that out$centers is a k-by-J matrix with the coordinates of the\n    # clusters' centers\n        \n        str(out$centers)\n\n num [1:4, 1:2] -0.739 0.919 1.097 -0.603 -0.72 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"gaming\" \"handsize\"\n\n        out$centers\n\n      gaming   handsize\n1 -0.7389810 -0.7204634\n2  0.9189017 -0.9874657\n3  1.0965455  0.6677762\n4 -0.6033999  0.8305672\n\n#grab the cluster membership as a variable and add it to our \n    # dataset as a factor/categorical variable\n    sub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n     sub %&gt;% count(cluster)\n\n# A tibble: 4 × 2\n  cluster     n\n  &lt;fct&gt;   &lt;int&gt;\n1 1         875\n2 2         568\n3 3         626\n4 4         931\n\n    out$size\n\n[1] 875 568 626 931\n\n     # Then, store the clusters' center locations in their own tibble/dataframe\n    centers &lt;- as_tibble(out$centers) \n    centers\n\n# A tibble: 4 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.739   -0.720\n2  0.919   -0.987\n3  1.10     0.668\n4 -0.603    0.831\n\n      # calculate mean and sd\n        SD   &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(sd)\n        MEAN &lt;- sub %&gt;% select(gaming, handsize) %&gt;% summarize_all(mean)\n        \n        SD\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   149.     1.17\n\n        MEAN\n\n# A tibble: 1 × 2\n  gaming handsize\n   &lt;dbl&gt;    &lt;dbl&gt;\n1   241.     7.11\n\n         # repeat/format the values so we can do math with centers (this is needed for line 129 below)\n        SD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        MEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \n        SD\n\n         [,1]     [,2]\n[1,] 148.6346 1.170988\n[2,] 148.6346 1.170988\n[3,] 148.6346 1.170988\n[4,] 148.6346 1.170988\n\n        MEAN\n\n        [,1]     [,2]\n[1,] 240.746 7.106557\n[2,] 240.746 7.106557\n[3,] 240.746 7.106557\n[4,] 240.746 7.106557\n\n        # unscale the centers (convert back into original units)\n        centers &lt;- centers*SD + MEAN\n        round(centers, 1)\n\n  gaming handsize\n1  130.9      6.3\n2  377.3      6.0\n3  403.7      7.9\n4  151.1      8.1\n\n        #plot the points (colored by cluster membership) and the cluster centers\n    ggplot() + \n        geom_point(data=sub,     aes(x=gaming, y=handsize, color=cluster)) + \n        geom_point(data=centers, aes(x=gaming, y=handsize), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n    # Run this function to show initial cluster points, in scaled space\n    \n        fun1 &lt;- function() { \n            # specify a starting point for the cluster centroids\n            c1 &lt;&lt;- c(gaming=-1, handsize= 2)\n            c2 &lt;&lt;- c(gaming= 1, handsize= 1)\n            c3 &lt;&lt;- c(gaming=-1, handsize=-1)\n            c4 &lt;&lt;- c(gaming= 2, handsize=-1)\n            \n            # convert to a data.frame\n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # pick colors\n            col4 &lt;- c(\"magenta\", \"green\", \"cyan\", \"purple\")\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), \n                           shape=21, fill=col4, color=\"black\", size=5) + \n                ggtitle(\"Kmeans centroids\") + \n                theme_minimal()\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun1()\n\n\n\n    # Run this function to show assignment of points\n    \n        fun2 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            # pick closest centroid as cluster to which each point is assigned\n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n                \n            \n            print(p)\n            return(invisible())\n        }\n    \n        fun2()\n\n\n\n    # run these functions a few times to show convergence\n    \n        fun3 &lt;- function() {\n            # Update cluster centers\n            c1 &lt;&lt;- apply(scl[clust==1, ], 2, mean)\n            c2 &lt;&lt;- apply(scl[clust==2, ], 2, mean)\n            c3 &lt;&lt;- apply(scl[clust==3, ], 2, mean)\n            c4 &lt;&lt;- apply(scl[clust==4, ], 2, mean)\n            \n            cent_dat &lt;&lt;- data.frame(rbind(c1, c2, c3, c4))\n            \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun4 &lt;- function() {\n            # get assignment criteria (euclidean distance to centroids)\n            c1ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c1)^2)))\n            c2ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c2)^2)))\n            c3ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c3)^2)))\n            c4ssq &lt;- apply(scl, 1, function(x) sqrt(sum((x-c4)^2)))\n            \n            clust &lt;&lt;- factor(apply(cbind(c1ssq, c2ssq, c3ssq, c4ssq), 1, which.min))\n        \n            # plot\n            p &lt;- ggplot() +\n                geom_point(data=scl, aes(gaming, handsize, color=clust)) +\n                geom_point(data=cent_dat, aes(gaming, handsize), size=4) + \n                ggtitle(\"Kmeans cluster membership and centroids\") + \n                theme_minimal() + \n                theme(legend.position = \"none\")\n            \n            print(p)\n            return(invisible())\n        }\n        \n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n        fun3()\n\n\n\n        fun4()\n\n\n\n# You can keep doing it and see the points moving \n        \n        \n    # clean up\n    rm(cent_dat, centers, c1, c2, c3, c4, clust)\n    \n    \n# add labels back to data\n    \n    cust_dat &lt;- cust_dat %&gt;% mutate(cluster = factor(out$cluster))\n    head(cust_dat)\n\n# A tibble: 6 × 20\n  gender height handsize   age gaming  chat  maps video social reading\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 female   71       7.88    20   316.  282.  77.0  500.   176.    93.4\n2 male     64.6     5.63    20   442.  163.  17.5  588.   104.    62.3\n3 male     66.8     7.5     20   245.  362. 143.   357.   319.    82.7\n4 male     67.3     5.97    19   386.  202.  82.7  538.   244.   117. \n5 male     68.1     6.71    19   204.  408. 144.   242.   344.    67.5\n6 male     67.9     7.7     20   499.  157.  56.6  374.   112.    48.6\n# ℹ 10 more variables: total_minutes &lt;dbl&gt;, days_ago &lt;dbl&gt;, years_ago &lt;dbl&gt;,\n#   discount &lt;chr&gt;, phone_id &lt;chr&gt;, brand &lt;chr&gt;, size_cat &lt;chr&gt;, price &lt;dbl&gt;,\n#   screen_size &lt;dbl&gt;, cluster &lt;fct&gt;\n\n# Also can do other market research\n# Profile the segments by demographics.  Specifically:\n# summarize the segments by age, gender, height, and time spent chatting\n    \n    # For numeric variables, we can simply take means. \n    # For categorical variables, we calculate a proportion by taking the mean over the number of \n    # times something is \"true\"\n    \n    cdat &lt;- cust_dat %&gt;% \n                group_by(cluster) %&gt;% \n                summarize(mean_age    = mean(age), \n                          prop_female = mean(gender==\"female\"), \n                          mean_height = mean(height),\n                          mean_chat   = mean(chat))\n    \n    # view results\n    cdat\n\n# A tibble: 4 × 5\n  cluster mean_age prop_female mean_height mean_chat\n  &lt;fct&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 1           20.0       0.625        63.7      324.\n2 2           20.1       0.674        62.6      197.\n3 3           20.1       0.428        69.2      197.\n4 4           20.0       0.380        69.9      309.\n\n    # We see that cluster two of the clusters chat substantially more than the other two clusters\n    # And we see that two of the clusters have a lower percentage of females than the other two clusters\n    \n    # We can plot some of these relationships\n    \n    ggplot(cdat) + \n        geom_col(aes(y=mean_chat, x=cluster, fill=cluster)) + \n        ggtitle(\"Time spent in chat apps by segment\") + \n        theme_minimal()\n\n\n\n# Similar to privious project lets fiture out what is the best K number by using elbow plot.\n    \n    # we might want more information on which to base our choice of k\n    # One thing we might do is try many different values of k, and evaluate\n    # the performance of the algorithm for each k.  Here, our performance\n    # criteria will be the within-group sum of squares (WSS) from the model.\n    # As k increases, the WSS will decrease. The question is:\n    # how fast does it decrease?\n    \n    # let's try k=1, k=2, ..., k=10\n    \n    # we'll create a vector named 'res' to store our results\n    res &lt;- vector(length=10)\n    \n    # we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"grey\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()"
  },
  {
    "objectID": "blog/project3/index.html#section-5-summary",
    "href": "blog/project3/index.html#section-5-summary",
    "title": "This is Project 3",
    "section": "",
    "text": "This Customer Segmentation Analysis project utilizes the K-means clustering algorithm to segment customers based on their gaming habits and hand size, drawing from a dataset that includes various customer attributes. Initially, the dataset is imported and displayed for preliminary inspection. The analysis proceeds by subsetting data relevant to gaming time and hand size, followed by visualization through a scatter plot to understand the distribution before clustering. The data is then standardized to neutralize scale discrepancies, ensuring that clustering reflects genuine patterns rather than differences in measurement units. K-means clustering is applied to this standardized data to determine distinct customer groups, with iterative adjustments of cluster centroids to optimize within-cluster homogeneity. This segmentation allows for targeted product development and marketing strategies, particularly for designing smartphones that cater to the ergonomic and usage preferences of gamers. The success of this approach in identifying meaningful customer segments demonstrates the versatility of K-means clustering, suggesting its applicability across different industries to meet varied market and industrial needs."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I Clean some data\n\n\n\nI analyzed the data\n\nlibrary(tidyverse)\nmtcars |&gt; \n    ggplot(aes(x = wt, y= mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I Clean some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n\nlibrary(tidyverse)\nmtcars |&gt; \n    ggplot(aes(x = wt, y= mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project7/index.html",
    "href": "blog/project7/index.html",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "",
    "text": "use the heterogeneous MNL from project 6 to optimize prices.\n\n\nThis script aims to optimize the pricing strategy for Samsung’s smartphone models using a heterogeneous Multinomial Logit (MNL) model. The analysis focuses on understanding consumer choice behavior in response to price variations and seeks to maximize the overall profitability of Samsung’s smartphone product line."
  },
  {
    "objectID": "blog/project7/index.html#objective",
    "href": "blog/project7/index.html#objective",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "",
    "text": "This script aims to optimize the pricing strategy for Samsung’s smartphone models using a heterogeneous Multinomial Logit (MNL) model. The analysis focuses on understanding consumer choice behavior in response to price variations and seeks to maximize the overall profitability of Samsung’s smartphone product line."
  },
  {
    "objectID": "blog/project7/index.html#how-do-you-learn-wtp",
    "href": "blog/project7/index.html#how-do-you-learn-wtp",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "How do you learn WTP?",
    "text": "How do you learn WTP?\nThe EVC method for product x:\n\nSelect a best, available alternative (y) and find it’s price.\nDetermine the incremental economic value of your product (x) over y [may involve the difference in non-price costs between x and y]\nSplit the benefit (not necessarily 50/50) with the customer"
  },
  {
    "objectID": "blog/project7/index.html#economic-value-to-the-customer-evc",
    "href": "blog/project7/index.html#economic-value-to-the-customer-evc",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "Economic Value to The Customer (EVC)",
    "text": "Economic Value to The Customer (EVC)\nBest Available Alternative (y) Might not be a commercial product. Choose y based on customer interviews. if wrong y, EVC estimate will be too high.\nIncremental Economic Value of X Consider benefits and non-price costs. Price of x doesn’t enter the calculation\nSplit the benefit Contrib margin (profit) VS con. surplus (growth) Fairness, pperceived vs actual benefit, etc\nCan do this by segment y and EVC can vary across customer segments. If EVC&lt;0, reconsider your product or target customer.\nsummary: The Economic Value to the Customer (EVC) method is a pricing strategy that determines the maximum price a customer is willing to pay based on the total value they derive from the product. The cost is the expense incurred to produce the product, while the price is set above the cost to ensure profitability. The perceived value is the customer’s assessment of the product’s worth, often higher than the price, providing the inducement to buy, which is the extra value perceived by the customer. The contribution margin is the profit margin between the cost and the price. The best available alternative (best avail. alt y) represents the value of the closest competing product. The area of negotiation is the range between the cost and the EVC, where the final price can be adjusted based on market dynamics and customer willingness to pay."
  },
  {
    "objectID": "blog/project7/index.html#price-optimization-under-ced-and-mnl",
    "href": "blog/project7/index.html#price-optimization-under-ced-and-mnl",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "Price Optimization Under CED and MNL",
    "text": "Price Optimization Under CED and MNL\n\nConstant Elasticity of Demand (CED) Model\n\nAssume a convenient functional form: \\(Q = e^{\\alpha} \\times P^{\\beta}\\)\nThis implies: \\(\\ln(Q) = \\alpha + \\beta \\times \\ln(P)\\)\nSlope coefficient is elasticity: \\(\\beta = \\frac{d(\\ln Q)}{d(\\ln P)} = \\frac{P}{Q} \\frac{dQ}{dP} = \\varepsilon\\)\nOptimal price is: \\(P^{*} = \\frac{c}{1 + \\frac{1}{\\varepsilon}}\\)\n\n\n\nHeterogeneous Multinomial Logit (MNL) Demand Model\n\nGet estimated sales from market share predictions for a given price: \\(q_{j}(p_{j}) = M \\times \\hat{s}_{j}(p_{j})\\)\nCalculate profit at that price: \\(\\pi(p_{j}) = q_{j}(p_{j}) \\times \\left[ p_{j} - c_{j}(q_{j}(p_{j})) \\right]\\)\nTry a bunch of candidate prices, and pick the one that maximizes profit: \\(p^{*} = \\underset{p_{m}}{\\arg\\max} \\ \\pi(p_{m})\\)"
  },
  {
    "objectID": "blog/project7/index.html#library-imports-and-data-preparation",
    "href": "blog/project7/index.html#library-imports-and-data-preparation",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "Library Imports and Data Preparation",
    "text": "Library Imports and Data Preparation\n\n# Data Importing \nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(mlogit))\nsuppressPackageStartupMessages(library(gridExtra))\n\n# load data, get segments, fit mnl model\n    # load main dataset\ncust_dat &lt;- read_csv(\"~//Desktop/MGT100 folder/smartphone_customer_data.csv\", show_col_types = FALSE)\n# load \"enhanced\" dataset for fitting mnl model\n    load(\"~/Desktop/MGT100 folder/mnl_datasets.RData\")\n     # fit mnl data\n    out &lt;- mlogit(choice ~ apple:segment + \n                           samsung:segment + \n                           price:segment +\n                           screen_size:segment + \n                           price:total_minutes:segment | 0, data=mdat1)"
  },
  {
    "objectID": "blog/project7/index.html#demand-curve-for-a-phone",
    "href": "blog/project7/index.html#demand-curve-for-a-phone",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "Demand Curve for A Phone",
    "text": "Demand Curve for A Phone\nConstruct the estimated demand curve for the small Samsung phone (S1). That means we will vary the price of S1 while holding everything else constant, and track how its market share changes as we change its price.\nRecall that last year S1 had a price of $799 and a market share of 25.3%.\n\n# Get a vector of price changes to use\npvec &lt;- seq(from = -200, to = 200, by = 10)\n\n# Construct an empty matrix to store shares at each price\nsmat &lt;- matrix(NA, nrow = length(pvec), ncol = 6)\ncolnames(smat) &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\")\n\n# Suppress output using invisible and capture.output\ninvisible(capture.output({\n  # Loop over the price change values\n  for (i in 1:length(pvec)) {\n    # Print progress (this will be suppressed)\n    cat(\"Working on\", i, \"of\", length(pvec), \"\\n\")\n    \n    # Get the price change amount\n    p &lt;- pvec[i]\n    \n    # Change prices for S1 phones\n    tempdat &lt;- as_tibble(mdat1) |&gt; mutate(price = ifelse(phone_id == \"S1\", price + p, price))\n    \n    # Make market share predictions with the temporarily-changed S1 prices\n    preds &lt;- predict(out, newdata = tempdat)\n    \n    # Calculate and store market shares\n    smat[i, ] &lt;- colMeans(preds)\n  }\n}, file = NULL))\n\n    \n    # gather our prices and estimated shares into a dataframe\n    relcol &lt;- which(colnames(smat) == \"S1\")\n    s1dat &lt;- tibble(scenario=1:length(pvec), price=pvec+799, share=smat[,relcol])\n    \n    # plot S1's inverse demand curve\n    ggplot(s1dat, aes(x=share, y=price)) +\n        geom_point() + \n        geom_line() + \n        labs(x=\"Share\", y=\"Price\") +\n        theme_bw()\n\n\n\n\nThe term “residual demand” is used when the demand is specific to a product and assumes that other factors remain constant. This approach acknowledges that changes in external factors, such as the prices of competing products like A1 or H1, will also affect the residual demand curve for S1.\nIt is important to observe that the model-predicted market share may not perfectly align with the observed market share. This discrepancy arises because the model includes brand-specific coefficients rather than product-specific coefficients. This difference is not a flaw in the model but rather an aspect to be cognizant of during analysis."
  },
  {
    "objectID": "blog/project7/index.html#model-fitting",
    "href": "blog/project7/index.html#model-fitting",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "Model Fitting",
    "text": "Model Fitting\n\n# actual market shares\n       invisible({ cust_dat |&gt; filter(years_ago == 1) |&gt; count(phone_id) |&gt; mutate(shr = n / sum(n))})\n        \n        # predicted market shares at 0 price change\n        smat[21,] |&gt; round(3)\n\n   A1    A2    S1    S2    H1    H2 \n0.249 0.107 0.232 0.119 0.172 0.120 \n\n        # Convert shares to number of phones\n    \n    # Suppose the market size is such that 150 million smartphones are sold in the US each year\n    # and further suppose that the college-age demographic that we've measured with our\n    # dataset comprises 1 out of every 15 smartphone sales, or 10 million phones.\n    \n    M &lt;- 10\n\n    # Let's scale our demand curve to be in the price-quantity space instead of the price-share space\n    \n    s1dat &lt;- s1dat |&gt; mutate(quantity = share*M)\n\n    ggplot(s1dat, aes(x=quantity, y=price)) + \n        geom_point() + \n        geom_line() + \n        labs(x=\"Quantity\", y=\"Price\") +\n        theme_bw()\n\n\n\n# Calculate S1 price to maximize S1 profits based on own-price elasticity\n     # Marginal cost\n    \n        # We need to know our marginal cost function. Suppose that a manager \n        # at Samsung informs us that it costs $470 to manufacture, transport, \n        # and advertise one S1 phone, regardless of how many S1 phones are produced. \n    \n        mc1 &lt;- 470\n        \n    # Calculate own-price elasticity at +/- $10 from actual price of $799\n    \n        p1 &lt;- s1dat |&gt; filter(price==799-10) |&gt; pull(price)\n        q1 &lt;- s1dat |&gt; slice(20) |&gt; pull(quantity)\n        \n        p2 &lt;- s1dat |&gt; slice(22) |&gt; pull(price)\n        q2 &lt;- s1dat |&gt; slice(22) |&gt; pull(quantity)\n        \n        elasticity &lt;- ((q2-q1)/q1) / ((p2-p1)/p1)\n        elasticity\n\n[1] -3.62357\n\n    # Approximate optimal price using the elasticity rule\n    \n        mc1 * 1 / (1 - 1/abs(elasticity))\n\n[1] 649.1452\n\n        # this approach suggests that S1 price should be set much lower ($649) than its \n        # current value ($799). However, it is based on an assumption of constant demand elasticity,\n        # whereas our estimated demand model does not restrict price elasticity to be constant\n    \n    # As an aside: check elasticity over full range of prices considered\n        \n        # total calc\n        p1 &lt;- s1dat |&gt; slice(1) |&gt; pull(price)\n        q1 &lt;- s1dat |&gt; slice(1) |&gt; pull(quantity)\n        \n        p2 &lt;- s1dat |&gt; slice(41) |&gt; pull(price)\n        q2 &lt;- s1dat |&gt; slice(41) |&gt; pull(quantity)\n        \n        elasticity &lt;- ((q2-q1)/q1) / ((p2-p1)/p1)\n        elasticity\n\n[1] -1.234135\n\n        # one-window at a time calc\n        res_e &lt;- vector(length=39)\n        for(i in 2:40) {\n            p1 &lt;- s1dat |&gt; slice(i-1) |&gt; pull(price)\n            q1 &lt;- s1dat |&gt; slice(i-1) |&gt; pull(quantity)\n            \n            p2 &lt;- s1dat |&gt; slice(i+1) |&gt; pull(price)\n            q2 &lt;- s1dat |&gt; slice(i+1) |&gt; pull(quantity)\n            \n            res_e[i-1] &lt;- ((q2-q1)/q1) / ((p2-p1)/p1)\n        }\n        summary(res_e)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -4.229  -4.102  -3.624  -3.346  -2.689  -1.644 \n\n        # Calculate S1 price to maximize S1 profit based on full set of MNL model estimates\n# (We'll do this with grid search)\n        \n    # revenue\n    \n        s1dat &lt;- s1dat |&gt; mutate(revenue = price * quantity)\n        \n        p1 &lt;- ggplot(s1dat) + geom_point(aes(x=quantity, y=price)) + theme_bw()\n        p2 &lt;- ggplot(s1dat) + geom_point(aes(x=quantity, y=revenue)) + theme_bw()\n        \n        grid.arrange(p2, p1, ncol=1)\n\n\n\n        # notice that revenue increases, but starts to flatten out, as quantity increases\n        \n    # margin\n        \n        s1dat &lt;- s1dat |&gt; mutate(cost = mc1*quantity)\n        \n        p3 &lt;- ggplot(s1dat) + geom_point(aes(x=quantity, y=cost)) + theme_bw()\n        \n        grid.arrange(p3, p2, p1, ncol=1)\n\n\n\n        # note how cost is linear, so unlike revenue, it does not start to flatten out\n        # at higher quantities\n        \n        # profit, at a particular price, is the distance between the revenue and cost\n        # curves. Let's look at that distance graphically\n        \n        ggplot(s1dat) + \n            geom_line(aes(x=quantity, y=revenue), color=\"blue\") +\n            geom_point(aes(x=quantity, y=revenue), color=\"blue\") +\n            geom_line(aes(x=quantity, y=cost), color=\"red\") + \n            geom_point(aes(x=quantity, y=cost), color=\"red\") + \n            xlab(\"Quantity\") +\n            ylab(\"Dollars\") +\n            ggtitle(\"Revenue (blue) and Cost (red)\") +\n            theme_bw()\n\n\n\n        # the distance/gap is largest somewhere around a quantity of 3 million phones,\n        # which roughly corresponds to price in the $700-750.  Let's now calculate\n        # profit and the profit maximizing price more exactly.\n        \n    # profit\n        \n        s1dat &lt;- s1dat |&gt; mutate(profit = revenue - cost)\n        \n        p4 &lt;- ggplot(s1dat) + geom_point(aes(x=quantity, y=profit)) + theme_bw()\n        \n        grid.arrange(p4, p3, p2, p1, ncol=1)\n\n\n\n    # find S1-profit-maximizing quantity and price, and compare profit to $799 price\n        \n        s1dat |&gt; filter(price == 799)\n\n# A tibble: 1 × 7\n  scenario price share quantity revenue  cost profit\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       21   799 0.232     2.32   1857. 1092.   765.\n\n        s1dat |&gt; filter(profit == max(profit))\n\n# A tibble: 1 × 7\n  scenario price share quantity revenue  cost profit\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       12   709 0.349     3.49   2474. 1640.   834.\n\n\nWe see that the profit-maximizing price of $709 results $834 million of profits. While the price of $799 results in only 765 million of profits\nThis demonstration has concentrated on determining the profit-maximizing price for the smartphone model S1, solely considering the profits generated by S1. However, it is essential to recognize that Samsung is interested in the overall profitability of its entire smartphone product line. Thus, the analysis so far has excluded the consideration of model S2. We will now extend our analysis to incorporate the impact of S2 on the total profit from Samsung’s smartphone product line."
  },
  {
    "objectID": "blog/project7/index.html#calculate-s1-price-to-maximize-total-samsung-smartphone-profit",
    "href": "blog/project7/index.html#calculate-s1-price-to-maximize-total-samsung-smartphone-profit",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "Calculate S1 price to maximize total Samsung smartphone profit",
    "text": "Calculate S1 price to maximize total Samsung smartphone profit\n\n# The manager at Samsung reports that S2 marginal costs are $490\n        \n        mc2 &lt;- 490\n        \n    # Let's calculate quantity, revenue, cost, and profit for the S2 phone\n        \n        s2dat &lt;- tibble(scenario=1:length(pvec), price=899, share=smat[,4])\n        \n        s2dat &lt;- s2dat |&gt; mutate(quantity = share*M,\n                                  revenue = price * quantity,\n                                  cost = mc2*quantity,\n                                  profit = revenue - cost)\n        \n    # now we will aggregate across phones to get total Samsung smartphone profit\n        \n        s2dat &lt;- s2dat |&gt; mutate(price=0)\n        \n        sdat &lt;- rbind(s1dat, s2dat)\n        \n        sdat &lt;- sdat |&gt; group_by(scenario) |&gt; \n            summarize_all(sum)\n        \n    # find Samsung profit-maximizing quantity and price \n         \n         sdat |&gt; filter(price == 799)\n\n# A tibble: 1 × 7\n  scenario price share quantity revenue  cost profit\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       21   799 0.352     3.52   2931. 1678.  1253.\n\n         sdat |&gt; filter(profit == max(profit))\n\n# A tibble: 1 × 7\n  scenario price share quantity revenue  cost profit\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1       16   749 0.404     4.04   3192. 1921.  1271.\n\n         # we get a profit maximizing price of $749 leading to $1,271 mil in profits, \n         # which is $18 million more than the profit of $1,253 when s1 is priced at $799\n\n    # why do we get different answer?\n         \n        # As you decrease the price of S1, S1 is estimated to garner a larger \n        # market share. The increase to S1's share results from decreases to\n        # other phones' market shares.  These other phone include competitors' \n        # phones like A1, A2, H1, and H2, but also Samsung's other phone S2.\n         \n        share_dat &lt;- as_tibble(cbind(S1_price=pvec+799, smat))\n        \n        share_dat &lt;- pivot_longer(share_dat, cols=A1:H2, names_to=\"phone\", values_to=\"share\")\n        \n        ggplot(share_dat, aes(x=S1_price, y=share, color=phone)) +\n            geom_line() + \n            geom_point() + \n            xlab(\"Price of S1 Phone\") + \n            ylab(\"Market Share\") + \n            ggtitle(\"Estimated Market Share Responses to S1 Price Change by Phone\") +\n            xlim(c(700,900)) + ylim(c(0,0.5)) + \n            theme_bw()\n\nWarning: Removed 126 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 126 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "blog/project7/index.html#conclusion",
    "href": "blog/project7/index.html#conclusion",
    "title": "Project 7: Price Optimization Using Demand Models",
    "section": "Conclusion",
    "text": "Conclusion\nTo determine the total profit-maximizing price for Samsung’s S1 smartphone, it is necessary to balance the increased revenue and profit from setting a low price for S1 against the potential loss of revenue and profit from customers switching from S2 to S1. Pricing S1 too low may attract buyers away from S2, thereby reducing the overall profitability of Samsung’s smartphone product line. Therefore, the optimal pricing strategy must consider both the direct profits from S1 and the indirect impact on S2 sales.\nThis script effectively demonstrates a comprehensive approach to price optimization using advanced econometric modeling. By employing the heterogeneous MNL model, the analysis provides insights into how price variations affect consumer demand for specific smartphone models. The results emphasize the importance of considering cross-product interactions and the overall product line profitability when making pricing decisions. This methodology can be applied to other product lines and industries to enhance pricing strategies and maximize overall profitability."
  },
  {
    "objectID": "resume/resume.html",
    "href": "resume/resume.html",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Benjamin Ma\n\n\n\n\n\n Ben26msh@gmail.com\n github.com/BenjaminMa\n +1 2069307372\nFor more information, please contact me via email.\n\n\n\n\n\nExperienced in statistical analysis, business mapping methods.\nExperience with business analysis and market (inbound, outbound) research with CRM and Salesforce.\nSkilled in leadership\nSkilled in R, Stata, Excel, and SQL\n\n\n\n\nThis resume was made with the R package pagedown.\nLast updated on 2024-09-11."
  },
  {
    "objectID": "resume/resume.html#contact",
    "href": "resume/resume.html#contact",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Ben26msh@gmail.com\n github.com/BenjaminMa\n +1 2069307372\nFor more information, please contact me via email."
  },
  {
    "objectID": "resume/resume.html#skills",
    "href": "resume/resume.html#skills",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "Experienced in statistical analysis, business mapping methods.\nExperience with business analysis and market (inbound, outbound) research with CRM and Salesforce.\nSkilled in leadership\nSkilled in R, Stata, Excel, and SQL"
  },
  {
    "objectID": "resume/resume.html#disclaimer",
    "href": "resume/resume.html#disclaimer",
    "title": "Benjamin Ma’s resume",
    "section": "",
    "text": "This resume was made with the R package pagedown.\nLast updated on 2024-09-11."
  },
  {
    "objectID": "resume/resume.html#title",
    "href": "resume/resume.html#title",
    "title": "Benjamin Ma’s resume",
    "section": "Benjamin Ma",
    "text": "Benjamin Ma\n\nUndergraduate Student from UC SanDiego\nlooking for a skill related job (Business & Marketing Analysis related)."
  },
  {
    "objectID": "resume/resume.html#education",
    "href": "resume/resume.html#education",
    "title": "Benjamin Ma’s resume",
    "section": "Education",
    "text": "Education\n\nUniversity of California, San Diego\nB.S. Business Economics\nSan Diego, USA\n2022-2024\n\n\n\nUniversity of Edinburgh\nStudy Abroad\nEdinburgh, UK\n2023"
  },
  {
    "objectID": "resume/resume.html#professional-experience",
    "href": "resume/resume.html#professional-experience",
    "title": "Benjamin Ma’s resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nData Science, intern\nTymeline\nDelaware, USA\n09/2024-Present\n\nGo-to-market strategy development and product launches.\nMarket research to identify opportunities and assess impact.\nCustomer segmentation, ROI, and market analysis.\nAnalyzed trends and customer behavior for data-driven decisions.\nBuilt marketing materials helps campaigns, boost visibility and engagement.\n\n\n\nAI Training for Marketing Research\nOutlier AI\nSan Francisco, USA\n04/2024-Present\n\nTrained AI models for marketing research, enhancing the models’ capability.\nInstructed AI models in analyzing customer data and segmenting markets.\nDeveloped training code for AI to identify high-value customers.\nEnsuring continuous inprovement and accuracy in marketing research applications\n\n\n\nBusiness Analyst, intern\nEntropy Technologies\nSydney, Australia\n04/2024-09/2024\n\nData sourcing, cleaning, and analysis\nGenerated business maps for market segmentation using advanced data analysis tools.\nUtilized Tableau to visualize data and generated insights to the management team.\nBuilt and optimized advertising landing pages based on weekly traffic analyzed results.\nIncreased daily click-through rates and traffic for promotional campaigns.\n\n\n\nBusiness Analyst, intern\nCamal Group.\nSan Diego, USA\n11/2023-03/2024\n\nSourcing, cleaning, and visualizing trade data from prospect.\nGenerating weekly summary report to assist make sale strategy decisions.\nExceeded KPIs and achieved multiple sales for the company.\nReceived return offer from company."
  },
  {
    "objectID": "resume/resume.html#volunteer-experience",
    "href": "resume/resume.html#volunteer-experience",
    "title": "Benjamin Ma’s resume",
    "section": "Volunteer Experience",
    "text": "Volunteer Experience\n\nUnited Nations Volunteer Program (Goodness and Mercy Missions)\n-Participated in non-profit organization -Developed marketing strategies with team. -Negotiated fundrasing with government agencies and global institutions. -Awarded with a certification\nNew York, USA\n11/01/2020-08/01/2021\n\n\nUnited Nations Volunteer Program (Cameroon Association of Active Youths)\n-Assisted facilitating donations from institutions -Led a safety equipment’s advisory group. -Cleaning donation data -Using data visualization tools for weekly project evaluation. -Received a certification and letter of appreciation.\nNew York, USA\n11/25/2020-03/25/2021"
  },
  {
    "objectID": "resume/resume.html#skills-1",
    "href": "resume/resume.html#skills-1",
    "title": "Benjamin Ma’s resume",
    "section": "Skills",
    "text": "Skills\nBusiness & Marketing Analysis, Data Visualization (Tableau, Power BI, Excel), Data Warehouse(Snowflake), Project Management(Jira,Linear, Asana), CRM, Salesforce, Microsoft-Office, SQL, STATA, R/RStudio."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Benjamin’s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project8/index.html",
    "href": "blog/project8/index.html",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "",
    "text": "use the heterogeneous MNL from project 6 to assess whether company should hire a celebrity to affiliate with their brand.\n\n\nThe objective of this research project is to systematically analyze the impact of celebrity endorsements on the profitability of consumer electronics companies, specifically focusing on Samsung. This study aims to quantify the financial benefits derived from such marketing strategies, evaluate the cost-effectiveness of hiring celebrity endorsers, and provide strategic recommendations for optimizing marketing expenditures. By examining market trends, consumer behavior, and financial outcomes, this research will contribute to a deeper understanding of the role of celebrity endorsements in enhancing corporate profitability and market share.\n\n\n\n\n# Data Importing\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(mlogit))\n# load \"enhanced\" dataset for fitting mnl model\n    load(\"~/Desktop/MGT100 folder/mnl_datasets.RData\")\n# get the product names for the 6 products\n  prod_vec &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\")\n  # fit mnl data\n    out &lt;- mlogit(choice ~ apple:segment + \n                           samsung:segment + \n                           price:segment +\n                           screen_size:segment + \n                           price:total_minutes:segment | 0, data=mdat1)  \n\n\n\n\n\n# expected/predicted product-specific market shares\n pred1 &lt;- colMeans(predict(out, newdata = mdat1))\n    names(pred1) &lt;- prod_vec\n    round(pred1*100, 1)\n\n  A1   A2   S1   S2   H1   H2 \n24.9 10.7 23.2 11.9 17.2 12.0 \n\n# calculate baseline prices and product-specific profits\n    d1 &lt;- tibble(price1 = 799, \n                 share1 = pred1[3],\n                 price2 = 899,\n                 share2 = pred1[4])\n    \n # assumed market size\n    M &lt;- 10\n    \n    # calculate quantities and revenues\n    d1 &lt;- d1 %&gt;% mutate(q1 = share1*M, \n                        q2 = share2*M,\n                        rev1 = q1 * price1,\n                        rev2 = q2 * price2)\n    \n    # marginal cost for phones S1 and s2\n    mc1 &lt;- 440\n    mc2 &lt;- 470\n    \n    # calculate costs and profits\n    d1 &lt;- d1 %&gt;% mutate(cost1 = mc1 * q1,\n                        cost2 = mc2 * q2,\n                        profit1 = rev1 - cost1,\n                        profit2 = rev2 - cost2,\n                        total_profit = profit1 + profit2)\n    \n    # print the result\n    d1\n\n# A tibble: 1 × 13\n  price1 share1 price2 share2    q1    q2  rev1  rev2 cost1 cost2 profit1\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1    799  0.232    899  0.119  2.32  1.19 1857. 1073. 1023.  561.    834.\n# ℹ 2 more variables: profit2 &lt;dbl&gt;, total_profit &lt;dbl&gt;\n\n  # calculate pure demand effect of association with a celebrity\n\n    # suppose we learn from other market research that a celebrity like \n    # Celebrity will improve our brand perception by an amount delta.\n    # And suppose we have done some work to convert delta into a \"CCE\"\n    # that is, a celebrity coefficient effect of size 0.005\n    \n    # celebrity coefficient effect\n    cce &lt;- 0.005\n    \n    # let's create an adjusted version of our model where we increase the \n    # samsung brand-intercept coefficients by the amount of the CCE\n    out_adj &lt;- out\n    out_adj$coefficients[4:6] &lt;- out_adj$coefficients[4:6] + cce\n    \n    # let's look at the original coefficients next to the adjusted ones, where\n    # we'll see that they are the same except the samsung:segment intercepts\n    # which are 0.005 higher\n    cbind(coef(out), coef(out_adj))\n\n                              [,1]          [,2]\napple:segment1        7.644964e-01  7.644964e-01\napple:segment2        8.806709e-01  8.806709e-01\napple:segment3        1.353091e+00  1.353091e+00\nsegment1:samsung      1.161551e+00  1.166551e+00\nsegment2:samsung      5.566730e-01  5.616730e-01\nsegment3:samsung      5.223327e-01  5.273327e-01\nsegment1:price       -1.597733e-02 -1.597733e-02\nsegment2:price       -3.093656e-02 -3.093656e-02\nsegment3:price       -2.899266e-02 -2.899266e-02\nsegment1:screen_size -5.171158e-01 -5.171158e-01\nsegment2:screen_size  1.574106e-01  1.574106e-01\nsegment3:screen_size -1.922263e-01 -1.922263e-01\nsegment1:price        8.262789e-06  8.262789e-06\nsegment2:price        1.902141e-05  1.902141e-05\nsegment3:price        1.702137e-05  1.702137e-05\n\n    # calculate revised predictions from this adjusted model\n    pred2 &lt;- colMeans(predict(out_adj, newdata = mdat1))\n    names(pred2) &lt;- prod_vec\n    \n    # calculate change in market shares\n    shr_change &lt;- pred2 - pred1\n    names(shr_change) &lt;- prod_vec\n    \n    # print original expected market shares, now with-celeb market shares,\n    # and their difference.  Notice that we multiple by 100, so the interpretation\n    # is that affiliation with Kaby leads to a 0.073% increase for the S1 phone and\n    # a 0.038% increase for the S2 phone\n    round(pred1*100, 2)\n\n   A1    A2    S1    S2    H1    H2 \n24.92 10.68 23.24 11.94 17.24 11.98 \n\n    round(pred2*100, 2)\n\n   A1    A2    S1    S2    H1    H2 \n24.88 10.66 23.32 11.98 17.21 11.95 \n\n    round(shr_change*100, 3)\n\n    A1     A2     S1     S2     H1     H2 \n-0.042 -0.018  0.073  0.038 -0.030 -0.021 \n\n    # let's plot a comparison of market shares with and without Kaby affiliation.\n    # You'll see the heights of the bars are very close.\n    pdat &lt;- rbind(\n        tibble(celeb=\"No\",  product=prod_vec, share=pred1),\n        tibble(celeb=\"Yes\", product=prod_vec, share=pred2)\n    )\n    \n    pdat &lt;- pdat %&gt;% mutate(celeb=fct_inorder(factor(celeb)))\n    ggplot(pdat) +\n        geom_col(aes(product, share, fill=celeb), position=\"dodge\") + \n        ggtitle(\"Pure Demand Effect\") +\n        ylab(\"Product-Specific Market Shares\") + \n        xlab(\"Product\") + \n        scale_fill_manual(\"Celebrity\", values=c(No=\"Firebrick\", Yes=\"Dodgerblue4\")) + \n        theme_bw()\n\n\n\n    # calculate new expected profit\n    d2 &lt;- tibble(price1 = 799, \n                 share1 = pred2[3],\n                 price2 = 899,\n                 share2 = pred2[4])\n    \n    # calculate quantities, revenues, costs, and profits\n    d2 &lt;- d2 %&gt;% mutate(q1 = share1*M, \n                        q2 = share2*M,\n                        rev1 = q1 * price1,\n                        rev2 = q2 * price2,\n                        cost1 = mc1 * q1,\n                        cost2 = mc2 * q2,\n                        profit1 = rev1 - cost1,\n                        profit2 = rev2 - cost2,\n                        total_profit = profit1 + profit2)\n    \n    # print the result\n    rbind(d1, d2)\n\n# A tibble: 2 × 13\n  price1 share1 price2 share2    q1    q2  rev1  rev2 cost1 cost2 profit1\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1    799  0.232    899  0.119  2.32  1.19 1857. 1073. 1023.  561.    834.\n2    799  0.233    899  0.120  2.33  1.20 1863. 1077. 1026.  563.    837.\n# ℹ 2 more variables: profit2 &lt;dbl&gt;, total_profit &lt;dbl&gt;\n\n    # calculate the change in profit\n    d2$total_profit - d1$total_profit\n\n      S1 \n4.233878 \n\n\n\n\n\nThe analysis indicates that the engagement of a celebrity endorser results in a substantial profit increment of $4.23 million. Consequently, if the fee charged by the celebrity for endorsement services is less than this amount, it would be financially advantageous for Samsung to proceed with the hiring. This conclusion underscores the potential profitability and strategic value of celebrity endorsements in enhancing company revenue."
  },
  {
    "objectID": "blog/project8/index.html#objective",
    "href": "blog/project8/index.html#objective",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "",
    "text": "The objective of this research project is to systematically analyze the impact of celebrity endorsements on the profitability of consumer electronics companies, specifically focusing on Samsung. This study aims to quantify the financial benefits derived from such marketing strategies, evaluate the cost-effectiveness of hiring celebrity endorsers, and provide strategic recommendations for optimizing marketing expenditures. By examining market trends, consumer behavior, and financial outcomes, this research will contribute to a deeper understanding of the role of celebrity endorsements in enhancing corporate profitability and market share."
  },
  {
    "objectID": "blog/project8/index.html#library-imports-and-data-preparation",
    "href": "blog/project8/index.html#library-imports-and-data-preparation",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "",
    "text": "# Data Importing\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(mlogit))\n# load \"enhanced\" dataset for fitting mnl model\n    load(\"~/Desktop/MGT100 folder/mnl_datasets.RData\")\n# get the product names for the 6 products\n  prod_vec &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\")\n  # fit mnl data\n    out &lt;- mlogit(choice ~ apple:segment + \n                           samsung:segment + \n                           price:segment +\n                           screen_size:segment + \n                           price:total_minutes:segment | 0, data=mdat1)"
  },
  {
    "objectID": "blog/project8/index.html#calculate-expected-profit-without-celebrity-affiliation",
    "href": "blog/project8/index.html#calculate-expected-profit-without-celebrity-affiliation",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "",
    "text": "# expected/predicted product-specific market shares\n pred1 &lt;- colMeans(predict(out, newdata = mdat1))\n    names(pred1) &lt;- prod_vec\n    round(pred1*100, 1)\n\n  A1   A2   S1   S2   H1   H2 \n24.9 10.7 23.2 11.9 17.2 12.0 \n\n# calculate baseline prices and product-specific profits\n    d1 &lt;- tibble(price1 = 799, \n                 share1 = pred1[3],\n                 price2 = 899,\n                 share2 = pred1[4])\n    \n # assumed market size\n    M &lt;- 10\n    \n    # calculate quantities and revenues\n    d1 &lt;- d1 %&gt;% mutate(q1 = share1*M, \n                        q2 = share2*M,\n                        rev1 = q1 * price1,\n                        rev2 = q2 * price2)\n    \n    # marginal cost for phones S1 and s2\n    mc1 &lt;- 440\n    mc2 &lt;- 470\n    \n    # calculate costs and profits\n    d1 &lt;- d1 %&gt;% mutate(cost1 = mc1 * q1,\n                        cost2 = mc2 * q2,\n                        profit1 = rev1 - cost1,\n                        profit2 = rev2 - cost2,\n                        total_profit = profit1 + profit2)\n    \n    # print the result\n    d1\n\n# A tibble: 1 × 13\n  price1 share1 price2 share2    q1    q2  rev1  rev2 cost1 cost2 profit1\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1    799  0.232    899  0.119  2.32  1.19 1857. 1073. 1023.  561.    834.\n# ℹ 2 more variables: profit2 &lt;dbl&gt;, total_profit &lt;dbl&gt;\n\n  # calculate pure demand effect of association with a celebrity\n\n    # suppose we learn from other market research that a celebrity like \n    # Celebrity will improve our brand perception by an amount delta.\n    # And suppose we have done some work to convert delta into a \"CCE\"\n    # that is, a celebrity coefficient effect of size 0.005\n    \n    # celebrity coefficient effect\n    cce &lt;- 0.005\n    \n    # let's create an adjusted version of our model where we increase the \n    # samsung brand-intercept coefficients by the amount of the CCE\n    out_adj &lt;- out\n    out_adj$coefficients[4:6] &lt;- out_adj$coefficients[4:6] + cce\n    \n    # let's look at the original coefficients next to the adjusted ones, where\n    # we'll see that they are the same except the samsung:segment intercepts\n    # which are 0.005 higher\n    cbind(coef(out), coef(out_adj))\n\n                              [,1]          [,2]\napple:segment1        7.644964e-01  7.644964e-01\napple:segment2        8.806709e-01  8.806709e-01\napple:segment3        1.353091e+00  1.353091e+00\nsegment1:samsung      1.161551e+00  1.166551e+00\nsegment2:samsung      5.566730e-01  5.616730e-01\nsegment3:samsung      5.223327e-01  5.273327e-01\nsegment1:price       -1.597733e-02 -1.597733e-02\nsegment2:price       -3.093656e-02 -3.093656e-02\nsegment3:price       -2.899266e-02 -2.899266e-02\nsegment1:screen_size -5.171158e-01 -5.171158e-01\nsegment2:screen_size  1.574106e-01  1.574106e-01\nsegment3:screen_size -1.922263e-01 -1.922263e-01\nsegment1:price        8.262789e-06  8.262789e-06\nsegment2:price        1.902141e-05  1.902141e-05\nsegment3:price        1.702137e-05  1.702137e-05\n\n    # calculate revised predictions from this adjusted model\n    pred2 &lt;- colMeans(predict(out_adj, newdata = mdat1))\n    names(pred2) &lt;- prod_vec\n    \n    # calculate change in market shares\n    shr_change &lt;- pred2 - pred1\n    names(shr_change) &lt;- prod_vec\n    \n    # print original expected market shares, now with-celeb market shares,\n    # and their difference.  Notice that we multiple by 100, so the interpretation\n    # is that affiliation with Kaby leads to a 0.073% increase for the S1 phone and\n    # a 0.038% increase for the S2 phone\n    round(pred1*100, 2)\n\n   A1    A2    S1    S2    H1    H2 \n24.92 10.68 23.24 11.94 17.24 11.98 \n\n    round(pred2*100, 2)\n\n   A1    A2    S1    S2    H1    H2 \n24.88 10.66 23.32 11.98 17.21 11.95 \n\n    round(shr_change*100, 3)\n\n    A1     A2     S1     S2     H1     H2 \n-0.042 -0.018  0.073  0.038 -0.030 -0.021 \n\n    # let's plot a comparison of market shares with and without Kaby affiliation.\n    # You'll see the heights of the bars are very close.\n    pdat &lt;- rbind(\n        tibble(celeb=\"No\",  product=prod_vec, share=pred1),\n        tibble(celeb=\"Yes\", product=prod_vec, share=pred2)\n    )\n    \n    pdat &lt;- pdat %&gt;% mutate(celeb=fct_inorder(factor(celeb)))\n    ggplot(pdat) +\n        geom_col(aes(product, share, fill=celeb), position=\"dodge\") + \n        ggtitle(\"Pure Demand Effect\") +\n        ylab(\"Product-Specific Market Shares\") + \n        xlab(\"Product\") + \n        scale_fill_manual(\"Celebrity\", values=c(No=\"Firebrick\", Yes=\"Dodgerblue4\")) + \n        theme_bw()\n\n\n\n    # calculate new expected profit\n    d2 &lt;- tibble(price1 = 799, \n                 share1 = pred2[3],\n                 price2 = 899,\n                 share2 = pred2[4])\n    \n    # calculate quantities, revenues, costs, and profits\n    d2 &lt;- d2 %&gt;% mutate(q1 = share1*M, \n                        q2 = share2*M,\n                        rev1 = q1 * price1,\n                        rev2 = q2 * price2,\n                        cost1 = mc1 * q1,\n                        cost2 = mc2 * q2,\n                        profit1 = rev1 - cost1,\n                        profit2 = rev2 - cost2,\n                        total_profit = profit1 + profit2)\n    \n    # print the result\n    rbind(d1, d2)\n\n# A tibble: 2 × 13\n  price1 share1 price2 share2    q1    q2  rev1  rev2 cost1 cost2 profit1\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1    799  0.232    899  0.119  2.32  1.19 1857. 1073. 1023.  561.    834.\n2    799  0.233    899  0.120  2.33  1.20 1863. 1077. 1026.  563.    837.\n# ℹ 2 more variables: profit2 &lt;dbl&gt;, total_profit &lt;dbl&gt;\n\n    # calculate the change in profit\n    d2$total_profit - d1$total_profit\n\n      S1 \n4.233878"
  },
  {
    "objectID": "blog/project8/index.html#summary",
    "href": "blog/project8/index.html#summary",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "",
    "text": "The analysis indicates that the engagement of a celebrity endorser results in a substantial profit increment of $4.23 million. Consequently, if the fee charged by the celebrity for endorsement services is less than this amount, it would be financially advantageous for Samsung to proceed with the hiring. This conclusion underscores the potential profitability and strategic value of celebrity endorsements in enhancing company revenue."
  },
  {
    "objectID": "blog/project8/index.html#hypothesis-employing-celebrity-to-optimize-pricing-strategies-for-samsung-phones",
    "href": "blog/project8/index.html#hypothesis-employing-celebrity-to-optimize-pricing-strategies-for-samsung-phones",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "Hypothesis: Employing Celebrity to Optimize Pricing Strategies for Samsung Phones",
    "text": "Hypothesis: Employing Celebrity to Optimize Pricing Strategies for Samsung Phones\nIn this analysis, we will undertake an optimization exercise similar to the one conducted in the previous week. However, instead of focusing on finding the optimal price point for a single phone, this study will aim to determine the optimal pricing for both the S1 and S2 Samsung phones simultaneously. By employing Kaby, we seek to enhance our pricing strategy to maximize market share and revenue for these two phone models. This comprehensive approach will allow us to better understand the interplay between the prices of the two products and their combined impact on market dynamics and consumer behavior.\n\n# calculate market shares under hypothetical prices\n\n    \n# get a vector of price changes to use for each phone\npvec &lt;- seq(from=-150, to=150, by=10)\n\n# get all combinations of price changes for the two phones\nres &lt;- expand.grid(pvec, pvec)\n\n# and construct empty matrix to store shares at each price\nsmat &lt;- matrix(NA_real_, nrow=nrow(res), ncol=6)\ncolnames(smat) &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\")\n\nres &lt;- cbind(res, smat)\n\n# loop over the 961 price change values\nfor(i in 1:nrow(res)) {\n    \n    # print progress (commented out)\n    # cat(\"Working on\", i, \"of\", nrow(res), \"\\n\")\n    \n    # get the price change amount\n    p1 &lt;- res[i,1]\n    p2 &lt;- res[i,2]\n    \n    # change prices for S1 phones\n    tempdat &lt;- as_tibble(mdat1)\n    tempdat &lt;- tempdat %&gt;% mutate(price = ifelse(phone_id == \"S1\", price + p1, price))\n    tempdat &lt;- tempdat %&gt;% mutate(price = ifelse(phone_id == \"S2\", price + p2, price))\n    \n    # make market share predictions with the temporarily-changed S1 and S2 prices\n    preds &lt;- predict(out_adj, newdata=tempdat)\n    \n    # calculate and store market shares\n    res[i,3:8] &lt;- colMeans(preds)\n}\n\n    # gather prices and estimated shares into a dataframe\n    d3 &lt;- tibble(scenario = 1:nrow(res), \n                   price1 = res[,1] + 799, \n                   share1 = res[,5],\n                   price2 = res[,2] + 899,\n                   share2 = res[,6])\n    \n    # calculate quantities, revenues, costs, and profits\n    d3 &lt;- d3 %&gt;% mutate(q1 = share1*M, \n                        q2 = share2*M,\n                        rev1 = q1 * price1,\n                        rev2 = q2 * price2,\n                        cost1 = mc1 * q1,\n                        cost2 = mc2 * q2,\n                        profit1 = rev1 - cost1,\n                        profit2 = rev2 - cost2,\n                        total_profit = profit1 + profit2)\n    \n    # plot heat map of profit\n    ggplot(d3) + \n        geom_tile(aes(x=price1, y=price2, fill=total_profit)) + \n        scale_fill_gradient(low=\"white\", high=\"blue\") + \n        xlab(\"S1 Price\") + \n        ylab(\"S2 Price\") + \n        ggtitle(\"Profit Heat Map by Product Pricing\") +\n        geom_point(data=d3 %&gt;% filter(total_profit == max(total_profit)), aes(price1, price2), color=\"white\", size=5) +\n        geom_point(data=d2, aes(price1, price2), color=\"red\", size=5) +\n        theme_bw()\n\n\n\n    # select profit-maximizing price combination and compare to prior calculations\n    d3 &lt;- d3 %&gt;% filter(total_profit == max(total_profit)) %&gt;% select(-scenario)\n    rbind(d1, d2, d3)\n\n# A tibble: 3 × 13\n  price1 share1 price2 share2    q1    q2  rev1  rev2 cost1 cost2 profit1\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1    799  0.232    899  0.119  2.32  1.19 1857. 1073. 1023.  561.    834.\n2    799  0.233    899  0.120  2.33  1.20 1863. 1077. 1026.  563.    837.\n3    749  0.261    779  0.203  2.61  2.03 1955. 1585. 1149.  956.    807.\n# ℹ 2 more variables: profit2 &lt;dbl&gt;, total_profit &lt;dbl&gt;"
  },
  {
    "objectID": "blog/project8/index.html#summary-1",
    "href": "blog/project8/index.html#summary-1",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "Summary",
    "text": "Summary\nThe predictive model indicates that, under baseline conditions, Samsung’s profit is estimated to be $1.347 billion. In scenarios where Samsung engages a celebrity for promotional activities, the projected profit experiences a marginal increase, reaching $1.351 billion. However, this figure requires adjustment to account for the costs associated with hiring the celebrity. Furthermore, if Samsung not only hires a celebrity but also optimizes its pricing strategy in conjunction with the celebrity endorsement, the anticipated profit significantly increases to $1.435 billion."
  },
  {
    "objectID": "blog/project8/index.html#calculating-change-in-profit-from-price-optimization",
    "href": "blog/project8/index.html#calculating-change-in-profit-from-price-optimization",
    "title": "Project 8: Price Optimization Using Demand Models",
    "section": "Calculating Change in Profit from Price Optimization",
    "text": "Calculating Change in Profit from Price Optimization\nProfit=(Price−Cost)×Quantity\nConsiderations Elasticity of Demand: The change in quantity sold due to the change in price should reflect the price elasticity of demand for the product. Fixed and Variable Costs: If there are significant fixed costs or if the cost per unit changes with the quantity produced, these should be incorporated into the calculation. Market Conditions: External factors such as competitor pricing, market trends, and consumer preferences should also be considered as they can impact the quantity sold at different price points.\n\nd3$total_profit - d2$total_profit\n\n      S1 \n84.28031"
  },
  {
    "objectID": "blog/project6/index.html",
    "href": "blog/project6/index.html",
    "title": "This is Project 6",
    "section": "",
    "text": "Extending the MNL model from previous project 5 data to include heterogeneity.\n\n\n\n\n\nDefine a function to visualize the probability that a customer will choose a specific phone, separated by different discount levels. We’ll begin by focusing on phone A1.\nThis visualization aims to highlight the differences in predicted preferences among individual customers. Each plot will have an x-axis displaying the predicted choice probability for the selected phone. Facets will categorize customers who encountered the same pricing conditions, specifically those who saw the same phones on discount.\nA histogram will be used to depict the distribution of these choice probabilities among customers. Initially, this distribution might appear uniform in a homogeneous model but will display greater variation as we introduce elements of customer heterogeneity.\nThe function will be applied sequentially to different phones to observe how the histograms vary when different discounts are applied to different phones.\n\n## Data Importing\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mlogit)\n\nLoading required package: dfidx\n\nAttaching package: 'dfidx'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\", show_col_types=F)\n    n &lt;- nrow(cust_dat) \nphone_dat &lt;- read_csv(\"./phone_dat.csv\",show_col_types=F)\n\nload(\"~/Desktop/MGT100 folder/mnl_datasets.RData\")\nload(\"~/Desktop/MGT100 folder/mnl_performance_functions.RData\")   \n    \n\n\n  myplot &lt;- function(data, model, phone) {\n        disc_levs &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\", \"\")\n        \n        preds &lt;- as_tibble(predict(model, newdata=data))\n        colnames(preds) &lt;- disc_levs[1:6]\n        \n        temp &lt;- cust_dat |&gt; \n            filter(years_ago == 1) |&gt; \n            mutate(discount = factor(discount, levels = disc_levs)) |&gt; \n            mutate(discount = fct_recode(discount, \"None\"=\"\")) |&gt; \n            mutate(pr = preds[[phone]]) |&gt; \n            select(discount, pr)\n        \n        p &lt;- temp |&gt; filter(discount != \"None\") |&gt; \n             ggplot(aes(x=pr)) + \n                geom_histogram(aes(fill=discount), binwidth = 0.01) + \n                facet_wrap(. ~ discount) + \n                xlab(paste(\"Probability a customer chooses phone\", phone)) + \n                ylab(\"Count of customers\") + \n                ggtitle(paste0(\"Pr(choose \",phone,\") by customer\")) + \n                labs(fill = \"Discounted Phone\") +\n                theme_bw() \n        \n        print(p)\n        return(invisible(NULL))\n  }\n\n# Calculate market shares \n    \n    # In project 5 I did it, but it's helpful to have these as reference points\n    \n    brand_shares &lt;- cust_dat  |&gt;  \n        filter(years_ago == 1) |&gt; \n        count(brand) |&gt; \n        mutate(shr = n / sum(n))\n    \n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    product_shares &lt;- cust_dat |&gt; \n        filter(years_ago == 1) |&gt; \n        count(phone_id) |&gt; \n        mutate(shr = n / sum(n))\n    \n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n# Fit homogeneous model from project 5\n    \n    out4 &lt;- mlogit(choice ~ phone_id + price | 0, data=mdat1)\n    \n    summary(out4)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000518 \nsuccessive function values within tolerance limits \n\nCoefficients :\n             Estimate Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2  0.3324964  0.2212103  1.5031  0.132819    \nphone_idH1 -1.2251497  0.1724673 -7.1037 1.215e-12 ***\nphone_idH2 -0.9080016  0.1303339 -6.9667 3.244e-12 ***\nphone_idS1 -0.3060387  0.1124682 -2.7211  0.006506 ** \nphone_idS2 -0.5545496  0.1346713 -4.1178 3.825e-05 ***\nprice      -0.0071582  0.0012933 -5.5346 3.119e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1668.6\n\n    brand_hit_rate(mdat1, out4)\n\n[1] 0.367284\n\n    product_hit_rate(mdat1, out4)\n\n[1] 0.2685185\n\n    ll_ratio(mdat1, out4)\n\n[1] 0.0419071\n\n    # plot each individual's probability of choosing phone A1\n    myplot(mdat1, out4, \"A1\")\n\n\n\n\n\n\n\nI am examining the modifications in histograms of choice probabilities for Product A1 as phone discounts fluctuate. Currently, the homogeneous demand model does not reveal any correlation between predicted customer choice shares and their responses to discounts. However, this situation is expected to change.\nI plan to enhance the model by introducing heterogeneity. To begin, I will incorporate individual heterogeneity based on customer attributes, specifically through an interaction term between price and usage (minutes). This addition aims to capture the nuanced effects of pricing on different customer segments, enriching the model’s predictive accuracy and relevance.\n\n\n\nTo introduce heterogeneity into the model, I will integrate an interaction term between “price” and “total minutes” used. This approach is based on the hypothesis that customers who use their phones more extensively may be willing to pay a higher price for their phone services. By including this interaction term, I aim to examine the relationship between phone usage and price sensitivity. A positive coefficient for this interaction would support the hypothesis, indicating that increased usage correlates with a greater willingness to pay, thus adding a layer of individual behavioral variation to the model. This enhancement should provide deeper insights into consumer behavior and improve the model’s ability to predict responses to pricing strategies.\n\n out5 &lt;- mlogit(choice ~ phone_id + price + price:total_minutes | 0, data=mdat1)\n\n    summary(out5)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price + price:total_minutes | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 6.41E-07 \ngradient close to zero \n\nCoefficients :\n                       Estimate  Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2           2.1881e-01  2.2622e-01  0.9672  0.333422    \nphone_idH1          -1.2755e+00  1.7421e-01 -7.3215 2.451e-13 ***\nphone_idH2          -9.2244e-01  1.3110e-01 -7.0361 1.977e-12 ***\nphone_idS1          -3.1735e-01  1.1303e-01 -2.8076  0.004991 ** \nphone_idS2          -5.7209e-01  1.3518e-01 -4.2322 2.314e-05 ***\nprice               -3.0727e-02  3.5267e-03 -8.7126 &lt; 2.2e-16 ***\nprice:total_minutes  1.8024e-05  2.5063e-06  7.1914 6.413e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1640.5\n\n    brand_hit_rate(mdat1, out5)\n\n[1] 0.4032922\n\n    product_hit_rate(mdat1, out5)\n\n[1] 0.287037\n\n    ll_ratio(mdat1, out5)\n\n[1] 0.05803016\n\n    # plot each individual's probability of choosing phone A1\n    myplot(mdat1, out5, \"A1\")\n\n\n\n\n\n\n\nBy incorporating the interaction term between price and total minutes used in our model, we have differentiated the impact of price on the utility derived by different consumers. This adjustment reveals that even among consumers facing the same phone discount, their probabilities of purchase can vary significantly due to their distinct phone usage behaviors.\nTo clarify, let’s consider the structural transformation of the model component concerning price. Originally, the model might have included separate terms for the effect of price and the interaction of price with minutes used:\nUtility=… +(ßprice x price)+(ßint x price x total minutes)+ …\nThis can be reformulated as:\nUtility=… +(ßprice + ßint x total minutes) x price + …\nIn this formulation ßprice is typically negative, indicating that higher prices generally decrease utility. However, when ßint is positive, it implies that consumers with higher usage (more total minutes) exhibit a less negative—or less sensitive—response to price increases. This is because the addition of ßint* Totalminutets to ßprice reduces the absolute value of the negative price coefficient.\nEssentially, this model transformation clarifies that as total minutes increase, the combined coefficient on price approaches zero, indicating a reduction in price sensitivity for high-usage consumers.\nGiven these theoretical considerations, it is now pertinent to delve into the data to explore how this heterogeneous model predicts consumer choice behavior. By examining the empirical effects of these variables, we can validate the model’s assumptions and refine our understanding of consumer dynamics in the market.\n\n\n\nI will now undertake a thorough analysis of the data to ascertain how effectively the heterogeneous model forecasts consumer choice behavior. This deep dive is essential to validate the theoretical constructs of the model and to refine our understanding of how consumer attributes, such as phone usage, influence their sensitivity to pricing.\n\n# grab the data for 2 consumers\n        x1 &lt;- sub1 |&gt; filter(customer_id == 9) |&gt; \n                       mutate(A2 = phone_id == \"A2\", \n                              S1 = phone_id == \"S1\", \n                              S2 = phone_id == \"S2\", \n                              H1 = phone_id == \"H1\", \n                              H2 = phone_id == \"H2\",\n                              ptm = price * total_minutes) |&gt; \n                       select(A2, H1, H2, S1, S2, price, ptm) |&gt; \n                       as.matrix()\n        \n        x2 &lt;- sub1 |&gt; filter(customer_id == 13) |&gt; \n                       mutate(A2 = phone_id == \"A2\", \n                              S1 = phone_id == \"S1\", \n                              S2 = phone_id == \"S2\", \n                              H1 = phone_id == \"H1\", \n                              H2 = phone_id == \"H2\",\n                              ptm = price * total_minutes) |&gt; \n                       select(A2, S1, S2, H1, H2, price, ptm) |&gt; \n                       as.matrix()\n        \n        # notice how the interaction variable (ptm) is different for the two consumers\n        x1\n\n     A2 H1 H2 S1 S2 price     ptm\n[1,]  0  0  0  0  0 764.1 1172627\n[2,]  1  0  0  0  0 999.0 1533117\n[3,]  0  0  0  1  0 799.0 1226187\n[4,]  0  0  0  0  1 899.0 1379652\n[5,]  0  1  0  0  0 749.0 1149454\n[6,]  0  0  1  0  0 799.0 1226187\n\n        x2\n\n     A2 S1 S2 H1 H2 price      ptm\n[1,]  0  0  0  0  0   849 836425.3\n[2,]  1  0  0  0  0   999 984203.6\n[3,]  0  1  0  0  0   799 787165.9\n[4,]  0  0  1  0  0   899 885684.7\n[5,]  0  0  0  1  0   749 737906.4\n[6,]  0  0  0  0  1   799 787165.9\n\n        # This is almost entirely driven by the variation in total minutes for the two consumers.\n        cust_dat |&gt; slice(9,13) |&gt; select(total_minutes)\n\n# A tibble: 2 × 1\n  total_minutes\n          &lt;dbl&gt;\n1         1535.\n2          985.\n\n        # calculate probabilities of purchase for the six phones for each of the two consumers\n        beta_hat &lt;- coef(out5)\n        \n        xb1 &lt;- t(x1 %*% beta_hat)\n        xb2 &lt;- t(x2 %*% beta_hat)\n        \n        example_shares &lt;- rbind(\n            round(exp(xb1) / rowSums(exp(xb1)), 3),\n            round(exp(xb2) / rowSums(exp(xb2)), 3)\n        )\n        \n        colnames(example_shares) &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\")\n        example_shares\n\n        A1    A2    S1    S2    H1    H2\n[1,] 0.305 0.184 0.199 0.114 0.089 0.109\n[2,] 0.177 0.031 0.094 0.037 0.470 0.191\n\n\n\n\n\nObservations from the model indicate that the probabilities of purchasing different phones vary significantly between consumers. For instance, I noted that the first consumer has an estimated 30.5% probability of purchasing the small Apple phone (A1), while this probability drops to 17.7% for the second consumer. In contrast, the second consumer shows a 47.0% probability of purchasing the small Huawei phone (H1), compared to only an 8.9% probability for the first consumer.\nThis discrepancy can be attributed to the differences in phone usage between the two consumers. The second consumer, who uses their phone substantially less than the first, is predicted by the model to be more sensitive to price. This increased price sensitivity leads the second consumer to prefer the more affordable H1 phone over the more expensive A1 phone.\nWith these insights, I will now return to refining the modeling process…\n\n\n\n\n  # compare brand-dummy only model before/after adding segments\n    \n        out1 &lt;- mlogit(choice ~ apple + samsung | 0, data=mdat1)\n        summary(out1)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n3 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00849 \nsuccessive function values within tolerance limits \n\nCoefficients :\n        Estimate Std. Error z-value Pr(&gt;|z|)  \napple   0.197455   0.080071  2.4660  0.01366 *\nsamsung 0.185829   0.080281  2.3147  0.02063 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1737.8\n\n        brand_hit_rate(mdat1, out1)\n\n[1] 0.3559671\n\n        ll_ratio(mdat1, out1)\n\n[1] 0.002181383\n\n        out6 &lt;- mlogit(choice ~ apple:segment + samsung:segment | 0, data=mdat1)\n        summary(out6)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment | 0, \n    data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 3.31E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                   Estimate Std. Error z-value  Pr(&gt;|z|)    \napple:segment1   -0.0645385  0.2074983 -0.3110 0.7557767    \napple:segment2    0.0073260  0.1210463  0.0605 0.9517394    \napple:segment3    0.4946962  0.1268762  3.8990 9.657e-05 ***\nsegment1:samsung  0.6061358  0.1794351  3.3780 0.0007301 ***\nsegment2:samsung  0.1177830  0.1178511  0.9994 0.3175902    \nsegment3:samsung  0.0099503  0.1410709  0.0705 0.9437684    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1721.4\n\n        brand_hit_rate(mdat1, out6)\n\n[1] 0.4166667\n\n        ll_ratio(mdat1, out6)\n\n[1] 0.01161601\n\n\n\n\n\nIn examining the coefficients between models 1 and 6, I observed notable variations. Initially, in model 1, the coefficient for Apple was 0.197. However, when I introduced interactions between Apple and segment dummies, the coefficients adjusted to -0.06, 0.007, and 0.49 respectively. The original value of 0.197 seems to act as a weighted average of these new segment-specific coefficients, providing a broader, albeit less detailed, perspective.\nWith this refined model, more nuanced insights emerge: relative to consumers preferring Huawei phones (the baseline), consumers in segment 1 exhibit a decreased preference for Apple phones, those in segment 2 show a slight increase in preference, and segment 3 consumers demonstrate a significantly heightened preference for Apple. This granularity allows for a deeper understanding of brand preferences across different consumer segments.\n\n\n\n\n # compare brand-dummy + price model before/after adding segments \n    \n        out2 &lt;- mlogit(choice ~ apple + samsung + price | 0, data=mdat1)\n        summary(out2)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000129 \nsuccessive function values within tolerance limits \n\nCoefficients :\n           Estimate  Std. Error  z-value  Pr(&gt;|z|)    \napple    1.05865609  0.10876661   9.7333 &lt; 2.2e-16 ***\nsamsung  0.63450323  0.08959903   7.0816 1.425e-12 ***\nprice   -0.00619721  0.00058165 -10.6545 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1677\n\n        product_hit_rate(mdat1, out2)\n\n[1] 0.2479424\n\n        brand_hit_rate(mdat1, out2)\n\n[1] 0.3549383\n\n        ll_ratio(mdat1, out2)\n\n[1] 0.03709335\n\n        out7 &lt;-  mlogit(choice ~ apple:segment + samsung:segment + price:segment | 0, data=mdat1)\n        summary(out7)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment + price:segment | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000745 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                    Estimate  Std. Error z-value  Pr(&gt;|z|)    \napple:segment1    1.02566857  0.27908247  3.6751 0.0002377 ***\napple:segment2    0.75968029  0.16565164  4.5860 4.518e-06 ***\napple:segment3    1.37531278  0.17076980  8.0536 8.882e-16 ***\nsegment1:samsung  1.18831667  0.20759621  5.7242 1.039e-08 ***\nsegment2:samsung  0.50237709  0.13212776  3.8022 0.0001434 ***\nsegment3:samsung  0.47240850  0.15413564  3.0649 0.0021775 ** \nsegment1:price   -0.00773883  0.00146750 -5.2735 1.339e-07 ***\nsegment2:price   -0.00537932  0.00088152 -6.1023 1.045e-09 ***\nsegment3:price   -0.00641654  0.00091503 -7.0124 2.343e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1659.8\n\n        brand_hit_rate(mdat1, out7)\n\n[1] 0.4176955\n\n        product_hit_rate(mdat1, out7)\n\n[1] 0.3014403\n\n        ll_ratio(mdat1, out7)\n\n[1] 0.04697134\n\n\n\n\n\nI will now concentrate on price sensitivity. By incorporating segment-specific price variables into the model, I am able to estimate distinct price sensitivities for each consumer segment. From this analysis, it is evident that consumers in segment 1 exhibit greater price sensitivity, whereas those in segment 2 demonstrate a lesser sensitivity to price changes. This differentiation allows for a more nuanced understanding of how price impacts different groups within the market.\n\n\n\n\n# compare brand-dummy + price + size model before/after adding segments \n        \n        out3 &lt;- mlogit(choice ~ apple + samsung + price + screen_size| 0, data=mdat1)\n        summary(out3)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price + screen_size | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000119 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error z-value  Pr(&gt;|z|)    \napple        0.9983423  0.1443294  6.9171 4.610e-12 ***\nsamsung      0.6357234  0.0894121  7.1100 1.160e-12 ***\nprice       -0.0056366  0.0010562 -5.3366 9.473e-08 ***\nscreen_size -0.1416883  0.2226515 -0.6364    0.5245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1676.8\n\n        product_hit_rate(mdat1, out3)\n\n[1] 0.2479424\n\n        brand_hit_rate(mdat1, out3)\n\n[1] 0.3549383\n\n        ll_ratio(mdat1, out3)\n\n[1] 0.03721033\n\n        out8 &lt;-  mlogit(choice ~ apple:segment + samsung:segment + price:segment \n                        + screen_size:segment | 0, data=mdat1)\n        summary(out8)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment + price:segment + \n    screen_size:segment | 0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00393 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                       Estimate Std. Error z-value  Pr(&gt;|z|)    \napple:segment1        0.7743060  0.3923699  1.9734 0.0484491 *  \napple:segment2        0.8032405  0.2160772  3.7174 0.0002013 ***\napple:segment3        1.2686962  0.2265921  5.5990 2.156e-08 ***\nsegment1:samsung      1.1593536  0.2077670  5.5801 2.404e-08 ***\nsegment2:samsung      0.5004437  0.1324940  3.7771 0.0001587 ***\nsegment3:samsung      0.4792747  0.1538156  3.1159 0.0018338 ** \nsegment1:price       -0.0055226  0.0028390 -1.9453 0.0517435 .  \nsegment2:price       -0.0057829  0.0015599 -3.7073 0.0002095 ***\nsegment3:price       -0.0053967  0.0016963 -3.1815 0.0014654 ** \nsegment1:screen_size -0.4904160  0.5378925 -0.9117 0.3619077    \nsegment2:screen_size  0.1009087  0.3221117  0.3133 0.7540737    \nsegment3:screen_size -0.2758123  0.3860816 -0.7144 0.4749868    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1659.1\n\n        brand_hit_rate(mdat1, out8)\n\n[1] 0.4053498\n\n        product_hit_rate(mdat1, out8)\n\n[1] 0.281893\n\n        ll_ratio(mdat1, out8)\n\n[1] 0.04739159\n\n\n\n\n\nLast project(5), I observed that including screen size in a model that already accounts for brand and price does not significantly enhance the model’s performance. This observation is supported by the statistical insignificance of the screen size coefficients and the very marginal changes in hit rates and likelihood ratios. These results suggest that screen size, within the context of this model, may not be a decisive factor in predicting consumer choices as compared to brand and price.\n\n\n\n\n#Construct a model with both segment-specific and individual-specific heterogeneity\n        \n        out9 &lt;- mlogit(choice ~ apple:segment + samsung:segment + \n                           price:segment + screen_size:segment + \n                           price:total_minutes:segment | 0, data=mdat1)\n        \n        summary(out9)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment + price:segment + \n    screen_size:segment + price:total_minutes:segment | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 2.77E-07 \ngradient close to zero \n\nCoefficients :\n                        Estimate  Std. Error z-value  Pr(&gt;|z|)    \napple:segment1        7.6450e-01  3.9285e-01  1.9460 0.0516533 .  \napple:segment2        8.8067e-01  2.1797e-01  4.0404 5.337e-05 ***\napple:segment3        1.3531e+00  2.2920e-01  5.9036 3.556e-09 ***\nsegment1:samsung      1.1616e+00  2.0804e-01  5.5832 2.361e-08 ***\nsegment2:samsung      5.5667e-01  1.3421e-01  4.1477 3.359e-05 ***\nsegment3:samsung      5.2233e-01  1.5541e-01  3.3609 0.0007769 ***\nsegment1:price       -1.5977e-02  1.3490e-02 -1.1844 0.2362662    \nsegment2:price       -3.0937e-02  4.7523e-03 -6.5098 7.524e-11 ***\nsegment3:price       -2.8993e-02  6.0525e-03 -4.7902 1.666e-06 ***\nsegment1:screen_size -5.1712e-01  5.3826e-01 -0.9607 0.3366917    \nsegment2:screen_size  1.5741e-01  3.2221e-01  0.4885 0.6251769    \nsegment3:screen_size -1.9223e-01  3.8722e-01 -0.4964 0.6195940    \nsegment1:price        8.2628e-06  1.0374e-05  0.7965 0.4257536    \nsegment2:price        1.9021e-05  3.2980e-06  5.7676 8.039e-09 ***\nsegment3:price        1.7021e-05  4.1106e-06  4.1409 3.460e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1632\n\n        brand_hit_rate(mdat1, out9)\n\n[1] 0.4269547\n\n        product_hit_rate(mdat1, out9)\n\n[1] 0.2973251\n\n        ll_ratio(mdat1, out9)\n\n[1] 0.06294537\n\n    # plot each individual's probability of choosing phone A1 or S2\n    myplot(mdat1, out9, \"A1\")\n\n\n\n    myplot(mdat1, out9, \"S2\")\n\n\n\n\n\n\n\nConcerns about overfitting are paramount, as they could diminish our model’s predictive accuracy. To address this, I propose selecting a model through the method of 10-fold cross-validation. This technique will enable a rigorous evaluation of the model’s performance across multiple subsets of the data, providing a more robust indication of its generalizability. Furthermore, I will focus on the mean squared prediction error (MSPE) as a key metric for assessing the accuracy of our predictions. This systematic approach ensures that we adopt a model not just tailored to fit the training data but also capable of delivering reliable predictions on new, unseen data.\n\n\n\n\n    cv_mspe &lt;- function(model, data, k=10, seed=4321) {\n        # control psuedo random numbers\n        set.seed(seed)\n        \n        # randomly assign each customer (J rows) to one of k folds\n        N &lt;- length(unique(data$customer_id))   # number of customers\n        J &lt;- length(unique(data$phone_id))      # number of products\n        fold &lt;- rep((1:N) %% k + 1, each=J)    # rep replicates list elements\n        \n        # %% is a modulus element, so what we're doing here is computing the remainder when\n        # dividing 1:N by (k+1). So mod(customer_id,10)+1 is always an integer between 1\n        # and 10. We repeat that 6 times for each customer, to match the mdat1 observations\n        \n        # preallocate the prediction storage\n        preds &lt;- vector(length=nrow(data))     \n        \n        # loop over folds \n        for(i in 1:k) {\n            \n            # create row indices for training & prediction observations      \n            row_ids_train &lt;- fold != i     # which rows to keep in for training\n            row_ids_test  &lt;- !row_ids_train  # which rows to hold out for prediction\n            \n            # fit/train model on training data\n            out  &lt;- mlogit(formula(model), data=data[row_ids_train,])\n            \n            # predict choice probabilities for prediction data\n            yhat &lt;- predict(model, newdata = data[row_ids_test,])\n            \n            # store yhat values for prediction data\n            preds[row_ids_test] &lt;- as.vector(t(yhat))\n        }\n        \n        # calculate mse\n        mspe &lt;- mean((data$choice - preds)^2)\n        return(mspe)\n    }\n    \n    # calculate the MSE's for each model\n    mspe &lt;- vector(length=9)\n    mspe[1] &lt;- cv_mspe(out1, mdat1)\n    mspe[2] &lt;- cv_mspe(out2, mdat1)\n    mspe[3] &lt;- cv_mspe(out3, mdat1)\n    mspe[4] &lt;- cv_mspe(out4, mdat1)\n    mspe[5] &lt;- cv_mspe(out5, mdat1)\n    mspe[6] &lt;- cv_mspe(out6, mdat1)\n    mspe[7] &lt;- cv_mspe(out7, mdat1)\n    mspe[8] &lt;- cv_mspe(out8, mdat1)\n    mspe[9] &lt;- cv_mspe(out9, mdat1)\n    \n    # plot the MSE's to compare them\n    tibble(mod_id=1:9, mspe=mspe) |&gt; \n        ggplot(aes(x=mod_id, y=mspe)) +\n        geom_point() + \n        geom_line() \n\n\n\n    ylim(c(0, .14))  # adjustment to show the abs differences\n\n&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 -- 0.14\n\n    # that adjustment helps to show why mspe is only 1 factor among several when \n    #    choosing a model specification\n    \n    # which model has the lowest cross-validated mean-squared-error?\n    which.min(mspe)\n\n[1] 9\n\n\n\n\n\nThis model, referred to as “out9,” has demonstrated the most robust performance based on our criteria, which include the hit rate and the log-likelihood ratio index. Furthermore, it effectively incorporates segment-level heterogeneity, enriching its analytical depth. I will utilize this specific model configuration for the subsequent two projects, and further exploring its capabilities and shows more variety predictive power within various consumer segments."
  },
  {
    "objectID": "blog/project6/index.html#mnl-for-heterogenity",
    "href": "blog/project6/index.html#mnl-for-heterogenity",
    "title": "This is Project 6",
    "section": "",
    "text": "Define a function to visualize the probability that a customer will choose a specific phone, separated by different discount levels. We’ll begin by focusing on phone A1.\nThis visualization aims to highlight the differences in predicted preferences among individual customers. Each plot will have an x-axis displaying the predicted choice probability for the selected phone. Facets will categorize customers who encountered the same pricing conditions, specifically those who saw the same phones on discount.\nA histogram will be used to depict the distribution of these choice probabilities among customers. Initially, this distribution might appear uniform in a homogeneous model but will display greater variation as we introduce elements of customer heterogeneity.\nThe function will be applied sequentially to different phones to observe how the histograms vary when different discounts are applied to different phones.\n\n## Data Importing\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mlogit)\n\nLoading required package: dfidx\n\nAttaching package: 'dfidx'\n\nThe following object is masked from 'package:stats':\n\n    filter\n\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\", show_col_types=F)\n    n &lt;- nrow(cust_dat) \nphone_dat &lt;- read_csv(\"./phone_dat.csv\",show_col_types=F)\n\nload(\"~/Desktop/MGT100 folder/mnl_datasets.RData\")\nload(\"~/Desktop/MGT100 folder/mnl_performance_functions.RData\")   \n    \n\n\n  myplot &lt;- function(data, model, phone) {\n        disc_levs &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\", \"\")\n        \n        preds &lt;- as_tibble(predict(model, newdata=data))\n        colnames(preds) &lt;- disc_levs[1:6]\n        \n        temp &lt;- cust_dat |&gt; \n            filter(years_ago == 1) |&gt; \n            mutate(discount = factor(discount, levels = disc_levs)) |&gt; \n            mutate(discount = fct_recode(discount, \"None\"=\"\")) |&gt; \n            mutate(pr = preds[[phone]]) |&gt; \n            select(discount, pr)\n        \n        p &lt;- temp |&gt; filter(discount != \"None\") |&gt; \n             ggplot(aes(x=pr)) + \n                geom_histogram(aes(fill=discount), binwidth = 0.01) + \n                facet_wrap(. ~ discount) + \n                xlab(paste(\"Probability a customer chooses phone\", phone)) + \n                ylab(\"Count of customers\") + \n                ggtitle(paste0(\"Pr(choose \",phone,\") by customer\")) + \n                labs(fill = \"Discounted Phone\") +\n                theme_bw() \n        \n        print(p)\n        return(invisible(NULL))\n  }\n\n# Calculate market shares \n    \n    # In project 5 I did it, but it's helpful to have these as reference points\n    \n    brand_shares &lt;- cust_dat  |&gt;  \n        filter(years_ago == 1) |&gt; \n        count(brand) |&gt; \n        mutate(shr = n / sum(n))\n    \n    brand_shares\n\n# A tibble: 3 × 3\n  brand       n   shr\n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n1 apple     346 0.356\n2 huawei    284 0.292\n3 samsung   342 0.352\n\n    product_shares &lt;- cust_dat |&gt; \n        filter(years_ago == 1) |&gt; \n        count(phone_id) |&gt; \n        mutate(shr = n / sum(n))\n    \n    product_shares\n\n# A tibble: 6 × 3\n  phone_id     n    shr\n  &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 A1         233 0.240 \n2 A2         113 0.116 \n3 H1         144 0.148 \n4 H2         140 0.144 \n5 S1         246 0.253 \n6 S2          96 0.0988\n\n# Fit homogeneous model from project 5\n    \n    out4 &lt;- mlogit(choice ~ phone_id + price | 0, data=mdat1)\n    \n    summary(out4)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000518 \nsuccessive function values within tolerance limits \n\nCoefficients :\n             Estimate Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2  0.3324964  0.2212103  1.5031  0.132819    \nphone_idH1 -1.2251497  0.1724673 -7.1037 1.215e-12 ***\nphone_idH2 -0.9080016  0.1303339 -6.9667 3.244e-12 ***\nphone_idS1 -0.3060387  0.1124682 -2.7211  0.006506 ** \nphone_idS2 -0.5545496  0.1346713 -4.1178 3.825e-05 ***\nprice      -0.0071582  0.0012933 -5.5346 3.119e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1668.6\n\n    brand_hit_rate(mdat1, out4)\n\n[1] 0.367284\n\n    product_hit_rate(mdat1, out4)\n\n[1] 0.2685185\n\n    ll_ratio(mdat1, out4)\n\n[1] 0.0419071\n\n    # plot each individual's probability of choosing phone A1\n    myplot(mdat1, out4, \"A1\")"
  },
  {
    "objectID": "blog/project6/index.html#illustrations",
    "href": "blog/project6/index.html#illustrations",
    "title": "This is Project 6",
    "section": "",
    "text": "I am examining the modifications in histograms of choice probabilities for Product A1 as phone discounts fluctuate. Currently, the homogeneous demand model does not reveal any correlation between predicted customer choice shares and their responses to discounts. However, this situation is expected to change.\nI plan to enhance the model by introducing heterogeneity. To begin, I will incorporate individual heterogeneity based on customer attributes, specifically through an interaction term between price and usage (minutes). This addition aims to capture the nuanced effects of pricing on different customer segments, enriching the model’s predictive accuracy and relevance."
  },
  {
    "objectID": "blog/project6/index.html#add-heterogenity",
    "href": "blog/project6/index.html#add-heterogenity",
    "title": "This is Project 6",
    "section": "",
    "text": "To introduce heterogeneity into the model, I will integrate an interaction term between “price” and “total minutes” used. This approach is based on the hypothesis that customers who use their phones more extensively may be willing to pay a higher price for their phone services. By including this interaction term, I aim to examine the relationship between phone usage and price sensitivity. A positive coefficient for this interaction would support the hypothesis, indicating that increased usage correlates with a greater willingness to pay, thus adding a layer of individual behavioral variation to the model. This enhancement should provide deeper insights into consumer behavior and improve the model’s ability to predict responses to pricing strategies.\n\n out5 &lt;- mlogit(choice ~ phone_id + price + price:total_minutes | 0, data=mdat1)\n\n    summary(out5)\n\n\nCall:\nmlogit(formula = choice ~ phone_id + price + price:total_minutes | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 6.41E-07 \ngradient close to zero \n\nCoefficients :\n                       Estimate  Std. Error z-value  Pr(&gt;|z|)    \nphone_idA2           2.1881e-01  2.2622e-01  0.9672  0.333422    \nphone_idH1          -1.2755e+00  1.7421e-01 -7.3215 2.451e-13 ***\nphone_idH2          -9.2244e-01  1.3110e-01 -7.0361 1.977e-12 ***\nphone_idS1          -3.1735e-01  1.1303e-01 -2.8076  0.004991 ** \nphone_idS2          -5.7209e-01  1.3518e-01 -4.2322 2.314e-05 ***\nprice               -3.0727e-02  3.5267e-03 -8.7126 &lt; 2.2e-16 ***\nprice:total_minutes  1.8024e-05  2.5063e-06  7.1914 6.413e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1640.5\n\n    brand_hit_rate(mdat1, out5)\n\n[1] 0.4032922\n\n    product_hit_rate(mdat1, out5)\n\n[1] 0.287037\n\n    ll_ratio(mdat1, out5)\n\n[1] 0.05803016\n\n    # plot each individual's probability of choosing phone A1\n    myplot(mdat1, out5, \"A1\")"
  },
  {
    "objectID": "blog/project6/index.html#summary-1",
    "href": "blog/project6/index.html#summary-1",
    "title": "This is Project 6",
    "section": "",
    "text": "By incorporating the interaction term between price and total minutes used in our model, we have differentiated the impact of price on the utility derived by different consumers. This adjustment reveals that even among consumers facing the same phone discount, their probabilities of purchase can vary significantly due to their distinct phone usage behaviors.\nTo clarify, let’s consider the structural transformation of the model component concerning price. Originally, the model might have included separate terms for the effect of price and the interaction of price with minutes used:\nUtility=… +(ßprice x price)+(ßint x price x total minutes)+ …\nThis can be reformulated as:\nUtility=… +(ßprice + ßint x total minutes) x price + …\nIn this formulation ßprice is typically negative, indicating that higher prices generally decrease utility. However, when ßint is positive, it implies that consumers with higher usage (more total minutes) exhibit a less negative—or less sensitive—response to price increases. This is because the addition of ßint* Totalminutets to ßprice reduces the absolute value of the negative price coefficient.\nEssentially, this model transformation clarifies that as total minutes increase, the combined coefficient on price approaches zero, indicating a reduction in price sensitivity for high-usage consumers.\nGiven these theoretical considerations, it is now pertinent to delve into the data to explore how this heterogeneous model predicts consumer choice behavior. By examining the empirical effects of these variables, we can validate the model’s assumptions and refine our understanding of consumer dynamics in the market."
  },
  {
    "objectID": "blog/project6/index.html#deeper-dive-into-the-data",
    "href": "blog/project6/index.html#deeper-dive-into-the-data",
    "title": "This is Project 6",
    "section": "",
    "text": "I will now undertake a thorough analysis of the data to ascertain how effectively the heterogeneous model forecasts consumer choice behavior. This deep dive is essential to validate the theoretical constructs of the model and to refine our understanding of how consumer attributes, such as phone usage, influence their sensitivity to pricing.\n\n# grab the data for 2 consumers\n        x1 &lt;- sub1 |&gt; filter(customer_id == 9) |&gt; \n                       mutate(A2 = phone_id == \"A2\", \n                              S1 = phone_id == \"S1\", \n                              S2 = phone_id == \"S2\", \n                              H1 = phone_id == \"H1\", \n                              H2 = phone_id == \"H2\",\n                              ptm = price * total_minutes) |&gt; \n                       select(A2, H1, H2, S1, S2, price, ptm) |&gt; \n                       as.matrix()\n        \n        x2 &lt;- sub1 |&gt; filter(customer_id == 13) |&gt; \n                       mutate(A2 = phone_id == \"A2\", \n                              S1 = phone_id == \"S1\", \n                              S2 = phone_id == \"S2\", \n                              H1 = phone_id == \"H1\", \n                              H2 = phone_id == \"H2\",\n                              ptm = price * total_minutes) |&gt; \n                       select(A2, S1, S2, H1, H2, price, ptm) |&gt; \n                       as.matrix()\n        \n        # notice how the interaction variable (ptm) is different for the two consumers\n        x1\n\n     A2 H1 H2 S1 S2 price     ptm\n[1,]  0  0  0  0  0 764.1 1172627\n[2,]  1  0  0  0  0 999.0 1533117\n[3,]  0  0  0  1  0 799.0 1226187\n[4,]  0  0  0  0  1 899.0 1379652\n[5,]  0  1  0  0  0 749.0 1149454\n[6,]  0  0  1  0  0 799.0 1226187\n\n        x2\n\n     A2 S1 S2 H1 H2 price      ptm\n[1,]  0  0  0  0  0   849 836425.3\n[2,]  1  0  0  0  0   999 984203.6\n[3,]  0  1  0  0  0   799 787165.9\n[4,]  0  0  1  0  0   899 885684.7\n[5,]  0  0  0  1  0   749 737906.4\n[6,]  0  0  0  0  1   799 787165.9\n\n        # This is almost entirely driven by the variation in total minutes for the two consumers.\n        cust_dat |&gt; slice(9,13) |&gt; select(total_minutes)\n\n# A tibble: 2 × 1\n  total_minutes\n          &lt;dbl&gt;\n1         1535.\n2          985.\n\n        # calculate probabilities of purchase for the six phones for each of the two consumers\n        beta_hat &lt;- coef(out5)\n        \n        xb1 &lt;- t(x1 %*% beta_hat)\n        xb2 &lt;- t(x2 %*% beta_hat)\n        \n        example_shares &lt;- rbind(\n            round(exp(xb1) / rowSums(exp(xb1)), 3),\n            round(exp(xb2) / rowSums(exp(xb2)), 3)\n        )\n        \n        colnames(example_shares) &lt;- c(\"A1\", \"A2\", \"S1\", \"S2\", \"H1\", \"H2\")\n        example_shares\n\n        A1    A2    S1    S2    H1    H2\n[1,] 0.305 0.184 0.199 0.114 0.089 0.109\n[2,] 0.177 0.031 0.094 0.037 0.470 0.191"
  },
  {
    "objectID": "blog/project6/index.html#summary-2",
    "href": "blog/project6/index.html#summary-2",
    "title": "This is Project 6",
    "section": "",
    "text": "Observations from the model indicate that the probabilities of purchasing different phones vary significantly between consumers. For instance, I noted that the first consumer has an estimated 30.5% probability of purchasing the small Apple phone (A1), while this probability drops to 17.7% for the second consumer. In contrast, the second consumer shows a 47.0% probability of purchasing the small Huawei phone (H1), compared to only an 8.9% probability for the first consumer.\nThis discrepancy can be attributed to the differences in phone usage between the two consumers. The second consumer, who uses their phone substantially less than the first, is predicted by the model to be more sensitive to price. This increased price sensitivity leads the second consumer to prefer the more affordable H1 phone over the more expensive A1 phone.\nWith these insights, I will now return to refining the modeling process…"
  },
  {
    "objectID": "blog/project6/index.html#add-heterogeneity-via-a-segment-interactions",
    "href": "blog/project6/index.html#add-heterogeneity-via-a-segment-interactions",
    "title": "This is Project 6",
    "section": "",
    "text": "# compare brand-dummy only model before/after adding segments\n    \n        out1 &lt;- mlogit(choice ~ apple + samsung | 0, data=mdat1)\n        summary(out1)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n3 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00849 \nsuccessive function values within tolerance limits \n\nCoefficients :\n        Estimate Std. Error z-value Pr(&gt;|z|)  \napple   0.197455   0.080071  2.4660  0.01366 *\nsamsung 0.185829   0.080281  2.3147  0.02063 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1737.8\n\n        brand_hit_rate(mdat1, out1)\n\n[1] 0.3559671\n\n        ll_ratio(mdat1, out1)\n\n[1] 0.002181383\n\n        out6 &lt;- mlogit(choice ~ apple:segment + samsung:segment | 0, data=mdat1)\n        summary(out6)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment | 0, \n    data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 3.31E-06 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                   Estimate Std. Error z-value  Pr(&gt;|z|)    \napple:segment1   -0.0645385  0.2074983 -0.3110 0.7557767    \napple:segment2    0.0073260  0.1210463  0.0605 0.9517394    \napple:segment3    0.4946962  0.1268762  3.8990 9.657e-05 ***\nsegment1:samsung  0.6061358  0.1794351  3.3780 0.0007301 ***\nsegment2:samsung  0.1177830  0.1178511  0.9994 0.3175902    \nsegment3:samsung  0.0099503  0.1410709  0.0705 0.9437684    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1721.4\n\n        brand_hit_rate(mdat1, out6)\n\n[1] 0.4166667\n\n        ll_ratio(mdat1, out6)\n\n[1] 0.01161601"
  },
  {
    "objectID": "blog/project6/index.html#summary-3",
    "href": "blog/project6/index.html#summary-3",
    "title": "This is Project 6",
    "section": "",
    "text": "In examining the coefficients between models 1 and 6, I observed notable variations. Initially, in model 1, the coefficient for Apple was 0.197. However, when I introduced interactions between Apple and segment dummies, the coefficients adjusted to -0.06, 0.007, and 0.49 respectively. The original value of 0.197 seems to act as a weighted average of these new segment-specific coefficients, providing a broader, albeit less detailed, perspective.\nWith this refined model, more nuanced insights emerge: relative to consumers preferring Huawei phones (the baseline), consumers in segment 1 exhibit a decreased preference for Apple phones, those in segment 2 show a slight increase in preference, and segment 3 consumers demonstrate a significantly heightened preference for Apple. This granularity allows for a deeper understanding of brand preferences across different consumer segments."
  },
  {
    "objectID": "blog/project6/index.html#brand-dummy-and-price-models",
    "href": "blog/project6/index.html#brand-dummy-and-price-models",
    "title": "This is Project 6",
    "section": "",
    "text": "# compare brand-dummy + price model before/after adding segments \n    \n        out2 &lt;- mlogit(choice ~ apple + samsung + price | 0, data=mdat1)\n        summary(out2)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000129 \nsuccessive function values within tolerance limits \n\nCoefficients :\n           Estimate  Std. Error  z-value  Pr(&gt;|z|)    \napple    1.05865609  0.10876661   9.7333 &lt; 2.2e-16 ***\nsamsung  0.63450323  0.08959903   7.0816 1.425e-12 ***\nprice   -0.00619721  0.00058165 -10.6545 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1677\n\n        product_hit_rate(mdat1, out2)\n\n[1] 0.2479424\n\n        brand_hit_rate(mdat1, out2)\n\n[1] 0.3549383\n\n        ll_ratio(mdat1, out2)\n\n[1] 0.03709335\n\n        out7 &lt;-  mlogit(choice ~ apple:segment + samsung:segment + price:segment | 0, data=mdat1)\n        summary(out7)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment + price:segment | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000745 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                    Estimate  Std. Error z-value  Pr(&gt;|z|)    \napple:segment1    1.02566857  0.27908247  3.6751 0.0002377 ***\napple:segment2    0.75968029  0.16565164  4.5860 4.518e-06 ***\napple:segment3    1.37531278  0.17076980  8.0536 8.882e-16 ***\nsegment1:samsung  1.18831667  0.20759621  5.7242 1.039e-08 ***\nsegment2:samsung  0.50237709  0.13212776  3.8022 0.0001434 ***\nsegment3:samsung  0.47240850  0.15413564  3.0649 0.0021775 ** \nsegment1:price   -0.00773883  0.00146750 -5.2735 1.339e-07 ***\nsegment2:price   -0.00537932  0.00088152 -6.1023 1.045e-09 ***\nsegment3:price   -0.00641654  0.00091503 -7.0124 2.343e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1659.8\n\n        brand_hit_rate(mdat1, out7)\n\n[1] 0.4176955\n\n        product_hit_rate(mdat1, out7)\n\n[1] 0.3014403\n\n        ll_ratio(mdat1, out7)\n\n[1] 0.04697134"
  },
  {
    "objectID": "blog/project6/index.html#summary-4",
    "href": "blog/project6/index.html#summary-4",
    "title": "This is Project 6",
    "section": "",
    "text": "I will now concentrate on price sensitivity. By incorporating segment-specific price variables into the model, I am able to estimate distinct price sensitivities for each consumer segment. From this analysis, it is evident that consumers in segment 1 exhibit greater price sensitivity, whereas those in segment 2 demonstrate a lesser sensitivity to price changes. This differentiation allows for a more nuanced understanding of how price impacts different groups within the market."
  },
  {
    "objectID": "blog/project6/index.html#comparing-brand-dummy",
    "href": "blog/project6/index.html#comparing-brand-dummy",
    "title": "This is Project 6",
    "section": "",
    "text": "# compare brand-dummy + price + size model before/after adding segments \n        \n        out3 &lt;- mlogit(choice ~ apple + samsung + price + screen_size| 0, data=mdat1)\n        summary(out3)\n\n\nCall:\nmlogit(formula = choice ~ apple + samsung + price + screen_size | \n    0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.000119 \nsuccessive function values within tolerance limits \n\nCoefficients :\n              Estimate Std. Error z-value  Pr(&gt;|z|)    \napple        0.9983423  0.1443294  6.9171 4.610e-12 ***\nsamsung      0.6357234  0.0894121  7.1100 1.160e-12 ***\nprice       -0.0056366  0.0010562 -5.3366 9.473e-08 ***\nscreen_size -0.1416883  0.2226515 -0.6364    0.5245    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1676.8\n\n        product_hit_rate(mdat1, out3)\n\n[1] 0.2479424\n\n        brand_hit_rate(mdat1, out3)\n\n[1] 0.3549383\n\n        ll_ratio(mdat1, out3)\n\n[1] 0.03721033\n\n        out8 &lt;-  mlogit(choice ~ apple:segment + samsung:segment + price:segment \n                        + screen_size:segment | 0, data=mdat1)\n        summary(out8)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment + price:segment + \n    screen_size:segment | 0, data = mdat1, method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 0.00393 \nsuccessive function values within tolerance limits \n\nCoefficients :\n                       Estimate Std. Error z-value  Pr(&gt;|z|)    \napple:segment1        0.7743060  0.3923699  1.9734 0.0484491 *  \napple:segment2        0.8032405  0.2160772  3.7174 0.0002013 ***\napple:segment3        1.2686962  0.2265921  5.5990 2.156e-08 ***\nsegment1:samsung      1.1593536  0.2077670  5.5801 2.404e-08 ***\nsegment2:samsung      0.5004437  0.1324940  3.7771 0.0001587 ***\nsegment3:samsung      0.4792747  0.1538156  3.1159 0.0018338 ** \nsegment1:price       -0.0055226  0.0028390 -1.9453 0.0517435 .  \nsegment2:price       -0.0057829  0.0015599 -3.7073 0.0002095 ***\nsegment3:price       -0.0053967  0.0016963 -3.1815 0.0014654 ** \nsegment1:screen_size -0.4904160  0.5378925 -0.9117 0.3619077    \nsegment2:screen_size  0.1009087  0.3221117  0.3133 0.7540737    \nsegment3:screen_size -0.2758123  0.3860816 -0.7144 0.4749868    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1659.1\n\n        brand_hit_rate(mdat1, out8)\n\n[1] 0.4053498\n\n        product_hit_rate(mdat1, out8)\n\n[1] 0.281893\n\n        ll_ratio(mdat1, out8)\n\n[1] 0.04739159"
  },
  {
    "objectID": "blog/project6/index.html#summary-5",
    "href": "blog/project6/index.html#summary-5",
    "title": "This is Project 6",
    "section": "",
    "text": "Last project(5), I observed that including screen size in a model that already accounts for brand and price does not significantly enhance the model’s performance. This observation is supported by the statistical insignificance of the screen size coefficients and the very marginal changes in hit rates and likelihood ratios. These results suggest that screen size, within the context of this model, may not be a decisive factor in predicting consumer choices as compared to brand and price."
  },
  {
    "objectID": "blog/project6/index.html#building-a-model-with-segment-specific-and-individual-specific-heterogeneity",
    "href": "blog/project6/index.html#building-a-model-with-segment-specific-and-individual-specific-heterogeneity",
    "title": "This is Project 6",
    "section": "",
    "text": "#Construct a model with both segment-specific and individual-specific heterogeneity\n        \n        out9 &lt;- mlogit(choice ~ apple:segment + samsung:segment + \n                           price:segment + screen_size:segment + \n                           price:total_minutes:segment | 0, data=mdat1)\n        \n        summary(out9)\n\n\nCall:\nmlogit(formula = choice ~ apple:segment + samsung:segment + price:segment + \n    screen_size:segment + price:total_minutes:segment | 0, data = mdat1, \n    method = \"nr\")\n\nFrequencies of alternatives:choice\n       1        2        3        4        5        6 \n0.239712 0.116255 0.253086 0.098765 0.148148 0.144033 \n\nnr method\n4 iterations, 0h:0m:0s \ng'(-H)^-1g = 2.77E-07 \ngradient close to zero \n\nCoefficients :\n                        Estimate  Std. Error z-value  Pr(&gt;|z|)    \napple:segment1        7.6450e-01  3.9285e-01  1.9460 0.0516533 .  \napple:segment2        8.8067e-01  2.1797e-01  4.0404 5.337e-05 ***\napple:segment3        1.3531e+00  2.2920e-01  5.9036 3.556e-09 ***\nsegment1:samsung      1.1616e+00  2.0804e-01  5.5832 2.361e-08 ***\nsegment2:samsung      5.5667e-01  1.3421e-01  4.1477 3.359e-05 ***\nsegment3:samsung      5.2233e-01  1.5541e-01  3.3609 0.0007769 ***\nsegment1:price       -1.5977e-02  1.3490e-02 -1.1844 0.2362662    \nsegment2:price       -3.0937e-02  4.7523e-03 -6.5098 7.524e-11 ***\nsegment3:price       -2.8993e-02  6.0525e-03 -4.7902 1.666e-06 ***\nsegment1:screen_size -5.1712e-01  5.3826e-01 -0.9607 0.3366917    \nsegment2:screen_size  1.5741e-01  3.2221e-01  0.4885 0.6251769    \nsegment3:screen_size -1.9223e-01  3.8722e-01 -0.4964 0.6195940    \nsegment1:price        8.2628e-06  1.0374e-05  0.7965 0.4257536    \nsegment2:price        1.9021e-05  3.2980e-06  5.7676 8.039e-09 ***\nsegment3:price        1.7021e-05  4.1106e-06  4.1409 3.460e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog-Likelihood: -1632\n\n        brand_hit_rate(mdat1, out9)\n\n[1] 0.4269547\n\n        product_hit_rate(mdat1, out9)\n\n[1] 0.2973251\n\n        ll_ratio(mdat1, out9)\n\n[1] 0.06294537\n\n    # plot each individual's probability of choosing phone A1 or S2\n    myplot(mdat1, out9, \"A1\")\n\n\n\n    myplot(mdat1, out9, \"S2\")"
  },
  {
    "objectID": "blog/project6/index.html#summary-6",
    "href": "blog/project6/index.html#summary-6",
    "title": "This is Project 6",
    "section": "",
    "text": "Concerns about overfitting are paramount, as they could diminish our model’s predictive accuracy. To address this, I propose selecting a model through the method of 10-fold cross-validation. This technique will enable a rigorous evaluation of the model’s performance across multiple subsets of the data, providing a more robust indication of its generalizability. Furthermore, I will focus on the mean squared prediction error (MSPE) as a key metric for assessing the accuracy of our predictions. This systematic approach ensures that we adopt a model not just tailored to fit the training data but also capable of delivering reliable predictions on new, unseen data."
  },
  {
    "objectID": "blog/project6/index.html#which-models-predict-better",
    "href": "blog/project6/index.html#which-models-predict-better",
    "title": "This is Project 6",
    "section": "",
    "text": "cv_mspe &lt;- function(model, data, k=10, seed=4321) {\n        # control psuedo random numbers\n        set.seed(seed)\n        \n        # randomly assign each customer (J rows) to one of k folds\n        N &lt;- length(unique(data$customer_id))   # number of customers\n        J &lt;- length(unique(data$phone_id))      # number of products\n        fold &lt;- rep((1:N) %% k + 1, each=J)    # rep replicates list elements\n        \n        # %% is a modulus element, so what we're doing here is computing the remainder when\n        # dividing 1:N by (k+1). So mod(customer_id,10)+1 is always an integer between 1\n        # and 10. We repeat that 6 times for each customer, to match the mdat1 observations\n        \n        # preallocate the prediction storage\n        preds &lt;- vector(length=nrow(data))     \n        \n        # loop over folds \n        for(i in 1:k) {\n            \n            # create row indices for training & prediction observations      \n            row_ids_train &lt;- fold != i     # which rows to keep in for training\n            row_ids_test  &lt;- !row_ids_train  # which rows to hold out for prediction\n            \n            # fit/train model on training data\n            out  &lt;- mlogit(formula(model), data=data[row_ids_train,])\n            \n            # predict choice probabilities for prediction data\n            yhat &lt;- predict(model, newdata = data[row_ids_test,])\n            \n            # store yhat values for prediction data\n            preds[row_ids_test] &lt;- as.vector(t(yhat))\n        }\n        \n        # calculate mse\n        mspe &lt;- mean((data$choice - preds)^2)\n        return(mspe)\n    }\n    \n    # calculate the MSE's for each model\n    mspe &lt;- vector(length=9)\n    mspe[1] &lt;- cv_mspe(out1, mdat1)\n    mspe[2] &lt;- cv_mspe(out2, mdat1)\n    mspe[3] &lt;- cv_mspe(out3, mdat1)\n    mspe[4] &lt;- cv_mspe(out4, mdat1)\n    mspe[5] &lt;- cv_mspe(out5, mdat1)\n    mspe[6] &lt;- cv_mspe(out6, mdat1)\n    mspe[7] &lt;- cv_mspe(out7, mdat1)\n    mspe[8] &lt;- cv_mspe(out8, mdat1)\n    mspe[9] &lt;- cv_mspe(out9, mdat1)\n    \n    # plot the MSE's to compare them\n    tibble(mod_id=1:9, mspe=mspe) |&gt; \n        ggplot(aes(x=mod_id, y=mspe)) +\n        geom_point() + \n        geom_line() \n\n\n\n    ylim(c(0, .14))  # adjustment to show the abs differences\n\n&lt;ScaleContinuousPosition&gt;\n Range:  \n Limits:    0 -- 0.14\n\n    # that adjustment helps to show why mspe is only 1 factor among several when \n    #    choosing a model specification\n    \n    # which model has the lowest cross-validated mean-squared-error?\n    which.min(mspe)\n\n[1] 9"
  },
  {
    "objectID": "blog/project6/index.html#summary-7",
    "href": "blog/project6/index.html#summary-7",
    "title": "This is Project 6",
    "section": "",
    "text": "This model, referred to as “out9,” has demonstrated the most robust performance based on our criteria, which include the hit rate and the log-likelihood ratio index. Furthermore, it effectively incorporates segment-level heterogeneity, enriching its analytical depth. I will utilize this specific model configuration for the subsequent two projects, and further exploring its capabilities and shows more variety predictive power within various consumer segments."
  },
  {
    "objectID": "blog/project4/index.html",
    "href": "blog/project4/index.html",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "The primary goal of this project is to utilize Principal Component Analysis (PCA) for market mapping by reducing dimensionality. This method streamlines complex data sets, extracting principal components that reveal crucial variances and trends within the market, thereby simplifying the analysis and enhancing visualization capabilities.\nThis market analysis initiative, conducted on behalf of T-Mobile, leverages data sourced directly from the company. The research aims to discern the target demographics for various phone brands within T-Mobile’s portfolio, focusing particularly on the relationship between phone size and consumer attributes. Key questions addressed in this study include:\nAre larger phones predominantly purchased by individuals with larger hands? Is there a trend towards increasing hand size over successive years? Do consumers with similar hand sizes tend to choose phones with the same screen size? Conversely, do individuals with identical hand sizes purchase phones of varying screen sizes? Over time, how does the relationship between hand size and phone size evolve?\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggrepel)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#map phones in 1D attribute space based on their screen sizes \n\n# create a small dataset with one row per phone\n    sub &lt;- cust_dat |&gt; \n            select(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            arrange(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            distinct()\n   head(sub)\n\n# A tibble: 6 × 5\n  years_ago brand   screen_size size_cat phone_id\n      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1         1 apple           6   s        A1      \n2         1 apple           6.5 l        A2      \n3         1 huawei          5.9 s        H1      \n4         1 huawei          6.3 l        H2      \n5         1 samsung         6.1 s        S1      \n6         1 samsung         6.8 l        S2      \n\n  # plot phones by size, one facet per year\n    ggplot(sub, aes(x=screen_size, y=0)) +        \n        facet_grid(rows=vars(years_ago)) +        # facet_grid to create separate plots by years_ago\n        geom_point(size=2) +                      \n        geom_hline(yintercept=0) +                # horizontal line at y=0\n        geom_text_repel(aes(label=phone_id)) +    # using ggrepel package, adds texts to points\n        theme_bw()                                # theme\n\n\n\n\n\n\n\nIn this one-dimensional analysis, several key trends are evident: Samsung’s larger phone models have consistently been significantly bigger than those offered by Apple and Huawei over the past two years. There is a noticeable annual increase in phone sizes across all brands. Not only are the phones getting larger each year, but the variability in phone sizes is also expanding. Despite these changes in size, the relative ordering of phones by size remains stable from year to year."
  },
  {
    "objectID": "blog/project4/index.html#project-overview",
    "href": "blog/project4/index.html#project-overview",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "The primary goal of this project is to utilize Principal Component Analysis (PCA) for market mapping by reducing dimensionality. This method streamlines complex data sets, extracting principal components that reveal crucial variances and trends within the market, thereby simplifying the analysis and enhancing visualization capabilities.\nThis market analysis initiative, conducted on behalf of T-Mobile, leverages data sourced directly from the company. The research aims to discern the target demographics for various phone brands within T-Mobile’s portfolio, focusing particularly on the relationship between phone size and consumer attributes. Key questions addressed in this study include:\nAre larger phones predominantly purchased by individuals with larger hands? Is there a trend towards increasing hand size over successive years? Do consumers with similar hand sizes tend to choose phones with the same screen size? Conversely, do individuals with identical hand sizes purchase phones of varying screen sizes? Over time, how does the relationship between hand size and phone size evolve?"
  },
  {
    "objectID": "blog/project4/index.html#analysis",
    "href": "blog/project4/index.html#analysis",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggrepel)\ncust_dat &lt;- read_csv(\"./smartphone_customer_data.csv\")\n\nRows: 3000 Columns: 19\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): gender, discount, phone_id, brand, size_cat\ndbl (14): height, handsize, age, gaming, chat, maps, video, social, reading,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#map phones in 1D attribute space based on their screen sizes \n\n# create a small dataset with one row per phone\n    sub &lt;- cust_dat |&gt; \n            select(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            arrange(years_ago, brand, screen_size, size_cat, phone_id) |&gt; \n            distinct()\n   head(sub)\n\n# A tibble: 6 × 5\n  years_ago brand   screen_size size_cat phone_id\n      &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;   \n1         1 apple           6   s        A1      \n2         1 apple           6.5 l        A2      \n3         1 huawei          5.9 s        H1      \n4         1 huawei          6.3 l        H2      \n5         1 samsung         6.1 s        S1      \n6         1 samsung         6.8 l        S2      \n\n  # plot phones by size, one facet per year\n    ggplot(sub, aes(x=screen_size, y=0)) +        \n        facet_grid(rows=vars(years_ago)) +        # facet_grid to create separate plots by years_ago\n        geom_point(size=2) +                      \n        geom_hline(yintercept=0) +                # horizontal line at y=0\n        geom_text_repel(aes(label=phone_id)) +    # using ggrepel package, adds texts to points\n        theme_bw()                                # theme"
  },
  {
    "objectID": "blog/project4/index.html#summary",
    "href": "blog/project4/index.html#summary",
    "title": "Project 4: Market Mapping through PCA",
    "section": "",
    "text": "In this one-dimensional analysis, several key trends are evident: Samsung’s larger phone models have consistently been significantly bigger than those offered by Apple and Huawei over the past two years. There is a noticeable annual increase in phone sizes across all brands. Not only are the phones getting larger each year, but the variability in phone sizes is also expanding. Despite these changes in size, the relative ordering of phones by size remains stable from year to year."
  },
  {
    "objectID": "blog/project4/index.html#key-questions-for-market-mapping",
    "href": "blog/project4/index.html#key-questions-for-market-mapping",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Key Questions for Market Mapping:",
    "text": "Key Questions for Market Mapping:\n\nScreen Size vs. Hand Size: What patterns emerge when correlating screen sizes with hand sizes?\nPhone Size and Hand Size Correlation: Are larger phones preferred by individuals with larger hands?\nHand Size Trends: Is there a trend of increasing hand sizes over the years?\nConsistency in Consumer Choices: Do consumers with similar hand sizes choose phones with the same screen size, or do they select varying sizes?\nChanging Size Relationships: How does the relationship between hand size and phone size change over time?"
  },
  {
    "objectID": "blog/project4/index.html#what-would-we-expect-to-see-in-a-screensize-x-handsize-map",
    "href": "blog/project4/index.html#what-would-we-expect-to-see-in-a-screensize-x-handsize-map",
    "title": "Project 4: Market Mapping through PCA",
    "section": "What would we expect to see in a screensize x handsize map?",
    "text": "What would we expect to see in a screensize x handsize map?\n\nScreen Size vs. Hand Size Correlation: We expect to see a correlation where larger phones are preferred by individuals with larger hands.\nHand Size Trends: A potential trend of increasing hand sizes over the years could be explored to see if it aligns with the trend of increasing phone screen sizes.\nConsistency in Consumer Choices: We might find that consumers with similar hand sizes consistently choose phones with similar screen sizes, or conversely, that they choose varying sizes, indicating a diverse preference range.\nChanging Size Relationships: The relationship between hand size and phone size may evolve over time, which could indicate changing consumer preferences or innovations in phone design.\n\n\n# Plot consumers' screensize vs handsize, facet by phone, for only years_ago==1\n    ggplot(cust_dat |&gt; filter(years_ago==1)) + \n        geom_histogram(aes(handsize), bins=50) + \n        facet_grid(rows=vars(screen_size)) + \n        ggtitle(\"Hand Size Distributions by Screen Size\") + \n        theme_bw()\n\n\n\n # Plot/map phones in screensize x handsize space\n    ggplot(sub, aes(x=screen_size, y=mhs)) +   # \n        geom_point() +                         #\n        facet_grid(rows=vars(years_ago)) +     # different plots for different years_ago\n        geom_smooth(method=\"lm\", se=F) +       # add linear trend (ie regression) line\n        geom_text_repel(aes(label=phone_id)) + #\n        theme_bw()                        \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "blog/project4/index.html#observations-summary",
    "href": "blog/project4/index.html#observations-summary",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Observations Summary",
    "text": "Observations Summary\nPositive Correlation: There exists a positive relationship between screen size and hand size, indicating that typically, larger screens are favored by individuals with larger hands.\nImperfect Correlation: This relationship, however, is not consistent across all observations. For instance, in the third facet of our analysis:\nBoth S1 and H2 models have comparable screen sizes, yet the H2 model is more commonly purchased by consumers with larger hands. Conversely, the S1 and A2 models have differing screen sizes but are frequently chosen by consumers with similar hand sizes. Evolving Trends: Over time, the strength of this relationship appears to be diminishing slightly, as evidenced by a gradual flattening of the trend line."
  },
  {
    "objectID": "blog/project4/index.html#pca",
    "href": "blog/project4/index.html#pca",
    "title": "Project 4: Market Mapping through PCA",
    "section": "PCA",
    "text": "PCA\n\n#use PCA to reduce our 2D (screensize, avg handsize) to 1D\npca_out1 &lt;- sub |&gt; ungroup() |&gt; filter(years_ago == 1) |&gt; select(screen_size, mhs) |&gt; prcomp()\npca_out2 &lt;- sub |&gt; ungroup() |&gt; filter(years_ago == 2) |&gt; select(screen_size, mhs) |&gt; prcomp()\npca_out3 &lt;- sub |&gt; ungroup() |&gt; filter(years_ago == 3) |&gt; select(screen_size, mhs) |&gt; prcomp()\n    \nsummary(pca_out1)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.7015 0.06153\nProportion of Variance 0.9924 0.00763\nCumulative Proportion  0.9924 1.00000\n\nsummary(pca_out2)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.5450 0.07835\nProportion of Variance 0.9798 0.02025\nCumulative Proportion  0.9798 1.00000\n\nsummary(pca_out3)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.5978 0.07243\nProportion of Variance 0.9855 0.01447\nCumulative Proportion  0.9855 1.00000\n\n # We see that one component explains the majority of variance in the data\n    # This is because mean-handsize and screen size are highly correlated\n    \n    sub |&gt; group_by(years_ago) |&gt; summarize(cor=cor(mhs, screen_size))\n\n# A tibble: 3 × 2\n  years_ago   cor\n      &lt;dbl&gt; &lt;dbl&gt;\n1         1 0.978\n2         2 0.926\n3         3 0.948\n\n    # Let's also compare the variance along the principal components to the \n    # variance alone the original dimensions\n    \n    summary(pca_out1)\n\nImportance of components:\n                          PC1     PC2\nStandard deviation     0.7015 0.06153\nProportion of Variance 0.9924 0.00763\nCumulative Proportion  0.9924 1.00000\n\n    sub |&gt; ungroup() |&gt; select(screen_size, mhs) |&gt; summarize_all(sd)\n\n# A tibble: 1 × 2\n  screen_size   mhs\n        &lt;dbl&gt; &lt;dbl&gt;\n1       0.376 0.526\n\n    # we see that the original data varied along both variables, while the rotated\n    # data varies mostly along the first principal component\n    \n    \n    # the rotated data is in pca$x\n    # we can use the first column of x for our 1D plot\n    \n    # first, \"extract\" x into a tibble, to make it easier to work with\n    pcs1 &lt;- as_tibble(pca_out1$x)\n    pcs2 &lt;- as_tibble(pca_out2$x)\n    pcs3 &lt;- as_tibble(pca_out3$x)\n    \n    # and append these tibbles together\n    pcs &lt;- bind_rows(pcs1, pcs2, pcs3, .id=\"years_ago\")\n    \n    # now we can plot in 1D space\n    ggplot(pcs, aes(x=PC1, y=0)) + \n        facet_grid(rows=vars(years_ago)) + \n        geom_point(size=3) + \n        geom_hline(yintercept=0) + \n        geom_text_repel(aes(label=sub$phone_id)) + \n        theme_bw()"
  },
  {
    "objectID": "blog/project4/index.html#observations-summary-1",
    "href": "blog/project4/index.html#observations-summary-1",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Observations Summary",
    "text": "Observations Summary\nDistance Between Models: The H1 model is now notably more distant from the other phone models in terms of selected attributes.\nYearly Comparison: In the first year, the A1 and S1 models were significantly further apart. In the second year, these models moved closer together. By the third year, the positioning of A1 and S1 relative to each other remained unchanged."
  },
  {
    "objectID": "blog/project4/index.html#key-findings",
    "href": "blog/project4/index.html#key-findings",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Key Findings:",
    "text": "Key Findings:\n\nPositive Correlation: The analysis confirmed a positive correlation between screen size and hand size, indicating that larger phones are typically favored by individuals with larger hands.\nInconsistent Trends: Despite the overall trend, the correlation between phone size and hand size was not uniformly observed across all models and brands. Specific instances showed that similar screen sizes could attract consumers with different hand sizes, and vice versa.\nDynamic Market Behavior: Over time, the relationship between hand size and phone size showed signs of weakening. This suggests that as the market evolves, consumer preferences may be influenced by factors other than just the physical size of the device."
  },
  {
    "objectID": "blog/project4/index.html#yearly-model-comparison",
    "href": "blog/project4/index.html#yearly-model-comparison",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Yearly Model Comparison:",
    "text": "Yearly Model Comparison:\nThe distance between specific models such as the H1 and others highlighted significant differentiation in market positioning. Additionally, the A1 and S1 models showed variable proximity over three years, indicating shifts in market strategy or consumer acceptance."
  },
  {
    "objectID": "blog/project4/index.html#strategic-implications",
    "href": "blog/project4/index.html#strategic-implications",
    "title": "Project 4: Market Mapping through PCA",
    "section": "Strategic Implications:",
    "text": "Strategic Implications:\nThe findings from this research are instrumental for T-Mobile to tailor its product strategies. Understanding that consumer preferences are not static but evolve with time can aid in more dynamically aligning product features with consumer needs. Moreover, recognizing the diverse preferences within consumer segments can help in targeted marketing and product development efforts.\n##Future Directions:\nBased on the outcomes of this PCA-driven market mapping, further research could explore more granular consumer data, incorporate additional variables such as consumer lifestyle and usage patterns, and apply predictive analytics to forecast upcoming trends in smartphone design and functionality."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "data file:mtcars I use Kmean algorithm in this project to analyse market segmentation problems.\n\n\n\n[1] mpg Miles/(US) gallon [2] cyl Number of cylinders [3] disp Displacement (cu.in.) [4] hp Gross horsepower [5] drat Rear axle ratio [6] wt Weight (1000 lbs) [7] qsec 1/4 mile time [8] vs Engine (0 = V-shaped, 1 = straight) [9] am Transmission (0 = automatic, 1 = manual) [10] gear Number of forward gears\n\nlibrary(tidyverse)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nsub &lt;- mtcars %&gt;% select(wt, mpg)\nsub\n\n                       wt  mpg\nMazda RX4           2.620 21.0\nMazda RX4 Wag       2.875 21.0\nDatsun 710          2.320 22.8\nHornet 4 Drive      3.215 21.4\nHornet Sportabout   3.440 18.7\nValiant             3.460 18.1\nDuster 360          3.570 14.3\nMerc 240D           3.190 24.4\nMerc 230            3.150 22.8\nMerc 280            3.440 19.2\nMerc 280C           3.440 17.8\nMerc 450SE          4.070 16.4\nMerc 450SL          3.730 17.3\nMerc 450SLC         3.780 15.2\nCadillac Fleetwood  5.250 10.4\nLincoln Continental 5.424 10.4\nChrysler Imperial   5.345 14.7\nFiat 128            2.200 32.4\nHonda Civic         1.615 30.4\nToyota Corolla      1.835 33.9\nToyota Corona       2.465 21.5\nDodge Challenger    3.520 15.5\nAMC Javelin         3.435 15.2\nCamaro Z28          3.840 13.3\nPontiac Firebird    3.845 19.2\nFiat X1-9           1.935 27.3\nPorsche 914-2       2.140 26.0\nLotus Europa        1.513 30.4\nFord Pantera L      3.170 15.8\nFerrari Dino        2.770 19.7\nMaserati Bora       3.570 15.0\nVolvo 142E          2.780 21.4\n\nscl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\nscl\n\n# A tibble: 32 × 2\n         wt    mpg\n      &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.610    0.151\n 2 -0.350    0.151\n 3 -0.917    0.450\n 4 -0.00230  0.217\n 5  0.228   -0.231\n 6  0.248   -0.330\n 7  0.361   -0.961\n 8 -0.0278   0.715\n 9 -0.0687   0.450\n10  0.228   -0.148\n# ℹ 22 more rows\n\nscl %&gt;% summarize_all(mean) %&gt;% round(3) \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0     0\n\nscl %&gt;% summarize_all(sd)  \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n\n#save the output from the kmeans() function into an object named \"out\"\nout &lt;- kmeans(scl, centers=4, nstart=10)\nK &lt;- 4\nD &lt;- 10\n#set seed for random number generator \nset.seed(1234)\n\n\n# \"out\" is a list.  This is common with model-fitting functions in R.  To get\n# a better sense for what's included in \"out\", let's run the structure function:\nout &lt;- kmeans(scl, centers=K, nstart=D)\nstr(out)\n\nList of 9\n $ cluster     : int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n $ centers     : num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"wt\" \"mpg\"\n $ totss       : num 62\n $ withinss    : num [1:4] 1.623 2.016 1.271 0.355\n $ tot.withinss: num 5.27\n $ betweenss   : num 56.7\n $ size        : int [1:4] 6 14 9 3\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n# 3 ways to extract a list element -- returns the element\nstr(out$cluster)\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[\"cluster\"]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[1]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# 2 related ways to subset a list into a one-element list (usually not what you want)\nstr(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[1])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# out$centers is a k-by-J matrix with the coordinates of the\n# clusters' centers\n        \nstr(out$centers)\n\n num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"wt\" \"mpg\"\n\nout$centers\n\n          wt        mpg\n1 -1.3738462  1.6552394\n2  0.3846068 -0.5957617\n3 -0.4054284  0.2799348\n4  2.1691456 -1.3700619\n\n#enhancing plot by using the data above\nsub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n#comparing a count from our data to the kmeans() 'size' output\nsub %&gt;% count(cluster)\n\n  cluster  n\n1       1  6\n2       2 14\n3       3  9\n4       4  3\n\nout$size\n\n[1]  6 14  9  3\n\n#store the clusters' center locations in their own tibble/dataframe\ncenters &lt;- as_tibble(out$centers) \ncenters\n\n# A tibble: 4 × 2\n      wt    mpg\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 -1.37   1.66 \n2  0.385 -0.596\n3 -0.405  0.280\n4  2.17  -1.37 \n\n# calculate mean and sd\nSD   &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(sd)\nMEAN &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(mean)\n        \nSD\n\n         wt      mpg\n1 0.9784574 6.026948\n\nMEAN\n\n       wt      mpg\n1 3.21725 20.09062\n\n#repeat/format the values so we can do math with centers \nSD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\nMEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \nSD\n\n          [,1]     [,2]\n[1,] 0.9784574 6.026948\n[2,] 0.9784574 6.026948\n[3,] 0.9784574 6.026948\n[4,] 0.9784574 6.026948\n\nMEAN\n\n        [,1]     [,2]\n[1,] 3.21725 20.09062\n[2,] 3.21725 20.09062\n[3,] 3.21725 20.09062\n[4,] 3.21725 20.09062\n\n# unscale the centers (convert back into original units)\ncenters &lt;- centers*SD + MEAN\nround(centers, 1)\n\n   wt  mpg\n1 1.9 30.1\n2 3.6 16.5\n3 2.8 21.8\n4 5.3 11.8\n\nggplot() + \n        geom_point(data=sub,     aes(x=wt, y=mpg, color=cluster)) + \n        geom_point(data=centers, aes(x=wt, y=mpg), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n# is k=4 the right number for k?\n# let's try k=1, k=2, ..., k=10\nres &lt;- vector(length=10)\n# we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"red\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()\n\n\n\n\n\n\n\n\n# Example with \"iris\" data\n#The Iris dataset comprises measurements of iris flowers from three different \n#species: Setosa, Versicolor, and Virginica. Each sample consists of \n#four features: sepal length, sepal width, petal length, and petal width. \n#Additionally, each sample is labeled with its corresponding species.\n# load data\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# plot petal length vs petal width by species\nggplot(iris) + \ngeom_point(aes(x=Petal.Length, y=Petal.Width, col=Species))\n\n\n\n# run kmeans\nout_iris &lt;- iris %&gt;% \nselect(Petal.Length, Petal.Width) %&gt;% \nkmeans(centers = 3, nstart = 10)\n# add segment membership\niris &lt;- iris %&gt;% mutate(cluster = factor(out_iris$cluster))\n    \n# plot segmented data\nggplot(iris, aes(x=Petal.Length, y=Petal.Width)) +\ngeom_point(aes(col=cluster)) + \ngeom_point(data=as_tibble(out_iris$centers), size=4) + \ntheme_minimal()\n\n\n\n# \"confusion matrix\" -- ie, a table of actual vs predicted\niris %&gt;% select(Species, cluster) %&gt;% table()\n\n            cluster\nSpecies       1  2  3\n  setosa      0 50  0\n  versicolor  2  0 48\n  virginica  46  0  4"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "data file:mtcars I use Kmean algorithm in this project to analyse market segmentation problems."
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "[1] mpg Miles/(US) gallon [2] cyl Number of cylinders [3] disp Displacement (cu.in.) [4] hp Gross horsepower [5] drat Rear axle ratio [6] wt Weight (1000 lbs) [7] qsec 1/4 mile time [8] vs Engine (0 = V-shaped, 1 = straight) [9] am Transmission (0 = automatic, 1 = manual) [10] gear Number of forward gears\n\nlibrary(tidyverse)\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\nsub &lt;- mtcars %&gt;% select(wt, mpg)\nsub\n\n                       wt  mpg\nMazda RX4           2.620 21.0\nMazda RX4 Wag       2.875 21.0\nDatsun 710          2.320 22.8\nHornet 4 Drive      3.215 21.4\nHornet Sportabout   3.440 18.7\nValiant             3.460 18.1\nDuster 360          3.570 14.3\nMerc 240D           3.190 24.4\nMerc 230            3.150 22.8\nMerc 280            3.440 19.2\nMerc 280C           3.440 17.8\nMerc 450SE          4.070 16.4\nMerc 450SL          3.730 17.3\nMerc 450SLC         3.780 15.2\nCadillac Fleetwood  5.250 10.4\nLincoln Continental 5.424 10.4\nChrysler Imperial   5.345 14.7\nFiat 128            2.200 32.4\nHonda Civic         1.615 30.4\nToyota Corolla      1.835 33.9\nToyota Corona       2.465 21.5\nDodge Challenger    3.520 15.5\nAMC Javelin         3.435 15.2\nCamaro Z28          3.840 13.3\nPontiac Firebird    3.845 19.2\nFiat X1-9           1.935 27.3\nPorsche 914-2       2.140 26.0\nLotus Europa        1.513 30.4\nFord Pantera L      3.170 15.8\nFerrari Dino        2.770 19.7\nMaserati Bora       3.570 15.0\nVolvo 142E          2.780 21.4\n\nscl &lt;- sub %&gt;% scale() %&gt;% as_tibble()\nscl\n\n# A tibble: 32 × 2\n         wt    mpg\n      &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.610    0.151\n 2 -0.350    0.151\n 3 -0.917    0.450\n 4 -0.00230  0.217\n 5  0.228   -0.231\n 6  0.248   -0.330\n 7  0.361   -0.961\n 8 -0.0278   0.715\n 9 -0.0687   0.450\n10  0.228   -0.148\n# ℹ 22 more rows\n\nscl %&gt;% summarize_all(mean) %&gt;% round(3) \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0     0\n\nscl %&gt;% summarize_all(sd)  \n\n# A tibble: 1 × 2\n     wt   mpg\n  &lt;dbl&gt; &lt;dbl&gt;\n1     1     1\n\n#save the output from the kmeans() function into an object named \"out\"\nout &lt;- kmeans(scl, centers=4, nstart=10)\nK &lt;- 4\nD &lt;- 10\n#set seed for random number generator \nset.seed(1234)\n\n\n# \"out\" is a list.  This is common with model-fitting functions in R.  To get\n# a better sense for what's included in \"out\", let's run the structure function:\nout &lt;- kmeans(scl, centers=K, nstart=D)\nstr(out)\n\nList of 9\n $ cluster     : int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n $ centers     : num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  .. ..$ : chr [1:2] \"wt\" \"mpg\"\n $ totss       : num 62\n $ withinss    : num [1:4] 1.623 2.016 1.271 0.355\n $ tot.withinss: num 5.27\n $ betweenss   : num 56.7\n $ size        : int [1:4] 6 14 9 3\n $ iter        : int 1\n $ ifault      : int 0\n - attr(*, \"class\")= chr \"kmeans\"\n\n# 3 ways to extract a list element -- returns the element\nstr(out$cluster)\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[\"cluster\"]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[[1]])\n\n int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# 2 related ways to subset a list into a one-element list (usually not what you want)\nstr(out[\"cluster\"])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\nstr(out[1])\n\nList of 1\n $ cluster: int [1:32] 3 3 3 3 2 2 2 3 3 2 ...\n\n# out$centers is a k-by-J matrix with the coordinates of the\n# clusters' centers\n        \nstr(out$centers)\n\n num [1:4, 1:2] -1.374 0.385 -0.405 2.169 1.655 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:4] \"1\" \"2\" \"3\" \"4\"\n  ..$ : chr [1:2] \"wt\" \"mpg\"\n\nout$centers\n\n          wt        mpg\n1 -1.3738462  1.6552394\n2  0.3846068 -0.5957617\n3 -0.4054284  0.2799348\n4  2.1691456 -1.3700619\n\n#enhancing plot by using the data above\nsub &lt;- sub %&gt;% mutate(cluster = factor(out$cluster))\n#comparing a count from our data to the kmeans() 'size' output\nsub %&gt;% count(cluster)\n\n  cluster  n\n1       1  6\n2       2 14\n3       3  9\n4       4  3\n\nout$size\n\n[1]  6 14  9  3\n\n#store the clusters' center locations in their own tibble/dataframe\ncenters &lt;- as_tibble(out$centers) \ncenters\n\n# A tibble: 4 × 2\n      wt    mpg\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 -1.37   1.66 \n2  0.385 -0.596\n3 -0.405  0.280\n4  2.17  -1.37 \n\n# calculate mean and sd\nSD   &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(sd)\nMEAN &lt;- sub %&gt;% select(wt, mpg) %&gt;% summarize_all(mean)\n        \nSD\n\n         wt      mpg\n1 0.9784574 6.026948\n\nMEAN\n\n       wt      mpg\n1 3.21725 20.09062\n\n#repeat/format the values so we can do math with centers \nSD   &lt;- SD   %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\nMEAN &lt;- MEAN %&gt;% unlist() %&gt;% rep(K) %&gt;% matrix(nrow=K, ncol=2, byrow=T)\n        \nSD\n\n          [,1]     [,2]\n[1,] 0.9784574 6.026948\n[2,] 0.9784574 6.026948\n[3,] 0.9784574 6.026948\n[4,] 0.9784574 6.026948\n\nMEAN\n\n        [,1]     [,2]\n[1,] 3.21725 20.09062\n[2,] 3.21725 20.09062\n[3,] 3.21725 20.09062\n[4,] 3.21725 20.09062\n\n# unscale the centers (convert back into original units)\ncenters &lt;- centers*SD + MEAN\nround(centers, 1)\n\n   wt  mpg\n1 1.9 30.1\n2 3.6 16.5\n3 2.8 21.8\n4 5.3 11.8\n\nggplot() + \n        geom_point(data=sub,     aes(x=wt, y=mpg, color=cluster)) + \n        geom_point(data=centers, aes(x=wt, y=mpg), size=4) + \n        ggtitle(\"Kmeans cluster membership and centroids\") + \n        theme_minimal()\n\n\n\n# is k=4 the right number for k?\n# let's try k=1, k=2, ..., k=10\nres &lt;- vector(length=10)\n# we loop over k=1 through k=10\n    for(i in 1:10) {\n        # run k means\n        out &lt;- kmeans(scl, centers=i)\n        \n        # grab the WSS value, store it in the i'th position of res\n        res[i] &lt;- out$tot.withinss\n    }\n    \n    # let's plot the WSS for each value of k\n    ggplot(data.frame(x=1:10, y=res), aes(x,y)) + \n        geom_line(color=\"red\") + \n        geom_point(size=3) + \n        xlab(\"Number of Clusters (K)\") + \n        ylab(\"Within-Group Sum of Squares (WSS)\") + \n        theme_minimal()"
  },
  {
    "objectID": "blog/project2/index.html#section-3-iris-dataset",
    "href": "blog/project2/index.html#section-3-iris-dataset",
    "title": "This is Project 2",
    "section": "",
    "text": "# Example with \"iris\" data\n#The Iris dataset comprises measurements of iris flowers from three different \n#species: Setosa, Versicolor, and Virginica. Each sample consists of \n#four features: sepal length, sepal width, petal length, and petal width. \n#Additionally, each sample is labeled with its corresponding species.\n# load data\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# plot petal length vs petal width by species\nggplot(iris) + \ngeom_point(aes(x=Petal.Length, y=Petal.Width, col=Species))\n\n\n\n# run kmeans\nout_iris &lt;- iris %&gt;% \nselect(Petal.Length, Petal.Width) %&gt;% \nkmeans(centers = 3, nstart = 10)\n# add segment membership\niris &lt;- iris %&gt;% mutate(cluster = factor(out_iris$cluster))\n    \n# plot segmented data\nggplot(iris, aes(x=Petal.Length, y=Petal.Width)) +\ngeom_point(aes(col=cluster)) + \ngeom_point(data=as_tibble(out_iris$centers), size=4) + \ntheme_minimal()\n\n\n\n# \"confusion matrix\" -- ie, a table of actual vs predicted\niris %&gt;% select(Species, cluster) %&gt;% table()\n\n            cluster\nSpecies       1  2  3\n  setosa      0 50  0\n  versicolor  2  0 48\n  virginica  46  0  4"
  }
]